# This is an implementation of the STRIDE solver, https://doi.org/10.1007/s10107-022-01912-6
module Stride

using LinearAlgebra, SparseArrays, StandardPacked, Printf
using ...PolynomialOptimization: @assert, @inbounds, @verbose_info, poly_problem, poly_optimize, poly_solutions, Result,
    Relaxation
using ..Solvers: EfficientCholmod
using ...Solver: MomentVector
import Optim

export stride_solve

struct Data{R<:Real,F<:Factorization,Fmt,SS<:Union{SparseMatrixCSC{R,Int},UniformScaling{Bool}}}
    d::Int
    A::Vector{SparseMatrixCSC{R,Int}}
    Astar::Vector{SparseMatrixCSC{R,Int}}
    AAstarinv::F
    C::Vector{SPMatrix{R,SparseVector{R,Int},Fmt}}
    equality_subspace::Tuple{SS,SparseVector{R,Int}}
    b::SparseVector{R,Int}
    offset::R
    X::Vector{SPMatrix{R,Vector{R},Fmt}} # each dimensions n·µ¢ √ó n·µ¢
    S::Vector{SPMatrix{R,Vector{R},Fmt}}
    tmpm1::Vector{SPMatrix{R,Vector{R},Fmt}}
    tmpm2::Vector{SPMatrix{R,Vector{R},Fmt}}
    y::Vector{R} # length d
    tmpv::Vector{R}
    tmpvlong::Vector{R} # length packedsize(max(n·µ¢))
end

function psdproject!(m::SPMatrix{R}) where {R}
    munscaled = packed_unscale!(m)
    eigs = eigen(munscaled, zero(R), typemax(R))
    fill!(munscaled, zero(eltype(m)))
    for (eval, evec) in zip(eigs.values, eachcol(eigs.vectors))
        eval > zero(R) && spr!(eval, evec, munscaled)
    end
    return packed_scale!(munscaled)
end

mutable struct PolyLBFGSState{R,S<:Optim.LBFGSState} <: Optim.AbstractOptimizerState
    const state::S
    const ssd::Data{R}
    const tol::R
    success::Bool
end

Optim.reset!(method, state::PolyLBFGSState, obj, x) = Optim.reset!(method, state.state, obj, x)
Optim.update_state!(d, state::PolyLBFGSState, method::Optim.LBFGS) = Optim.update_state!(d, state.state, method)
Optim.update_h!(d, state::PolyLBFGSState, method::Optim.LBFGS) = Optim.update_h!(d, state.state, method)
Optim.trace!(tr, d, state::PolyLBFGSState, iteration, method::Optim.LBFGS, options, curr_time=time()) =
    Optim.trace!(tr, d, state.state, iteration, method, options, curr_time)
function Base.getproperty(state::PolyLBFGSState, prop::Symbol)
    if prop === :state || prop === :ssd || prop === :tol || prop === :success
        return getfield(state, prop)
    else
        return getproperty(getfield(state, :state), prop)
    end
end
function Base.setproperty!(state::PolyLBFGSState, prop::Symbol, v)
    if prop === :success
        return setfield!(state, prop, v)
    else
        return setfield!(getfield(state, :state), prop, v)
    end
end

function Optim.assess_convergence(state::PolyLBFGSState{R}, d, ::Optim.Options) where {R}
    ssd = state.ssd
    Astar, Z, W, Œæ = ssd.Astar, ssd.X, ssd.S, state.state.x
    for (W·µ¢, Z·µ¢, Astar·µ¢) in zip(W, Z, Astar)
        copyto!(W·µ¢, Z·µ¢)
        mul!(W·µ¢, Astar·µ¢, Œæ, -one(R), -one(R))
        psdproject!(W·µ¢)
    end
    # x_converged, f_converged, g_converged, f_increased - we disregard all this and use the criterion from Algorithm 6
    state.success = converged = Œ∑proj(ssd, W, Œæ) ‚â§ state.tol
    return converged, converged, converged, false
end

function stride_solve(ssd::Data{R}, relaxation::Relaxation.AbstractRelaxation, data; verbose::Bool=false,
    tol::R=1e-8, maxiter::Integer=10,
    tol_init::R=1e-4, maxiter_init::Integer=1000, verbose_init::Bool=false,
    tol_sgsapg::R=1e-12, maxiter_sgsapg::Integer=1000, verbose_sgsapg::Bool=false,
    tol_lbfgs::R=1e-12, mem_lbfgs::Integer=10, maxiter_lbfgs::Integer=1000, verbose_lbfgs::Bool=false,
    ls_lbfgs=Optim.LineSearches.MoreThuente(),
    œÉ::R=10.,
    verbose_local::Bool=false, kwargs_local::Dict=Dict(),
    opti_local=begin
        @verbose_info("No local optimization specified; automatically constructing one (disable by passing opti_local=nothing).")
        setup_static = @elapsed(ol = poly_optimize(Val(:LANCELOT), poly_problem(relaxation); verbose=verbose_local,
            kwargs_local...))
        @verbose_info("Local optimization constructed in ", setup_static, " seconds.")
        ol
    end,
    opti_round=nothing) where {R<:Real}
    (tol_init > 0 && maxiter_init > 0 && tol > 0 && œÉ > 0) || throw(ArgumentError("Invalid arguments"))
    # A: output, [X‚ÇÅ, ...], Œ±, Œ≤ -> Œ± output + Œ≤ A(X‚ÇÅ, ...)
    # Astar: output, y, i, Œ±, Œ≤ -> Œ± output + Œ≤ X·µ¢(y)
    # AAstarinv: output, y -> ÃÉy
    Astar, b, C, X, S, y, tmpm = ssd.Astar, ssd.b, ssd.C, ssd.X, ssd.S, ssd.y, ssd.tmpm1
    # Input: Generate
    # - an initial point (X‚ÇÄ, y‚ÇÄ, S‚ÇÄ) via spadmm (i.e., Algorithm 4). Not necessarily feasible.
    spadmm!(ssd, œÉ, (1 + sqrt(5)) / 2, tol_init, maxiter_init, verbose_init)
    all(M -> isposdef(M, tol), S) || error("An initial point was generated that is not positive semidefinite")
    # - a nondecreasing positive sequence {œÉ‚Çñ}; choose œÉ‚Çñ ‚â• O(‚àök) to get convergence according to Theorem 2.
    #   > choose œÉ‚Çñ = sqrt(k) ... according to the IEEE paper, choose 10 (constant)
    # - a nonnegative sequence {œµ‚Çñ} such that {kœµ‚Çñ} is summable,
    #   > looks like it is not required
    # - a stopping criterion Œ∑: ùïä‚Çä‚Åø √ó ‚Ñù·µê √ó ùïä‚Çä‚Åø ‚Üí ‚Ñù‚Çä with a tolerance Tol > 0
    # - V = {X‚ÇÄ} if X‚ÇÄ is feasible, V = ‚àÖ otherwise. We don't store the Xs, but just their objectives
    #   > make it feasible by projecting onto the positive part as a first step.
    @verbose_info("Post-validating the initial point")
    for (X·µ¢, S·µ¢) in zip(X, S)
        psdproject!(X·µ¢)
        psdproject!(S·µ¢)
    end
    Vopt = dot(b, ssd.y)
    #   > now we have positive X and S, but feasibility also requires the linear constraints to be satisfied. Check it for the
    #     dual problem:
    Œ∑d = zero(R)
    for (Astar·µ¢, S·µ¢, C·µ¢, tmpm·µ¢) in zip(Astar, S, C, tmpm)
        copyto!(tmpm·µ¢, S·µ¢)
        mul!(tmpm·µ¢, Astar·µ¢, y, true, true)
        axpy!(-one(R), C·µ¢, tmpm·µ¢)
        Œ∑d += norm(tmpm·µ¢)
    end
    if Œ∑d > tol
        # no, we are infeasible. So Vopt is not an appropriate value for the last good result.
        @verbose_info("Initial candidate generated by ADMM+ is infeasible with error ", Œ∑d)
        Vopt = R(Inf)
    else
        @verbose_info("Initial candidate generated by ADMM+ is feasible")
    end
    CX = Vopt
    # - a positive integer r ‚àà [1, n] - this is the rank
    # - a positive constant œµ > 0. According to the IEEE paper, choose 1e-12

    if !isnothing(opti_local)
        groupings = Relaxation.groupings(relaxation)
        maxfn = gr -> maximum(length, gr, init=0)
        tmpmon = Vector{R}(undef, max(maxfn(groupings.obj), maximum(maxfn, groupings.nonnegs, init=0),
            maximum(maxfn, groupings.psds, init=0)))
        sqrt2 = sqrt(R(2))
    end
    local lbfgs_fg!
    let Z=X, A=ssd.A, Astar=ssd.Astar, b=ssd.b
        # œï(Œæ) := ‚ÄñŒ†(A* Œæ + Z)‚Äñ^2/2 - ‚ü®b, Œæ‚ü©
        # ‚àáœï(Œæ) = AŒ†(A* Œæ + Z) - b
        lbfgs_fg! = Optim.only_fg!((F, G, Œæ) -> begin
            œï = zero(R)
            isnothing(G) || (G .= .-b)
            for (i, (tmpm·µ¢, Z·µ¢, A·µ¢, Astar·µ¢)) in enumerate(zip(tmpm, Z, A, Astar))
                copyto!(tmpm·µ¢, Z·µ¢)
                mul!(tmpm·µ¢, Astar·µ¢, Œæ, true, true)
                psdproject!(tmpm·µ¢)
                isnothing(F) || (œï += norm(tmpm·µ¢)^2)
                isnothing(G) || mul!(G, A·µ¢, tmpm·µ¢, true, true)
            end
            isnothing(F) || return R(1/2)*œï - dot(b, Œæ)
            nothing
        end)
    end
    lbfgs = Optim.LBFGS(m=mem_lbfgs, linesearch=ls_lbfgs)
    lbfgs_options = Optim.Options(g_tol=tol_lbfgs, show_trace=verbose_lbfgs, show_every=50, iterations=maxiter_lbfgs)
    lbfgs_d = Optim.OnceDifferentiable(lbfgs_fg!, y, zero(R))
    lbfgs_state = PolyLBFGSState(Optim.LBFGSState(
        y, # x
        similar(y), # x_previous
        similar(y), # g_previous
        Vector{R}(undef, mem_lbfgs), # rho
        [similar(y) for _ in 1:mem_lbfgs], # dx_history
        [similar(y) for _ in 1:mem_lbfgs], # dg_history
        fill(R(NaN), length(y)), # dx
        fill(R(NaN), length(y)), # # dg
        fill(R(NaN), length(y)), # u
        R(NaN), # f_x_previous
        similar(y), # twoloop_q
        Vector{R}(undef, mem_lbfgs), # twoloop_alpha
        0, # pseudo_iteration
        similar(y), # s
        similar(y), # x_ls
        one(R)
    ), ssd, tol_lbfgs, false)

    status = :max_iter
    œÉ‚Çñ = zero(R)
    # Iterate the following steps for k = 1, ...:
    @inbounds for k in 1:maxiter
        œÉ‚Çñ += œÉ
        # Step 1 (Projection). Compute (ÃÑX‚Çñ, y‚Çñ, S‚Çñ) satisfying (5) and (6), where ÃÑX‚Çñ is an inexact projection of X‚Çñ‚Çã‚ÇÅ - œÉ‚Çñ C onto
        # the primal feasible set of the SDP, by running algorithm sgsapg and algorithm mlbfgs, warmstarted by (y‚Çñ‚Çã‚ÇÅ, S‚Çñ‚Çã‚ÇÅ).
        # ‚Üí Section 4
        @inbounds for (X·µ¢, C·µ¢) in zip(X, C)
            axpy!(-œÉ‚Çñ, C·µ¢, X·µ¢)
        end
        sgsapg!(ssd, tol_sgsapg, maxiter_sgsapg, verbose_sgsapg)
        verbose_lbfgs && @verbose_info("Entering limited-memory BFGS method")
        # we have to duplicate the behavior of initial_state here
        Optim.reset!(lbfgs, lbfgs_state, lbfgs_d, y)
        copyto!(lbfgs_state.state.x_previous, y) # not strictly necessary (else it would be part of reset!)
        lbfgs_state.success = false
        Optim.optimize(lbfgs_d, y, lbfgs, lbfgs_options, lbfgs_state)
        if !lbfgs_state.success
            for (W·µ¢, Z·µ¢, Astar·µ¢) in zip(S, X, Astar)
                copyto!(W·µ¢, Z·µ¢)
                mul!(W·µ¢, Astar·µ¢, y, -one(R), -one(R))
                psdproject!(W·µ¢)
            end
            Œ∑proj(ssd, S, y) # to calculate X (which is stored in tmpm2)
        end
        # else checking the termination criteria already constructed the state
        # We got an updated y and S, now we have to construct X = Œ†(A*Œæ + Z), where Z = X‚Çñ‚Çã‚ÇÅ - œÉ‚Çñ C ‚â° X

        for (X·µ¢, Astar·µ¢) in zip(X, Astar)
            mul!(X·µ¢, Astar·µ¢, y, true, true)
            psdproject!(X·µ¢)
        end

        # (ÃÑX‚Çñ, y‚Çñ, S‚Çñ) = (X(W, Œæ), 1/œÉ‚Çñ Œæ, 1/œÉ‚Çñ W) [note that Œæ and y are aliased, as well as W and S]
        œÉinv‚Çñ = inv(œÉ‚Çñ)
        rmul!(y, œÉinv‚Çñ)
        rmul!.(S, œÉinv‚Çñ)

        # Step 2 (Certification). If Œ∑(ÃÑX‚Çñ, y‚Çñ, S‚Çñ) < Tol, output (ÃÑX‚Çñ, y‚Çñ, S‚Çñ) and stop.
        CX, by, Œ∑p, Œ∑d, Œ∑g = Œ∑(ssd, Val(true))
        (verbose_sgsapg || !isnothing(opti_local) || isone(k)) &&
            @verbose_info("Iteration | Primal objective | Dual objective | Primal infeasibility | Dual infeasibility | Relative duality gap")
        @verbose_info(@sprintf("%9d | %16g | %14g | %20g | %18g | %20g", k, CX, by, Œ∑p, Œ∑d, Œ∑g))
        if max(Œ∑p, Œ∑g, Œ∑d) ‚â§ tol
            @verbose_info("Successfully converged.")
            Vopt = CX
            status = :ok
            break
        end

        # Step 3 (Acceleration): Compute ÃÇX‚Çñ via rounding, local NLP optimization and lifting, starting from ÃÑX‚Çñ:
        if !isnothing(opti_local)
            # Original description:
            # ÃÑX‚Çñ = ‚àë·µ¢ Œª·µ¢ v·µ¢ v·µ¢·µÄ (truncate after r)
            # ÃÑx‚Çñ·µ¢ = rounding(v·µ¢). Problem-dependent, but can first normalize v·µ¢ such that the constant component is 1, then
            #                     extract the order-1 entries. If easily possible, project to the feasible set.
            # This would be problematic, to the claim in the Yang paper that "the algorithms...can be extended...in a
            # straightforward way to solve sparse relaxations. We now have multiple X, contrary to the Yang paper. If those
            # just came from constraints, we would simply disregard all but the moment matrix. However, we allow for generic
            # sparsity structure, i.e., there may be multiple sub-moment matrices with possibly overlapping variables. To
            # account for this, we will use our solution extraction heuristic. This is different from the eigenvector
            # decomposition method!
            # (Also note that the poor-man's option to add a dense first-order moment matrix for each clique suggested
            # previously by Wang et al. [CS-TSSOS] appears to be rather pointless.)
            @verbose_info("Entering local optimization")
            res = Result(relaxation, :StrideMoment, NaN64, nothing, :interrupted, CX)
            res.moments = MomentVector(relaxation, data, X, nothing)
            solutions = isnothing(opti_round) ? poly_solutions(:heuristic, res; verbose) : opti_round(res; verbose)
            # ÃÇx‚Çñ·µ¢ = nlp(ÃÑx‚Çñ·µ¢)
            # ÃÇx‚Çñ = argmin p(ÃÇx‚Çñ·µ¢) over all { ÃÇx‚Çñ·µ¢ : i = 1, ..., r }
            best = typemax(R)
            local best_solution
            for solution in solutions
                for i in eachindex(solution)
                    if isnan(solution[i])
                        solution[i] = rand()
                    end
                end
                val, pos = opti_local(solution) # note: we REQUIRE solutions returned by opti_local to be feasible (should
                                                # return (Inf, arbitrary) if not)!
                # opti_local is allowed to modify its parameter, e.g., by writing pos into solution.
                if val < best
                    best_solution = pos
                    best = val
                end
            end
            # ÃÇX‚Çñ = proj(monomial lifting of ÃÇx‚Çñ)
            # Step 4 (Policy). Choose the better candidate in {ÃÑX‚Çñ, ÃÇX‚Çñ} to update X‚Çñ:
            # X‚Çñ = ÃÇX‚Çñ if ‚ü®C, ÃÇX‚Çñ‚ü© < min( ‚ü®C, ÃÑX‚Çñ‚ü©, min_{X ‚àà V} { ‚ü®C, X‚ü© : X ‚àà V } ) - œµ ‚àß ÃÇX‚Çñ feasible
            # We decide to be a bit more careful here, which is necessary due to the absence of BFGS. Our SGSAPG result may be
            # very infeasible, so it is not fair to compare to this value unless both feasibilities are sufficienty well under
            # control.
            if Œ∑p < 100tol && Œ∑d < 100tol
                bound = min(Vopt, CX)
            else
                bound = Vopt
            end
            if best < bound - R(1//100_000_000_000)
                @verbose_info("Found better optimization candidate with objective ", best)
                i = 1
                @inbounds for grtype in ((groupings.obj,), groupings.nonnegs, groupings.psds),
                    groups::typeof(groupings.obj) in grtype, gr·µ¢ in groups
                    tmpmon·µ¢ = @view(tmpmon[1:length(gr·µ¢)])
                    tmpmon·µ¢ .= (best_solution,) .|> gr·µ¢
                    X·µ¢ = X[i]
                    idx = 1
                    for col in 1:length(gr·µ¢)
                        X·µ¢[idx] = tmpmon·µ¢[col]^2
                        idx += 1
                        for row in col+1:length(gr·µ¢)
                            X·µ¢[idx] = sqrt2 * tmpmon·µ¢[col] * tmpmon·µ¢[row]
                            idx += 1
                        end
                    end
                    i += 1
                end
                # If X‚Çñ = ÃÇX‚Çñ, set V ‚Üê V ‚à™ { ÃÇX‚Çñ }
                Vopt = best
                CX = best
            else
                @verbose_info("Optimization did not improve")
            end
        end
        #    = ÃÑX‚Çñ otherwise (nop, we already did this)
    end
    @verbose_info("Optimization completed")

    return status, CX
end

function spadmm!(ssd::Data{R}, œÉ::R, Œ≥::R, tol::R, maxiter::Integer, verbose::Bool) where {R<:Real}
    @assert(0 ‚â§ Œ≥ ‚â§ 2 && œÉ > 0 && tol ‚â• 0 && maxiter > 0)
    A, Astar, AAstarinv, C, b, X, S, y = ssd.A, ssd.Astar, ssd.AAstarinv, ssd.C, ssd.b, ssd.X, ssd.S, ssd.y
    tmpm = ssd.tmpm1
    @verbose_info("Entering ADMM+ with penalty œÉ = ", œÉ, ", step length Œ≥ = ", Œ≥, ", and tolerance ", tol,
        "\nIteration | Primal objective | Dual objective | Primal infeasibility | Dual infeasibility | Relative duality gap")
    # Initialization: Initial points X‚ÇÄ = S‚ÇÄ = 0 ‚àà ùïä‚Åø
    fill!.(X, zero(R))
    fill!.(S, zero(R))
    œÉ‚Åª¬π = inv(œÉ)
    Œ≥œÉ = Œ≥ * œÉ
    local y
    # Iterate the following steps for k = 1, ...
    @inbounds for k in 1:maxiter
        # Step 1: Compute
        #    ÃÇy‚Çñ‚Çä‚ÇÅ = (A A*)‚Åª¬π(b/œÉ - A(X‚Çñ/œÉ + S‚Çñ - C))
        for (i, (X·µ¢, S·µ¢, C·µ¢, A·µ¢)) in enumerate(zip(X, S, C, A))
            # We don't need S·µ¢ any more, so let's overwrite it
            S·µ¢ .+= X·µ¢ .* œÉ‚Åª¬π
            axpy!(-one(R), C·µ¢, S·µ¢) # we use axpy! for all sparse changes instead of broadcasting, as this can exploit the
                                   # sparsity pattern. In contrast, axpy! seems to be much worse than broadcasting for dense
                                   # vectors.
            mul!(y, A·µ¢, S·µ¢, -one(R), !isone(i))
        end
        axpy!(œÉ‚Åª¬π, b, y)
        ldiv!(AAstarinv, y)
        # Step 2: Compute
        #    S‚Çñ‚Çä‚ÇÅ = ( Œ†(X‚Çñ + œÉ(A* ÃÇy‚Çñ‚Çä‚ÇÅ - C)) - (X‚Çñ + œÉ(A* ÃÇy‚Çñ‚Çä‚ÇÅ - C)) ) / œÉ
        #             ^ projection onto PSD, use LAPACK.syevx
        for (X·µ¢, S·µ¢, tmpm·µ¢, Astar·µ¢, C·µ¢) in zip(X, S, tmpm, Astar, C)
            mul!(tmpm·µ¢, Astar·µ¢, y, œÉ, false)
            axpy!(-œÉ, C·µ¢, tmpm·µ¢)
            tmpm·µ¢ .+= X·µ¢
            copyto!(S·µ¢, tmpm·µ¢)
            # We effectively project onto the (absolute of the) negative part, which is done by a positive projection, since
            # the positive part is expected to have low rank
            psdproject!(S·µ¢)
            S·µ¢ .= (S·µ¢ .- tmpm·µ¢) .* œÉ‚Åª¬π
        end
        # Step 3: Compute
        #    y‚Çñ‚Çä‚ÇÅ = (A A*)‚Åª¬π(b/œÉ - A(X‚Çñ/œÉ + S‚Çñ‚Çä‚ÇÅ - C))
        for (i, (X·µ¢, S·µ¢, C·µ¢, tmpm·µ¢, A·µ¢)) in enumerate(zip(X, S, C, tmpm, A))
            # This time, we need the S·µ¢ again (as well as the Xs), so we need the temporary storage
            tmpm·µ¢ .= X·µ¢ .* œÉ‚Åª¬π .+ S·µ¢
            axpy!(-one(R), C·µ¢, tmpm·µ¢)
            mul!(y, A·µ¢, tmpm·µ¢, -one(R), !isone(i))
        end
        axpy!(œÉ‚Åª¬π, b, y)
        ldiv!(AAstarinv, y)
        # Step 4: Compute
        #    X‚Çñ‚Çä‚ÇÅ = X‚Çñ + Œ≥ œÉ (S‚Çñ‚Çä‚ÇÅ + A* y‚Çñ‚Çä‚ÇÅ - C)
        for (X·µ¢, S·µ¢, C·µ¢, Astar·µ¢) in zip(X, S, C, Astar)
            X·µ¢ .+= Œ≥œÉ .* S·µ¢
            axpy!(-Œ≥œÉ, C·µ¢, X·µ¢)
            mul!(X·µ¢, Astar·µ¢, y, Œ≥œÉ, true)
        end
        # Until Œ∑(X‚Çñ‚Çä‚ÇÅ, y‚Çñ‚Çä‚ÇÅ, S‚Çñ‚Çä‚ÇÅ) ‚â§ tol (eq. 28)
        CX, by, Œ∑p, Œ∑d, Œ∑g = Œ∑(ssd, Val(true))
        success = max(Œ∑p, Œ∑g, Œ∑d) ‚â§ tol
        if verbose && (success || k == maxiter || isone(k % 20))
            @verbose_info(@sprintf("%9d | %16g | %14g | %20g | %18g | %20g", k, CX, by, Œ∑p, Œ∑d, Œ∑g))
        end
        # Output X‚Çñ‚Çä‚ÇÅ, y‚Çñ‚Çä‚ÇÅ, S‚Çñ‚Çä‚ÇÅ
        if success
            @verbose_info("Found an initial point up to the given tolerance")
            return
        end
    end
    @verbose_info("Terminated initial point finding due to maxiter")
end

function sgsapg!(ssd::Data{R}, tol::R, maxiter::Integer, verbose::Bool) where {R<:Real}
    @assert(tol ‚â• 0) # also all(W .‚™∞ 0), but this is too expensive to check
    A, Astar, AAstarinv, b, Z, W, Œæ = ssd.A, ssd.Astar, ssd.AAstarinv, ssd.b, ssd.X, ssd.S, ssd.y
    tmpm1, tmpm2, tmpv = ssd.tmpm1, ssd.tmpm2, ssd.tmpv
    @verbose_info("Entering sGS-based accelerated proximal gradient method\nIteration |     residue")
    # Initialization: Set ÃÉW‚ÇÅ = W‚ÇÄ and t‚ÇÅ = 1
    copyto!.(tmpm1, W)
    t = 1.
    # Iterate the following steps for k = 1, ...
    k = 1
    @inbounds while true
        # Step 1 (sGS update): Compute
        #    ÃÉŒæ‚Çñ = (A A*)‚Åª¬π(b - A(Z) - A(ÃÉW‚Çñ))
        for (i, (A·µ¢, Z·µ¢, tmpm1·µ¢)) in enumerate(zip(A, Z, tmpm1))
            tmpm1·µ¢ .+= Z·µ¢
            mul!(tmpv, A·µ¢, tmpm1·µ¢, -one(R), !isone(i))
        end
        tmpv .+= b
        copyto!.(tmpm1, W) # we need to back up the previous W
        ldiv!(Œæ, AAstarinv, tmpv)
        #    W‚Çñ = Œ†(-A*ÃÉŒæ‚Çñ - Z) = Œ†(A*Œæ‚Çñ + Z) - (A*Œæ‚Çñ + Z)
        for (Z·µ¢, W·µ¢, tmpm2·µ¢, Astar·µ¢) in zip(Z, W, tmpm2, Astar)
            copyto!(W·µ¢, Z·µ¢)
            mul!(W·µ¢, Astar·µ¢, Œæ, true, true)
            copyto!(tmpm2·µ¢, W·µ¢)
            psdproject!(W·µ¢)
            W·µ¢ .-= tmpm2·µ¢
        end
        #    Œæ‚Çñ = (A A*)‚Åª¬π(b - A(Z) - A(W‚Çñ))
        for (i, (A·µ¢, Z·µ¢, W·µ¢)) in enumerate(zip(A, Z, W))
            mul!(tmpv, A·µ¢, Z·µ¢, -one(R), !isone(i))
            mul!(tmpv, A·µ¢, W·µ¢, -one(R), true)
        end
        tmpv .+= b
        ldiv!(Œæ, AAstarinv, tmpv)

        # Until Œ∑proj(W‚Çñ, Œæ‚Çñ) ‚â§ tol
        Œ∑p = Œ∑proj(ssd, W, Œæ) # Œ∑proj will use tmpm2, tmpvlong, and tmpv as temporaries
        success = Œ∑p ‚â§ tol
        if verbose && (success || k == maxiter || isone(k % 20))
            @verbose_info(@sprintf("%9d | %11g", k, Œ∑p))
        end
        if success
            return # Output W‚Çñ, Œæ‚Çñ
        elseif k == maxiter
            @verbose_info("Maximum iteration count reached")
            return
        end

        # Step 2: Compute
        #    t‚Çñ‚Çä‚ÇÅ = (1 + sqrt(1 + 4t‚Çñ^2))/2
        newt = (one(R) + sqrt(one(R) + R(4)*t^2)) / R(2)
        #    ÃÉW‚Çñ‚Çä‚ÇÅ = W‚Çñ + (t‚Çñ-1)/t‚Çñ‚Çä‚ÇÅ (W‚Çñ - W‚Çñ‚Çã‚ÇÅ)
        frac = (t - one(R)) / newt
        t = newt
        # now W‚Çñ is W, W‚Çñ‚Çã‚ÇÅ is tmpm1, and we need to fill ÃÉW‚Çñ‚Çä‚ÇÅ, which will be tmpm1
        for (W·µ¢, tmpm1·µ¢) in zip(W, tmpm1)
            tmpm1·µ¢ .= (one(R) + frac) .* W·µ¢ .- frac .* tmpm1·µ¢
        end

        k += 1
    end
end

function Œ∑(ssd::Data{R}, ::Val{details}=Val(false)) where {R,details}
    A, Astar, C, b, X, S, y, tmpm, tmpv = ssd.A, ssd.Astar, ssd.C, ssd.b, ssd.X, ssd.S, ssd.y, ssd.tmpm1, ssd.tmpv
    # here, note that b = (1, 0, ..., 0)
    # Œ∑p(X) = ‚ÄñA(X) - b‚Äñ/(1 + ‚Äñb‚Äñ)
    for (i, (A·µ¢, X·µ¢)) in enumerate(zip(A, X))
        mul!(tmpv, A·µ¢, X·µ¢, true, !isone(i))
    end
    @inbounds tmpv .-= b
    Œ∑p = norm(tmpv) / (1 + norm(b))
    # Œ∑d(y, S) = ‚ÄñA*(y) + S - C‚Äñ/(1 + ‚ÄñC‚Äñ)
    # Œ∑g(X, y) = |‚ü®C, X‚ü© - ‚ü®b, y‚ü©|/(1 + |‚ü®C, X‚ü©| + |‚ü®b, y‚ü©|)
    Œ∑dnumsq, normCsq, CX, by = zero(R), zero(R), zero(R), dot(b, y)
    @inbounds for (S·µ¢, X·µ¢, C·µ¢, tmpm·µ¢, Astar·µ¢) in zip(S, X, C, tmpm, Astar)
        mul!(tmpm·µ¢, Astar·µ¢, y, true, false)
        tmpm·µ¢ .+= S·µ¢
        axpy!(-one(R), C·µ¢, tmpm·µ¢)
        CX += dot(C·µ¢, X·µ¢)
        Œ∑dnumsq += LinearAlgebra.norm(tmpm·µ¢)^2
        normCsq += LinearAlgebra.norm(C·µ¢)^2
    end
    Œ∑d = sqrt(Œ∑dnumsq) / (1 + sqrt(normCsq))
    Œ∑g = abs(CX - by) / (1 + abs(CX) + abs(by))
    return details ? (ssd.offset - CX, ssd.offset - by, Œ∑p, Œ∑d, Œ∑g) : max(Œ∑p, Œ∑d, Œ∑g)
end

function Œ∑proj(ssd::Data{R,<:Factorization,Fmt}, W, Œæ, recompute_AstarŒæplusZ::Bool=true) where {R,Fmt}
    A, Astar, b, Z, X = ssd.A, ssd.Astar, ssd.b, ssd.X, ssd.tmpm2
    tmpmvec, tmpv = ssd.tmpvlong, ssd.tmpv
    # X(W, Œæ) := A*Œæ + W + Z
    # Œ∑proj := max( ‚ÄñA(X(W, Œæ)) - b‚Äñ, ‚ÄñX(W, Œæ) - Œ†(A*Œæ + Z)‚Äñ )
    norm2 = zero(R)
    if recompute_AstarŒæplusZ
        @inbounds for (X·µ¢, W·µ¢, Z·µ¢, Astar·µ¢) in zip(X, W, Z, Astar)
            tmpm·µ¢ = let s = size(X·µ¢, 1)
                SPMatrix(s, view(tmpmvec, 1:packedsize(s)), Fmt)
            end
            copyto!(tmpm·µ¢, Z·µ¢)
            mul!(tmpm·µ¢, Astar·µ¢, Œæ, true, true)
            X·µ¢ .= tmpm·µ¢ .+ W·µ¢
            psdproject!(tmpm·µ¢)
            j = 0
            for q in 1:size(tmpm·µ¢, 2)
                @simd for k in 1:q-1
                    norm2 += 2(X·µ¢[j+k] - tmpm·µ¢[j+k])^2
                end
                norm2 += (X·µ¢[j+q] - tmpm·µ¢[j+q])^2
                j += q
            end
        end
    else
        # if this is called from the BFGS function, the algorithm guarantees that norm2 is indeed exactly zero. But we still
        # need X.
        @inbounds for (X·µ¢, W·µ¢, Z·µ¢, Astar·µ¢) in zip(X, W, Z, Astar)
            copyto!(X·µ¢, Z·µ¢)
            mul!(X·µ¢, Astar·µ¢, Œæ, true, true)
            X·µ¢ .+= W·µ¢
        end
    end
    for (i, (A·µ¢, X·µ¢)) in enumerate(zip(A, X))
        mul!(tmpv, A·µ¢, X·µ¢, true, !isone(i))
    end
    axpy!(-one(R), b, tmpv)
    norm1 = norm(tmpv)
    return max(norm1, sqrt(norm2))
end

end