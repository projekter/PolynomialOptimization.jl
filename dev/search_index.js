var documenterSearchIndex = {"docs":
[{"location":"backend.html","page":"Backend","title":"Backend","text":"CurrentModule = PolynomialOptimization.Solver\nCollapsedDocStrings = true","category":"page"},{"location":"backend.html#Backend","page":"Backend","title":"Backend","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"PolynomialOptimization mostly uses external solvers that have Julia bindings or are implemented in Julia, but it also provides own solver implementations particularly for the purpose of polynomial optimization. This page is only relevant if you intend to implement an interface between PolynomialOptimization and a new solver or if you want to provide missing functionality for an existing solver. It is of no relevance if you only want to use the existing solvers.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"warning: Warning\nThe package does not introduce any hard dependencies on the external solvers. Therefore, you may or may not decide to install them on your system. Instead, the solvers are addressed as weak dependencies. This means that you have to load the solver Julia package manually before you are able to use it for optimization.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"PolynomialOptimization also provides an interface that can be implemented if another solver should be supported. This consists of just a few methods for the various functions.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"info: MathOptInterface\nWhy do we re-invent the wheel and don't just use MathOptInterface? This would immediately bring compatibility with a huge number of solvers that are available under Julia.This would certainly be possible. It might even be a not-too-bad idea to provide an automatic fallback to MathOptInterface in the future for solvers that are not supported natively.However, MathOptInterface is enormous in its feature range. Considering that, it is amazingly fast. But in PolynomialOptimization, only a very small subset of all the features is required, and a lot of additional assumptions on how the problem is constructed can be made. This allows to use the solver's API in a most efficient way, which typically is not the way in which implementations of MathOptInterface would address the solver.Additionally, in some situations, a lot of very similar sub-problems need to be solved; in these cases, PolynomialOptimization's own interface allows to keep the optimizer task alive and just do the tiny modification instead of setting things up from the start again - which for MathOptInterface only works if a solver is implemented in a particular way.This focus on efficiency is very important as (relevant) polynomial optimization problems are huge. It is just not possible to waste time and memory in bookkeeping that is not really needed.","category":"page"},{"location":"backend.html#[poly_optimize](@ref)","page":"Backend","title":"poly_optimize","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The optimization of polynomial problems requires a solver that understands linear and semidefinite constraints. All functions in this section are defined (and exported) in the submodule PolynomialOptimization.Solver.","category":"page"},{"location":"backend.html#Solver-interface","page":"Backend","title":"Solver interface","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"In general, a solver implementation can do whatever it wants; it just needs to implement the poly_optimize method with the appropriate Val-wrapped solver method as its first parameter. However, it is very helpful to just do some basic setup such as creating the solver object in this function and delegate all the work of setting up the actual problem to moment_setup! or sos_setup!. In order to do so, a solver implementation should create a new type that contains all the relevant data during setup of the problem. Usually, a solver falls in one of three categories:","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Problem data has to be supplied in matrix/vector form; in this case, the new type should be a descendant of AbstractSparseMatrixSolver. Usually, open-source solvers fall in this category.\nProblem data is constructed incrementally via various calls to API functions of the solver, which does not provide access to its internals. In this case, the new type should be a descendant of AbstractAPISolver. Usually, commercial solvers fall in this category.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"However, it is not required that the type is in fact a subtype of either of those; the most general possible supertype is AbstractSolver, which does not make any assumptions or provide any but the most skeleton fallback implementations and a default for mindex.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"AbstractSolver\nmindex","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.AbstractSolver","page":"Backend","title":"PolynomialOptimization.Solver.AbstractSolver","text":"AbstractSolver{T,V<:Real}\n\nAbstract supertype for any solver state. T is the type that is returned by calling mindex with such a state; V is the type of the coefficients used in the solver. Using the default implementation for mindex, T will be UInt. Most likely, V will be Float64.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#PolynomialOptimization.Solver.mindex","page":"Backend","title":"PolynomialOptimization.Solver.mindex","text":"mindex(::AbstractSolver{T}, monomials::IntMonomialOrConj...)::T\n\nCalculates the index that the product of all monomials will have in the SDP represented by state. The default implementation calculates the one-based monomial index according to a dense deglex order and returns an UInt. The returned index is arbitrary as long as it is unique for the total monomial.\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Every implementation of poly_optimize should return a tuple that contains some internal state of the solver as well as the optimal value and the status of the solver. A method for issuccess should then translate this status into a simple boolean, where deciding on ambiguities (near success) is up to the solver implementation.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"poly_optimize(::Val, ::AbstractRelaxation, ::RelaxationGroupings)\nissuccess(::Val, ::Any)","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.poly_optimize-Tuple{Val, PolynomialOptimization.Relaxation.AbstractRelaxation, PolynomialOptimization.Relaxation.RelaxationGroupings}-backend","page":"Backend","title":"PolynomialOptimization.Solver.poly_optimize","text":"poly_optimize(::Val{method}, relaxation::AbstractRelaxation,\n    groupings::RelaxationGroupings; representation, verbose, kwargs...)\n\nThis is the central entry point that a solver has to implement. It has to carry out the optimization and must return a tuple of three values:\n\nAn internal state that can be used later on to access the solver again to extract solutions or (if possible) reoptimization.\nThe success status as returned by the solver and understood by the issuccess implementation.\nThe minimum value of the objective.\n\nSee also moment_setup!, sos_setup!.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#LinearAlgebra.issuccess-Tuple{Val, Any}-backend","page":"Backend","title":"LinearAlgebra.issuccess","text":"issuccess(::Val{method}, status)\n\nA solver must implement this method for all of its possible methods to indicate whether a status status signifies success.\n\n\n\n\n\n","category":"method"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Once a solver has been implemented, it should add its solver symbol to the vector solver_methods, which enables this solver to be chosen automatically. Apart from the exact specification :<solvername>Moment or :<solvername>SOS, a short form :<solvername> that chooses the recommended method should also be implemented. For this, the @solver_alias macro can be used. When details on the solution data a requested, the extract_moments, extract_sos, or extract_info function is called, where at least the former two have to be implemented for each solver:","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"extract_moments\nextract_sos\nextract_sos_prepare\nextract_info","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.extract_moments","page":"Backend","title":"PolynomialOptimization.Solver.extract_moments","text":"extract_moments(relaxation::AbstractRelaxation, state)\n\nExtracts a MomentVector from a solved relaxation. The state parameter is the first return value of the poly_optimize call by the solver. This function is only called once for each result; the output is cached.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.extract_sos","page":"Backend","title":"PolynomialOptimization.Solver.extract_sos","text":"extract_sos(relaxation::AbstractRelaxation, state, ::Val{type},\n    index::Union{<:Integer,<:AbstractUnitRange}, rawstate) where {type}\n\nExtracts data that contains the raw solver information about the SOS data contained in the result. For moment optimizations, this corresponds to the dual data; for SOS optimizations, this is the primal data. rawstate is the return value of the preceding call to extract_sos_prepare (by default, nothing). Note that the SOS data may be queried in any order, partially or completely.\n\nThe parameters type and index indicates which constraint/variable the data corresponds to. type is a symbol, index is the range of indices within constraints of the same type, although both the type as well as the interpretation of index may change by providing custom definitions for addtocounter! or using the macros @counter_atomic and @counter_alias.\n\nThe return value of this function should be a scalar, vector, or matrix, depending on what data was requested. The following relations should hold:\n\ntype result type\n:fix (moment only) vector\n:free (SOS only) vector\n:nonnegative vector\n:quadratic vector\n:rotated_quadratic vector\n:psd vector[1] or matrix\n:psd_complex vector[1][2] or matrix\n:dd vector[1] or matrix\n:dd_complex vector[1][2] or matrix\n:lnorm vector\n:lnorm_complex vector[2]\n:sdd vector\n:sdd_complex vector[2]\n\ninfo: Info\nIt is guaranteed that the range that is queries using index always corresponds to data that was added contiguously, with no other cones interspersed.\n\n[1]: If the return type is a vector, psd_indextype should be defined on state, and it must return a   PSDIndextypeVector. The scaling property of the index type will automatically be inverted, so that what was   sqrt2 before now becomes frac1sqrt2.\n\n[2]: Complex values can be treated either by returning a vector of Complex element type, or by returning a real-valued   vector where the diagonals (PSD/DD/SDD)/first elements (ell-norm) have a single entry and off-diagonals two.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.extract_sos_prepare","page":"Backend","title":"PolynomialOptimization.Solver.extract_sos_prepare","text":"extract_sos_prepare(relaxation::AbstractRelaxation, state)\n\nPrepares for one or multiple calls of extract_sos. The return value will be passed to the function as an argument. This is particularly relevant for solvers which don't allow the extraction of subsets of the data, but only the whole vector: Retrieve it here, then pass it as an output. The default implementation does nothing.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.extract_info","page":"Backend","title":"PolynomialOptimization.Solver.extract_info","text":"extract_info(state)\n\nReturns the internal information on the problem that was given back by moment_setup! or sos_setup!. There is a default implementation that returns the value of the property info on state (which also works if the original state is wrapped as the first element of a tuple).\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"In order to relate aspects of the problem with data in the solver, the cones that are added are counted. This works automatically, keeping a separate counter for every type of cone and counting vector-valued cones (which are most) with their actual length. This behavior can be customized:","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"@counter_alias\n@counter_atomic\naddtocounter!\nCounters","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.@counter_alias","page":"Backend","title":"PolynomialOptimization.Solver.@counter_alias","text":"@counter_alias(::Type{<:AbstractSolver}, counter, alias)\n\nDefines the addtocounter! function in such a way that contraints of type counter instead only affect the counter alias. Do not use @counter_atomic together with this macro on the same counter.\n\ncounter may either be a Symbol, a tuple of Symbols, or the value Any. Note that Any has weakest precendence, irrespective of when the macro was called.\n\nwarning: Warning\nRegardless of whether counter or alias where made atomic before, after this macro, counter will not be so (although it shares the same counter as alias). This may or may not be desirable (most likely not), so always make atomic counters explicit using @counter_atomic, which allows the definition of aliases.\n\n\n\n\n\n","category":"macro"},{"location":"backend.html#PolynomialOptimization.Solver.@counter_atomic","page":"Backend","title":"PolynomialOptimization.Solver.@counter_atomic","text":"@counter_atomic(::Type{<:AbstractSolver}, counter[, alias])\n\nDefines the addtocounter! function in such a way that a multi-dimensional constraint of type counter is always counted as a single entry. This macro may be called at most once for each counter when defining a solver. As a consequence, extract_sos will may have an Int or a UnitRange{Int} (if multiple constraints are required, which will never be the case for matrix cones) as index parameter for the counter type.\n\nIf the alias parameter is present, whenever the type counter is encountered, the counter of type alias is incremented instead. Do not use @counter_alias together with this macro on the same counter.\n\ncounter may either be a Symbol, a tuple of Symbols, or the value Any. Note that Any has weakest precedence, irrespective of when the macro is called.\n\n\n\n\n\n","category":"macro"},{"location":"backend.html#PolynomialOptimization.Solver.addtocounter!","page":"Backend","title":"PolynomialOptimization.Solver.addtocounter!","text":"addtocounter!(state, counters::Counters, ::Val{type}, dim::Integer) where {type} -> Int\naddtocounter!(state, counters::Counters, ::Val{type}, num::Integer,\n    dim::Integer) where {type} -> UnitRange{Int}\n\nIncrements the internal information about how many constraints/variables of a certain type were already added. Usually, a solver implementation does not have to overwrite the default implementation; but it might be useful to do so if some types are for example counted in a single counter or bunch of added elements counts as a single entry. dim is the length of the conic constraint or variable, while num indicates that multiple such constraints or variables of the same type are added. For a list of possible symbols type, see the documentation for extract_sos.\n\nSee also Counters, @counter_alias, @counter_atomic.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.Counters","page":"Backend","title":"PolynomialOptimization.Solver.Counters","text":"Counters\n\nMutable struct that keeps track of the number of conic constraints/variables currently in use. Every possible symbol listed in the documentation for extract_sos has an Int field of the same name.\n\n\n\n\n\n","category":"type"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Using this information, an additional implementation may be provided for a faster re-optimization of the same problem:","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"poly_optimize(::Val, ::Any, ::AbstractRelaxation, ::RelaxationGroupings)","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.poly_optimize-Tuple{Val, Any, PolynomialOptimization.Relaxation.AbstractRelaxation, PolynomialOptimization.Relaxation.RelaxationGroupings}","page":"Backend","title":"PolynomialOptimization.Solver.poly_optimize","text":"poly_optimize(::Val{method}, oldstate, relaxation::AbstractRelaxation,\n    groupings::RelaxationGroupings; representation, verbose, kwargs)\n\nA solver that supports re-optimization of an already optimized problem with changed rotations on the DD and SDD representations should implement this method. It is guaranteed that only the rotations change, and only in a structure-preserving way (diagonal and nondiagonal rotations will not interchange). The return value is as documented in poly_optimize, and the oldstate parameter holds the first return value of the previous call to poly_optimize.\n\n\n\n\n\n","category":"method"},{"location":"backend.html","page":"Backend","title":"Backend","text":"While this page documents in detail how a new solver can be implemented, the explanation is far more extensive than an actual implementation. In order to implement a new solver, it is therefore recommended to first determine the category in which it falls, then copy and modify an appropriate existing implementation.","category":"page"},{"location":"backend.html#[AbstractSparseMatrixSolver](@ref)","page":"Backend","title":"AbstractSparseMatrixSolver","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"This solver type accumulates data in a COO matrix-like format. The callback functions can use append! to quickly add given data to the temporary storage. However, the format is not yet suitable for passing data to the solver, as all monomials are densely indexed. Therefore, in a postprocessing step, the COO matrices have to be converted to CSC matrices with continuous monomial indices using coo_to_csc!. After the optimization is done, the optimal moment vector (the decision variables for moment optimization, the constraint duals for SOS optimization) can be constructed using MomentVector.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"AbstractSparseMatrixSolver\nSparseMatrixCOO\nappend!(::SparseMatrixCOO{I,K,V,Offset}, ::Indvals{K,V}) where {I<:Integer,K<:Integer,V<:Real,Offset}\nappend!(::SparseMatrixCOO{I,K,V,Offset}, ::IndvalsIterator{K,V}) where {I<:Integer,K<:Integer,V<:Real,Offset}\ncoo_to_csc!(::Union{SparseMatrixCOO{I,K,V,Offset},<:Tuple{AbstractVector{K},AbstractVector{V}}}...) where {I<:Integer,K<:Integer,V<:Real,Offset}\nMomentVector(::AbstractRelaxation{<:Problem{<:IntPolynomial{<:Any,Nr,Nc}}}, ::Vector{V}, ::K, ::SparseMatrixCOO{<:Integer,K,V,Offset}, ::SparseMatrixCOO{<:Integer,K,V,Offset}...) where {Nr,Nc,K<:Integer,V<:Real,Offset}","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.AbstractSparseMatrixSolver","page":"Backend","title":"PolynomialOptimization.Solver.AbstractSparseMatrixSolver","text":"AbstractSparseMatrixSolver{I<:Integer,K<:Integer,V<:Real}\n\nSuperclass for a solver that requires its data in sparse matrix form. The data is aggregated in COO form using append! and can be converted to CSC form using coo_to_csc!. The type of the indices in final CSC form is I, where the monomials during construction will be represented by numbers of type K. Any type inheriting from this class is supposed to have a field slack::K which is initialized to -one(K) if K is signed or typemax(K) if it is unsigned.\n\nSee also SparseMatrixCOO.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#PolynomialOptimization.Solver.SparseMatrixCOO","page":"Backend","title":"PolynomialOptimization.Solver.SparseMatrixCOO","text":"SparseMatrixCOO{I<:Integer,K<:Integer,V<:Real,Offset}\n\nRepresentation of a sparse matrix in COO form. Fields are rowinds::FastVec{I}, moninds::FastVec{K} (where K is of the type returned by monomial_index), and nzvals::FastVec{V}. The first row/column for the solver has index Offset (of type I).\n\nIf this matrix is used in the context of a vectorized COO matrix, the fields are rows, cols, and vals with K === I.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#Base.append!-Union{Tuple{Offset}, Tuple{V}, Tuple{K}, Tuple{I}, Tuple{PolynomialOptimization.Solver.SparseMatrixCOO{I, K, V, Offset}, PolynomialOptimization.Solver.Indvals{K, V}}} where {I<:Integer, K<:Integer, V<:Real, Offset}","page":"Backend","title":"Base.append!","text":"append!(coo::SparseMatrixCOO, indvals::Indvals)\n\nAppends the data given in indvals into the next row in coo. Returns the index of the row that was added.\n\nSee also Indvals.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#Base.append!-Union{Tuple{Offset}, Tuple{V}, Tuple{K}, Tuple{I}, Tuple{PolynomialOptimization.Solver.SparseMatrixCOO{I, K, V, Offset}, PolynomialOptimization.Solver.IndvalsIterator{K, V, L, VT, VV} where {L, VT<:AbstractVector{K}, VV<:AbstractVector{V}}}} where {I<:Integer, K<:Integer, V<:Real, Offset}","page":"Backend","title":"Base.append!","text":"append!(coo::SparseMatrixCOO, indvals::IndvalsIterator)\n\nAppends the data given in indvals into successive rows in coo (first(indvals) to the first row, the next to the second, ...). Returns the range of indices of all rows that were added.\n\nSee also IndvalsIterator.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.coo_to_csc!-Union{Tuple{Vararg{Union{Tuple{AbstractVector{K}, AbstractVector{V}}, PolynomialOptimization.Solver.SparseMatrixCOO{I, K, V, Offset}}}}, Tuple{Offset}, Tuple{V}, Tuple{K}, Tuple{I}} where {I<:Integer, K<:Integer, V<:Real, Offset}","page":"Backend","title":"PolynomialOptimization.Solver.coo_to_csc!","text":"coo_to_csc!(coo::Union{SparseMatrixCOO{I,K,V}},Tuple{FastVec{K},FastVec{V}}}...)\n\nConverts sparse COO matrix or vector representations, where the monomial indices of the coo matrices or the entries of the vectors can be arbitrarily sparse, to a CSC-based matrix representation with continuous columns, and the vectors are converted to dense ones. No more than two matrices may be supplied. The input data may be mutated; and this mutated data must be passed on to MomentVector. The following values are returned:\n\nnumber of distinct columns\nfor each input, if it is a matrix, a tuple containing the colptr, rowval, nzval vectors\nfor each input, if it is a vector, the corresponding dense vector\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.MomentVector-Union{Tuple{Offset}, Tuple{V}, Tuple{K}, Tuple{Nc}, Tuple{Nr}, Tuple{PolynomialOptimization.Relaxation.AbstractRelaxation{<:PolynomialOptimization.Problem{<:PolynomialOptimization.IntPolynomials.IntPolynomial{var\"#s3\", Nr, Nc, M} where {var\"#s3\", M<:(PolynomialOptimization.IntPolynomials.IntMonomialVector{Nr, Nc, I, E, T} where {I<:Integer, E, T<:(PolynomialOptimization.IntPolynomials.IntMonomial{Nr, Nc, I})})}}}, Vector{V}, K, PolynomialOptimization.Solver.SparseMatrixCOO{<:Integer, K, V, Offset}, Vararg{PolynomialOptimization.Solver.SparseMatrixCOO{<:Integer, K, V, Offset}}}} where {Nr, Nc, K<:Integer, V<:Real, Offset}","page":"Backend","title":"PolynomialOptimization.MomentVector","text":"MomentVector(relaxation::AbstractRelaxation, moments::Vector{<:Real},\n    slack::Integer, coo::SparseMatrixCOO...)\n\nGiven the moments vector as obtained from a AbstractSparseMatrixSolver solver, convert it to a MomentVector. Note that this function is not fully type-stable, as the result may be based either on a dense or sparse vector depending on the relaxation. To establish the mapping between the solver output and the actual moments, all the column-sorted COO data (i.e., as returned by coo_to_csc!) used in the problem construction needs to be passed on. slack must contain the current value of the slack field of the AbstractSparseMatrixSolver.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#[AbstractAPISolver](@ref)","page":"Backend","title":"AbstractAPISolver","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"This solver type accumulates the data continuously by the means of API calls provided by the solver. The monomials therefore have to have contiguously defined indices from the beginning. This is done by internal bookkeeping; however, it requires the implementation of an additional callback function append! to add new monomials to the solver. After the optimization is done, the optimal moment vector (the decision variables for moment optimization, the constraint duals for SOS optimization) can be constructed using MomentVector.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"AbstractAPISolver\nappend!(::AbstractAPISolver{K}, ::K) where {K<:Integer}\nMomentVector(::AbstractRelaxation{<:Problem{<:IntPolynomial{<:Any,Nr,Nc}}}, ::Vector{V}, ::AbstractAPISolver{K}) where {Nr,Nc,K<:Integer,V<:Real}","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.AbstractAPISolver","page":"Backend","title":"PolynomialOptimization.Solver.AbstractAPISolver","text":"AbstractAPISolver{K<:Integer,T,V} <: AbstractSolver{T,V}\n\nSuperclass for a solver that requires new variables/constraints to be added via API calls. Solvers that are of this type must implement append! in such a way that they directly add a variable (moment-case) to or constraint (SOS-case) to the solver. Concrete types that inherit from AbstractAPISolver must have a property mon_to_solver::Dict{FastKey{K},T}.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#Base.append!-Union{Tuple{K}, Tuple{PolynomialOptimization.Solver.AbstractAPISolver{K}, K}} where K<:Integer","page":"Backend","title":"Base.append!","text":"append!(solver::AbstractAPISolver{K}, key::K)\n\nAppends at least one new variable (moment-case) or constraint (SOS-case) to the solver state that represents the monomial given by key.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.MomentVector-Union{Tuple{V}, Tuple{K}, Tuple{Nc}, Tuple{Nr}, Tuple{PolynomialOptimization.Relaxation.AbstractRelaxation{<:PolynomialOptimization.Problem{<:PolynomialOptimization.IntPolynomials.IntPolynomial{var\"#s3\", Nr, Nc, M} where {var\"#s3\", M<:(PolynomialOptimization.IntPolynomials.IntMonomialVector{Nr, Nc, I, E, T} where {I<:Integer, E, T<:(PolynomialOptimization.IntPolynomials.IntMonomial{Nr, Nc, I})})}}}, Vector{V}, PolynomialOptimization.Solver.AbstractAPISolver{K}}} where {Nr, Nc, K<:Integer, V<:Real}","page":"Backend","title":"PolynomialOptimization.MomentVector","text":"MomentVector(relaxation::AbstractRelaxation, moments::Vector{<:Real},\n    solver::AbstractAPISolver)\n\nGiven the moments vector as obtained from an AbstractAPISolver, convert it to a MomentVector. Note that this function is not fully type-stable, as the result may be based either on a dense or sparse vector depending on the relaxation.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#Defining-solver-capabilities","page":"Backend","title":"Defining solver capabilities","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"There are some functions that should be implemented to tell PolynomialOptimization what kind of data the solver expects and which cones are supported; these should return constants.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"supports_rotated_quadratic\nsupports_quadratic\nsupports_psd_complex\nsupports_dd\nsupports_dd_complex\nsupports_lnorm\nsupports_lnorm_complex\nsupports_sdd\nsupports_sdd_complex\nPSDIndextype\nPSDIndextypeMatrixCartesian\nPSDIndextypeVector\nPSDIndextypeCOOVectorized\npsd_indextype\nnegate_fix\nnegate_free\nprepend_fix\nprepend_free","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.supports_rotated_quadratic","page":"Backend","title":"PolynomialOptimization.Solver.supports_rotated_quadratic","text":"supports_rotated_quadratic(::AbstractSolver)\n\nIndicates the solver support for rotated quadratic cones: if true, the rotated second-order cone 2x_1x_2 geq sum_i geq 3 x_i^2 is supported. The default implementation returns false.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.supports_quadratic","page":"Backend","title":"PolynomialOptimization.Solver.supports_quadratic","text":"supports_quadratic(::AbstractSolver)\n\nIndicates the solver support for the quadratic cone: if true, the second-order cone x_1^2 geq sum_i geq 2 x_i^2 is supported. The default implementation returns the same value as supports_rotated_quadratic.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.supports_psd_complex","page":"Backend","title":"PolynomialOptimization.Solver.supports_psd_complex","text":"supports_psd_complex(::AbstractSolver)\n\nThis function indicates whether the solver natively supports a complex-valued PSD cone. If it returns false (default), the complex-valued PSD constraints will be rewritten into real-valued PSD constraints; this is completely transparent for the solver. If the function returns true, the solver must additionally implement add_var_psd_complex! and add_constr_psd_complex!.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.supports_dd","page":"Backend","title":"PolynomialOptimization.Solver.supports_dd","text":"supports_dd(::AbstractSolver)\n\nThis function indicates whether the solver natively supports a diagonally-dominant cone (or its dual for the moment case). If it returns false (default), the constraint will be rewritten in terms of multiple ell_infty/ell_1 norm constraints (if supported, see supports_lnorm), together with slack variables and equality constraints. If these ell-norm constraints are also not supported, linear constraints will be used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.supports_dd_complex","page":"Backend","title":"PolynomialOptimization.Solver.supports_dd_complex","text":"supports_dd_complex(::AbstractSolver)\n\nThis function indicates whether the solver natively supports a complex-valued diagonally-dominant cone (or its dual for the moment case). If it returns false (default), the constraint will be rewritten in terms of quadratic constraints (if supported, see supports_quadratic) or multiple ell_infty/ell_1 norm constraints (if supported, see supports_lnorm_complex).\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.supports_lnorm","page":"Backend","title":"PolynomialOptimization.Solver.supports_lnorm","text":"supports_lnorm(::AbstractSolver)\n\nIndicates the solver support for ell_infty (in the moment case) and ell_1 (in the SOS case) norm cones: if true, the cone x_1 geq max_i geq 2 lvert x_irvert or x_1 geq sum_i geq 2 lvert x_irvert is supported. The default implementation returns false.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.supports_lnorm_complex","page":"Backend","title":"PolynomialOptimization.Solver.supports_lnorm_complex","text":"supports_lnorm_complex(::AbstractSolver)\n\nIndicates the solver support for complex-valued ell_infty (in the moment case) and ell_1 (in the SOS case) norm cones: if true, the cone x_1 geq max_i geq 2 lvertoperatornameRe x_i + mathrm i operatornameIm x_irvert or x_1 geq sum_i geq 2 lvertoperatornameRe x_i + mathrm i operatornameIm x_irvert is supported. The default implementation returns false.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.supports_sdd","page":"Backend","title":"PolynomialOptimization.Solver.supports_sdd","text":"supports_sdd(::AbstractSolver)\n\nThis function indicates whether the solver natively supports a scaled diagonally-dominant cone (or its dual for the moment case). If it returns false (default), the constraints will be rewritten in terms of multiple rotated quadratic or quadratic constraints, one of which must be supported (see supports_rotated_quadratic and supports_quadratic).\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.supports_sdd_complex","page":"Backend","title":"PolynomialOptimization.Solver.supports_sdd_complex","text":"supports_sdd_complex(::AbstractSolver)\n\nThis function indicates whether the solver natively supports a complex-valued scaled diagonally-dominant cone (or its dual for the moment case). If it returns false (default), the constraints will be rewritten in terms of multiple rotated quadratic or quadratic constraints, one of which must be supported (see supports_rotated_quadratic and supports_quadratic).\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.PSDIndextype","page":"Backend","title":"PolynomialOptimization.Solver.PSDIndextype","text":"PSDIndextype{Tri}\n\nUnion for all supported types in which a solver can represent a PSD matrix.\n\nSee also PSDIndextypeMatrixCartesian, PSDIndextypeVector.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#PolynomialOptimization.Solver.PSDIndextypeMatrixCartesian","page":"Backend","title":"PolynomialOptimization.Solver.PSDIndextypeMatrixCartesian","text":"PSDIndextypeMatrixCartesian(triangle, offset) <: PSDIndextype\n\nThe solver implements PSD matrix constraints by using a monolithic PSD matrix variable or an LMI-style representation. Entries from the variable are obtained (or put into the LMI) by using a cartesian index of two integers of the type parameter T of the AbstractSolver. This index represents one triangle of the matrix (the lower if triangle === :L, the upper if triangle === :U). The first entry has the index (offset, offset), typically either 0 or 1.\n\nIf this index type is used with primal_moment_setup!, the resulting data will be an iterator through SparseMatrixCOO. In this case, triangle === :F is also permitted, resulting in the full triangle.\n\ninfo: Info\nNote that even if only one triangle is indexed, it is assumed that the solver will by default populate the other triangle in a completely symmetric way. This corresponds to the typical behavior of solvers that expose PSD variables and allow accessing elements in them via sparse symmetric matrices where only one triangle is given, but the other half is implicit.\n\nSee also PSDMatrixCartesian.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#PolynomialOptimization.Solver.PSDIndextypeVector","page":"Backend","title":"PolynomialOptimization.Solver.PSDIndextypeVector","text":"PSDIndextypeVector(triangle[, scaling]) <: PSDIndextype\n\nThe solver implements PSD matrix constraints by demanding that the matrixization of a vector of decision variables be PSD. This index type is not permitted for use with primal_moment_setup!.\n\nIf triangle === :F, the vector is formed by stacking all the columns of the matrix. scaling should be omitted.\n\nIf triangle === :L, the columns of the lower triangle are assumed to be stacked and scaled, i.e., off-diagonal variables that enter the cone are implicitly multiplied by 1 / scaling in the matrix; so the coefficients will already be premultiplied by scaling (for the add_constr_psd! case) or by 1 / scaling (for the add_var_psd! case). The default value for scaling is sqrt2; however, the parameter has to be specified explictly in order to make sure the scaling has the correct type. Note: if no scaling is desired, the preferred value is true, which is equivalent to a multiplicative identity; however, the multiplication can be completely removed during compilation. This is because the value false, which would mean that alll off-diagonal entries are set to zero, is explicitly forbidden.\n\nIf triangle === :U, the columns of the upper triangle are assumed to be stacked and scaled.\n\nSee also IndvalsIterator.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#PolynomialOptimization.Solver.PSDIndextypeCOOVectorized","page":"Backend","title":"PolynomialOptimization.Solver.PSDIndextypeCOOVectorized","text":"PSDIndextypeCOOVectorized(triangle[, scaling], offset)\n\nThe solver implements constraints on a monolithic PSD matrix variable using row-by-row scalar products with the vectorized matrix. During vectorization, only the specified triangle is retained. This index type is valid only for the use with primal_moment_setup!.\n\nIf triangle === :F, the full matrix is stacked column-wise; scaling should be omitted.\n\nIf triangle === :L, the columns of the lower triangled are assumed to be stacked and scaled, i.e., off-diagonal variables that enter the cone are implicitly multiplied by 1 / scaling in the matrix; so the coefficients will already be premultiplied by 1 / scaling. If the solver internally works with the vectorized version, the appropriate value is probably sqrt2; if the solver automatically rewrites everything for full matrices, the appropriate value is either true or 2.\n\nIf triangle === :U, the column of the upper triangle are assumed to be stacked and scaled.\n\nThis all refers to the column index of the constraint matrix, where the row index is the index of the constraint. The data is supplied in COO form with specified offset and may be converted to CSC or CSR as desired.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#PolynomialOptimization.Solver.psd_indextype","page":"Backend","title":"PolynomialOptimization.Solver.psd_indextype","text":"psd_indextype(::AbstractSolver)\n\nThis function must indicate in which format the solver expects its data for PSD variables. The return type must be an instance of a PSDIndextype subtype.\n\nSee also PSDIndextypeMatrixCartesian, PSDIndextypeVector, PSDIndextypeCOOVectorized.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.negate_fix","page":"Backend","title":"PolynomialOptimization.Solver.negate_fix","text":"negate_fix(::AbstractSolver)\n\nDepending on the exact definition of equality constraints (where to put the minus), the dual solutions, i.e., the SOS decomposition, may yield wrong values; then define this function to return true, which will pass the equality constraints with opposite sign.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.negate_free","page":"Backend","title":"PolynomialOptimization.Solver.negate_free","text":"negate_free(::AbstractSolver)\n\nDepending on the exact definition of equality constraints (where to put the minus), the dual solutions, i.e., the SOS decomposition, may yield wrong values; then define this function to return true, which will flip the sign of free variables.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.prepend_fix","page":"Backend","title":"PolynomialOptimization.Solver.prepend_fix","text":"prepend_fix(::AbstractSolver)\n\nIf this method yields true (default), fixed constraints will be created before all others. If it is false, PSD (or nonnegative) constraints will be created first.\n\ninfo: Info\nNote that this does not imply a global order; if DD or SDD cones are used, fixed constraints may still appear at an arbitrary position. However, this method guarantees the order with respect to cones that will use the same monomial indices.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.prepend_free","page":"Backend","title":"PolynomialOptimization.Solver.prepend_free","text":"prepend_free(::AbstractSolver)\n\nIf this method yields true (default), free variables will be created before all others. If it is false, PSD (or nonnegative) variables will be created first.\n\ninfo: Info\nNote that this does not imply a global order; if DD or SDD cones are used, free variables may still appear at an arbitrary position. However, this method guarantees the order with respect to cones that will use the same monomial indices.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#Working-with-data-from-the-interface","page":"Backend","title":"Working with data from the interface","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The interface functions that need to be implemented will get their data in the form of index-value pairs or a specific structure related to the desired matrix format.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Indvals\nPSDMatrixCartesian\nIndvalsIterator","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.Indvals","page":"Backend","title":"PolynomialOptimization.Solver.Indvals","text":"Indvals{T,V}\n\nSupertype for an iterable that returns a Tuple{T,V} on iteration, where the first is a variable/constraint index and the second its coefficient in the constraint matrix. The parameters of the type correspond to those in AbstractSolver. The properties indices and values can be accessed and will give AbstractVectors of the appropriate type. Note that the fields should only be used if an iterative approach is not feasible, as they might be constructed on-demand (this will only happen for the first two indvals in the standard quadratic cone, all other elements can be accessed with zero cost).\n\n\n\n\n\n","category":"type"},{"location":"backend.html#PolynomialOptimization.Solver.PSDMatrixCartesian","page":"Backend","title":"PolynomialOptimization.Solver.PSDMatrixCartesian","text":"PSDMatrixCartesian\n\nAn iterable that returns matrix elements of a PSD cone. Iterating through it will give Pairs with keys that contain the mindex of the monomial, and a 3-Tuple of AbstractVectors as the values which contain the row and column indices together with their coefficients to describe where the monomial appears. Note that the vectors will be reused in every iteration.\n\nSee also PSDIndextypeMatrixCartesian.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#PolynomialOptimization.Solver.IndvalsIterator","page":"Backend","title":"PolynomialOptimization.Solver.IndvalsIterator","text":"IndvalsIterator{T,V}\n\nAn iterable that returns consecutive elements in a vectorized PSD cone. This type stores a vector of indices and values together with information about the length of the individual subsequences. Iterating through it will give Indvals that contain views into the indices and the values. The vector of indices is available via SparseArrays.rowvals, the vector of values via SparseArrays.nonzeros, and the lengths of the subsequences via Base.index_lengths.\n\nSee also PSDIndextypeVector.\n\n\n\n\n\n","category":"type"},{"location":"backend.html#Interface-for-the-moment-optimization","page":"Backend","title":"Interface for the moment optimization","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The custom implementation of poly_optimize first has to set up all the necessary initial data; then, a call to moment_setup! is sufficient to trigger the process.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"moment_setup!\nmoment_add_matrix!\nmoment_add_equality!","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.moment_setup!","page":"Backend","title":"PolynomialOptimization.Solver.moment_setup!","text":"moment_setup!(state::AbstractSolver, relaxation::AbstractRelaxation,\n    groupings::RelaxationGroupings[; representation])\n\nSets up all the necessary moment matrices, variables, constraints, and objective of a polynomial optimization problem problem according to the values given in grouping (where the first entry corresponds to the basis of the objective, the second of the equality, the third of the inequality, and the fourth of the PSD constraints). The function returns a Vector{<:Vector{<:Tuple{Symbol,Any}}} that contains internal information on the problem. This information is required to obtain dual variables and re-optimize the problem and should be stored in the state.\n\nThe following methods must be implemented by a solver to make this function work:\n\nmindex\nadd_constr_nonnegative!\nadd_constr_rotated_quadratic! (optional, then set supports_rotated_quadratic to true)\nadd_constr_quadratic! (optional, then set supports_quadratic to true)\nadd_constr_psd!\nadd_constr_psd_complex! (optional, then set supports_psd_complex to true)\nadd_constr_dddual! (optional, then set supports_dd to true)\nadd_constr_dddual_complex! (optional, then set supports_dd_complex to true)\nadd_constr_linf! (optional, then set supports_lnorm to true)\nadd_constr_linf_complex! (optional, then set supports_lnorm_complex to true)\nadd_constr_sdddual! (optional, then set supports_sdd to true)\nadd_constr_sdddual_complex! (optional, then set supports_sdd_complex to true)\npsd_indextype\nadd_constr_fix_prepare! (optional)\nadd_constr_fix!\nadd_constr_fix_finalize! (optional)\nfix_objective!\nadd_var_slack!\n\nwarning: Indices\nThe variable indices used in all solver functions directly correspond to the indices given back by mindex. However, in a sparse problem there may be far fewer indices present; therefore, when the problem is finally given to the solver, care must be taken to eliminate all unused indices. The functionality provided by AbstractAPISolver and AbstractSparseMatrixSolver already takes care of this.\n\ninfo: Order\nThe individual constraint types can be added in any order (including interleaved).\n\ninfo: Representation\nThis function may also be used to describe simplified cones such as the (scaled) diagonally dominant one. The representation parameter can be used to define a representation that is employed for the individual groupings. This may either be an instance of a RepresentationMethod - which requires the method to be independent of the dimension of the grouping - or a callable. In the latter case, it will be passed as a first parameter an identifier[3] of the current conic variable, and as a second parameter the side dimension of its matrix. The method must then return a RepresentationMethod instance.[3]: This identifier will be a tuple, where the first element is a symbol - either :objective, :nonneg, or :psd - to   indicate the general reason why the variable is there. The second element is an Int denoting the index of the   constraint (and will be undefined for the objective, but still present to avoid extra compilation). The last element   is an Int denoting the index of the grouping within the constraint/objective.\n\nSee also sos_setup!, moment_add_matrix!, moment_add_equality!, RepresentationMethod.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.moment_add_matrix!","page":"Backend","title":"PolynomialOptimization.Solver.moment_add_matrix!","text":"moment_add_matrix!(state::AbstractSolver, grouping::IntMonomialVector,\n    constraint::Union{<:IntPolynomial,<:AbstractMatrix{<:IntPolynomial}},\n    representation::RepresentationMethod=RepresentationPSD())\n\nParses a constraint in the moment hierarchy with a basis given in grouping (this might also be a partial basis due to sparsity), premultiplied by constraint (which may be the unit polynomial for the moment matrix) and calls the appropriate solver functions to set up the problem structure according to representation.\n\nTo make this function work for a solver, implement the following low-level primitives:\n\nmindex\nadd_constr_nonnegative!\nadd_constr_rotated_quadratic! (optional, then set supports_rotated_quadratic to true)\nadd_constr_quadratic! (optional, then set supports_quadratic to true)\nadd_constr_psd!\nadd_constr_psd_complex! (optional, then set supports_psd_complex to true)\nadd_constr_dddual! (optional, then set supports_dd to true)\nadd_constr_dddual_complex! (optional, then set supports_dd_complex to true)\nadd_constr_linf! (optional, then set supports_lnorm to true)\nadd_constr_linf_complex! (optional, then set supports_lnorm_complex to true)\nadd_constr_sdddual! (optional, then set supports_sdd to true)\nadd_constr_sdddual_complex! (optional, then set supports_sdd_complex to true)\npsd_indextype\nadd_var_slack!\n\nUsually, this function does not have to be called explicitly; use moment_setup! instead.\n\nSee also moment_add_equality!, RepresentationMethod.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.moment_add_equality!","page":"Backend","title":"PolynomialOptimization.Solver.moment_add_equality!","text":"moment_add_equality!(state::AbstractSolver, grouping::IntMonomialVector,\n    constraint::IntPolynomial)\n\nParses a polynomial equality constraint for moments and calls the appropriate solver functions to set up the problem structure. grouping contains the basis that will be squared in the process to generate the prefactor.\n\nTo make this function work for a solver, implement the following low-level primitives:\n\nadd_constr_fix_prepare! (optional)\nadd_constr_fix!\nadd_constr_fix_finalize! (optional)\n\nUsually, this function does not have to be called explicitly; use moment_setup! instead.\n\nSee also moment_add_matrix!.\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"For this to work, the following methods (or a subset as previously indicated) must be implemented.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"add_constr_nonnegative!\nadd_constr_rotated_quadratic!\nadd_constr_quadratic!\nadd_constr_psd!\nadd_constr_psd_complex!\nadd_constr_dddual!\nadd_constr_dddual_complex!\nadd_constr_linf!\nadd_constr_linf_complex!\nadd_constr_sdddual!\nadd_constr_sdddual_complex!\nadd_constr_fix_prepare!\nadd_constr_fix!\nadd_constr_fix_finalize!\nfix_objective!\nadd_var_slack!","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_nonnegative!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_nonnegative!","text":"add_constr_nonnegative!(state::AbstractSolver{T,V}, indvals::Indvals{T,V}) where {T,V}\n\nAdd a nonnegative constraint to the solver that contains the decision variables (columns in the linear constraint matrix) indexed according to indvals. Falls back to the vector-valued version if not implemented.\n\nSee also Indvals.\n\n\n\n\n\nadd_constr_nonnegative!(state::AbstractSolver{T,V},\n    indvals::IndvalsIterator{T,V}) where {T,V}\n\nAdds multiple nonnegative constraints to the solver that contain the decision variables (columns in the linear constraint matrix) indices according to the entries in indvals. Falls back to calling the scalar-valued version multiple times if not implemented.\n\nSee also IndvalsIterator.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_rotated_quadratic!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_rotated_quadratic!","text":"add_constr_rotated_quadratic!(state::AbstractSolver{T,V},\n    indvals::IndvalsIterator{T,V}) where {T,V}\n\nAdds a rotated quadratic constraint to the N = length(indvals) linear combinations of decision variables (columns in the conic constraint matrix) indexed according to the indvals. This will read (where X_i is mathitindvals_imathitvalues cdot x_mathitindvals_imathitindices) X_1 X_2 geq 0, 2X_1 X_2 geq sum_i = 3^N X_i^2.\n\nSee also Indvals, IndvalsIterator.\n\nnote: Number of parameters\nIn the real-valued case, indvals is always of length three, in the complex case, it is of length four. If the scaled diagonally dominant representation is requested, indvals can have any length.\n\nwarning: Warning\nThis function will only be called if supports_rotated_quadratic returns true for the given state. If (rotated) quadratic constraints are unsupported, a fallback to a 2x2 PSD constraint is used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_quadratic!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_quadratic!","text":"add_constr_quadratic!(state::AbstractSolver{T,V}, indvals::IndvalsIterator{T,V}) where {T,V}\n\nAdds a quadratic constraint to the N = length(indvals) linear combinations of decision variables (columns in the conic constraint matrix) indexed according to the indvals. This will read (where X_i is mathitindvals_imathitvalues cdot x_mathitindvals_imathitindices) X_1 geq 0, X_1^2 geq sum_i = 2^N X_i^2.\n\nSee also Indvals, IndvalsIterator.\n\nnote: Number of parameters\nIn the real-valued case, indvals is always of length three, in the complex case, it is of length four. If the scaled diagonally dominant representation is requested, indvals can have any length.\n\nwarning: Warning\nThis function will only be called if supports_quadratic returns true for the given state. If (rotated) quadratic constraints are unsupported, a fallback to a 2x2 PSD constraint is used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_psd!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_psd!","text":"add_constr_psd!(state::AbstractSolver{T,V}, dim::Integer,\n    data::PSDMatrixCartesian{T,V}) where {T,V}\n\nAdd a PSD constraint of side dimension dim  3 to the solver. Its requested triangle is indexed according to the return value of psd_indextype); these elements make up a linear matrix inequality with variables given by the keys when iterating through data, which are of the type T. Note that if add_constr_quadratic! is not implemented, dim may also be 2. This method is called if psd_indextype returns a PSDIndextypeMatrixCartesian.\n\nhint: Complex-valued PSD variables\nNote that this function will also be called for complex-valued PSD cones if supports_psd_complex returns false. The data will have been rewritten in terms of a real-valued PSD cone, which doubles the dimension. If the solver natively supports complex-valued PSD cones, add_constr_psd_complex! must be implemented.\n\n\n\n\n\nadd_constr_psd!(state::AbstractSolver{T,V}, dim::Integer, data::IndvalsIterator{T,V}) where {T,V}\n\nAdd a PSD constraint of side dimension dim  3 to the solver. data is an iterable through the elements of the PSD matrix one-by-one, in the order specified by psd_indextype. The individual entries are Indvals. This method is called if psd_indextype returns a PSDIndextypeVector.\n\nhint: Complex-valued PSD variables\nNote that this function will also be called for complex-valued PSD cones if supports_psd_complex returns false. The data will have been rewritten in terms of a real-valued PSD cone, which doubles the dimension. If the solver natively supports complex-valued PSD cones, add_constr_psd_complex! must be implemented.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_psd_complex!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_psd_complex!","text":"add_constr_psd_complex!(state::AbstractSolver{T,V}, dim::Int,\n    data::PSDMatrixCartesian{T,Complex{V}}) where {T,V}\n\nAdd a Hermitian PSD constraint of side dimension dim  3 to the solver. Its requested triangle is indexed according to the return value of psd_indextype); these elements make up a linear matrix inequality with variables given by the keys when iterating through data, which are of the type T. The real part of any coefficient corresponds to the coefficient in front of the real part of the matrix entry, the imaginary part is the coefficient for the imaginary part of the matrix entry. Note that if add_constr_quadratic! is not implemented, dim may also be 2. This method is called if psd_indextype returns a PSDIndextypeMatrixCartesian.\n\nwarning: Warning\nThis function will only be called if supports_psd_complex is defined to return true for the given state.\n\n\n\n\n\nadd_constr_psd_complex!(state::AbstractSolver{T,V}, dim::Int, data::IndvalsIterator{T,V}) where {T,V}\n\nAdd a Hermitian PSD constraint of side dimension dim  3 to the solver. data is an iterable through the elements of the PSD matrix one-by-one, in the order specified by psd_indextype. The individual entries are Indvals. This method is called if psd_indextype returns a PSDIndextypeVector. Regardless of the travelling order, for diagonal elements, there will be exactly one entry, which is the real part. For off-diagonal elements, the real part will be followed by the imaginary part. Therefore, the coefficients are real-valued.\n\nwarning: Warning\nThis function will only be called if supports_psd_complex is defined to return true for the given state.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_dddual!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_dddual!","text":"add_constr_dddual!(state::AbstractSolver{T,V}, dim::Integer, data::IndvalsIterator{T,V},\n    u) where {T,V}\n\nAdd a constraint for membership in the dual cone to diagonally dominant matrices to the solver. data is an iterator through the scaled lower triangle of the matrix. A basis change is induced by u, with the meaning for the primal cone that M  DD(u)  M = u Q u with Q  DD.\n\nwarning: Warning\nThis function will only be called if supports_dd returns true for the given state. If diagonally dominant cones are not supported directly, a fallback to a columnwise representation in terms of ell_infty norms will be used (or the fallbacks if this norm is not supported).\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_dddual_complex!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_dddual_complex!","text":"add_constr_dddual_complex!(state::AbstractSolver{T,V}, dim::Integer,\n    data::IndvalsIterator{T,V}, u) where {T,V}\n\nAdd a constraint for membership in the dual cone to complex-valued diagonally dominant matrices to the solver. data is an iterator through the scaled lower triangle of the matrix. A basis change is induced by u, with the meaning for the primal cone that M  DD(u)  M = u Q u with Q  DD. For diagonal elements, there will be exactly one entry, which is the real part. For off-diagonal elements, the real part will be followed by the imaginary part. Therefore, the coefficients are real-valued.\n\nwarning: Warning\nThis function will only be called if supports_dd_complex returns true for the given state. If complex-valued diagonally dominant cones are not supported directly, a fallback to quadratic constraints on the complex-valued data is tried first (if supported), followed by a columnwise representation in terms of ell_infty norms or their fallback on the realification of the matrix data if not.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_linf!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_linf!","text":"add_constr_linf!(state::AbstractSolver{T,V}, indvals::IndvalsIterator{T,V}) where {T,V}\n\nAdds an ell_infty norm constraint to the N = length(indvals) linear combinations of decision variables (columns in the conic constraint matrix) indexed according to the indvals. This will read (where X_i is mathitindvals_imathitvalues cdot x_mathitindvals_imathitindices) X_1 geq max_i  2 lvert X_irvert.\n\nSee also Indvals, IndvalsIterator.\n\nwarning: Warning\nThis function will only be called if supports_lnorm returns true for the given state. If ell_infty norm constraints are unsupported, a fallback to multiple linear constraints will be used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_linf_complex!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_linf_complex!","text":"add_constr_linf_complex!(state, indvals::IndvalsIterator{T,V}) where {T,V<:Real}\n\nSame as add_constr_linf!, but now two successive items in indvals (starting from the second) are interpreted as determining the real and imaginary part of a component of the ell_infty norm cone.\n\nwarning: Warning\nThis function will only be called if supports_lnorm_complex returns true for the given state. If complex-valued ell_infty norm constraints are unsupported, a fallback to multiple linear constraints and quadratic cones will be used. If supports_quadratic is not true, complex-valued DD cones cannot be used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_sdddual!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_sdddual!","text":"add_constr_sdddual!(state::AbstractSolver{T,V}, dim::Integer, data::IndvalsIterator{T,V},\n    u) where {T,V}\n\nAdd a constraint for membership in the dual cone to scaled diagonally dominant matrices to the solver. data is an iterator through the (unscaled) lower triangle of the matrix. A basis change is induced by u, with the meaning for the primal cone that M  SDD(u)  M = u Q u with Q  SDD.\n\nwarning: Warning\nThis function will only be called if supports_sdd returns true for the given state. If scaled diagonally dominant cones are not supported directly, a fallback to (rotated) quadratic cones will be used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_sdddual_complex!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_sdddual_complex!","text":"add_constr_sdddual_complex!(state::AbstractSolver{T,V}, dim::Integer,\n    data::IndvalsIterator{T,V}, u) where {T,V}\n\nAdd a constraint for membership in the dual cone to complex-valued scaled diagonally dominant matrices to the solver. data is an iterator through the (unscaled) lower triangle of the matrix. A basis change is induced by u, with the meaning for the primal cone that M  SDD(u)  M = u Q u with Q  SDD. For diagonal elements, there will be exactly one entry, which is the real part. For off-diagonal elements, the real part will be followed by the imaginary part. Therefore, the coefficients are real-valued.\n\nwarning: Warning\nThis function will only be called if supports_sdd_complex returns true for the given state. If complex-valued sclaed diagonally dominant cones are not supported directly, a fallback to quadratic constraints is automatically performed.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_fix_prepare!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_fix_prepare!","text":"add_constr_fix_prepare!(state::AbstractSolver, num::Int)\n\nPrepares to add exactly num constraints that are fixed to a certain value; the actual data is then put into the solver by subsequent calls of add_constr_fix! and the whole transaction is completed by add_constr_fix_finalize!. The return value of this function is passed on as constrstate to add_constr_fix!. The default implementation does nothing.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_fix!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_fix!","text":"add_constr_fix!(state::AbstractSolver{T,V}, constrstate, indvals::Indvals{T,V},\n    rhs::V) where {T,V}\n\nAdd a constraint fixed to rhs to the solver that is composed of all variables (columns in the linear constraint matrix) indexed according to indvals. The parameter constrstate is, upon first call, the value returned by add_constr_fix_prepare!; and on all further calls, it will be the return value of the previous call. Note that rhs will almost always be zero, so if the right-hand side is represented by a sparse vector, it is worth checking for this value (the compiler will be able to remove the check).\n\nSee also Indvals.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_fix_finalize!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_fix_finalize!","text":"add_constr_fix_finalize!(state::AbstractSolver, constrstate)\n\nFinishes the addition of fixed constraints to state; the value of constrstate is the return value of the last call to add_constr_fix!. The default implementation does nothing.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.fix_objective!","page":"Backend","title":"PolynomialOptimization.Solver.fix_objective!","text":"fix_objective!(state, indvals::Indvals)\n\nPuts the variables indexed according to indvals into the objective (that is to be minimized). This function will be called exactly once by moment_setup! after all variables and constraints have been set up.\n\nSee also Indvals.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_slack!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_slack!","text":"add_var_slack!(state::AbstractSolver{T}, num::Int)\n\nCreates num slack variables in the problem. Slack variables must be free. The result should be an abstract vector (typically a unit range) that contains the indices of all created slack variables. The indices should be of type T.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#Interface-for-the-SOS-optimization","page":"Backend","title":"Interface for the SOS optimization","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"This is in complete analogy to the moment case; the entry point is now sos_setup!.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"sos_setup!\nsos_add_matrix!\nsos_add_equality!","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.sos_setup!","page":"Backend","title":"PolynomialOptimization.Solver.sos_setup!","text":"sos_setup!(state::AbstractSolver, relaxation::AbstractRelaxation,\n    groupings::RelaxationGroupings[; representation])\n\nSets up all the necessary SOS matrices, free variables, objective, and constraints of a polynomial optimization problem problem according to the values given in grouping (where the first entry corresponds to the basis of the objective, the second of the equality, the third of the inequality, and the fourth of the PSD constraints). The function returns a Vector{<:Vector{<:Tuple{Symbol,Any}}} that contains internal information on the problem. This information is required to obtain dual constraints and re-optimize the problem and should be stored in the state.\n\nThe following methods must be implemented by a solver to make this function work:\n\nmindex\nadd_var_nonnegative!\nadd_var_rotated_quadratic! (optional, then set supports_rotated_quadratic to true)\nadd_var_quadratic! (optional, then set supports_quadratic to true)\nadd_var_psd!\nadd_var_psd_complex! (optional, then set supports_psd_complex to true)\nadd_var_dd! (optional, then set supports_dd to true)\nadd_var_dd_complex! (optional, then set supports_dd_complex to true)\nadd_var_l1! (optional, then set supports_lnorm to true)\nadd_var_l1_complex! (optional, then set supports_lnorm_complex to true)\nadd_var_sdd! (optional, then set supports_sdd to true)\nadd_var_sdd_complex! (optional, then set supports_sdd_complex to true)\npsd_indextype\nadd_var_free_prepare! (optional)\nadd_var_free!\nadd_var_free_finalize! (optional)\nfix_constraints!\nadd_constr_slack!\n\nwarning: Indices\nThe constraint indices used in all solver functions directly correspond to the indices given back by mindex. However, in a sparse problem there may be far fewer indices present; therefore, when the problem is finally given to the solver, care must be taken to eliminate all unused indices. The functionality provided by AbstractAPISolver and AbstractSparseMatrixSolver already takes care of this.\n\ninfo: Order\nThe individual variable types can be added in any order (including interleaved).\n\ninfo: Representation\nThis function may also be used to describe simplified cones such as the (scaled) diagonally dominant one. The representation parameter can be used to define a representation that is employed for the individual groupings. This may either be an instance of a RepresentationMethod - which requires the method to be independent of the dimension of the grouping - or a callable. In the latter case, it will be passed as a first parameter an identifier[3] of the current conic variable, and as a second parameter the side dimension of its matrix. The method must then return a RepresentationMethod instance.\n\nSee also moment_setup!, sos_add_matrix!, sos_add_equality!, RepresentationMethod.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.sos_add_matrix!","page":"Backend","title":"PolynomialOptimization.Solver.sos_add_matrix!","text":"sos_add_matrix!(state::AbstractSolver, grouping::IntMonomialVector,\n    constraint::Union{<:IntPolynomial,<:AbstractMatrix{<:IntPolynomial}},\n    representation::RepresentationMethod=RepresentationPSD())\n\nParses a SOS constraint with a basis given in grouping (this might also be a partial basis due to sparsity), premultiplied by constraint (which may be the unit polynomial for the SOS cone membership) and calls the appropriate solver functions to set up the problem structure according to representation.\n\nTo make this function work for a solver, implement the following low-level primitives:\n\nmindex\nadd_var_nonnegative!\nadd_var_rotated_quadratic! (optional, then set supports_rotated_quadratic to true)\nadd_var_quadratic! (optional, then set supports_quadratic to true)\nadd_var_psd!\nadd_var_psd_complex! (optional, then set supports_psd_complex to true)\nadd_var_dd! (optional, then set supports_dd to true)\nadd_var_dd_complex! (optional, then set supports_dd_complex to true)\nadd_var_l1! (optional, then set supports_lnorm to true)\nadd_var_l1_complex! (optional, then set supports_lnorm_complex to true)\nadd_var_sdd! (optional, then set supports_sdd to true)\nadd_var_sdd_complex! (optional, then set supports_sdd_complex to true)\npsd_indextype\nadd_constr_slack!\n\nUsually, this function does not have to be called explicitly; use sos_setup! instead.\n\nSee also sos_add_equality!.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.sos_add_equality!","page":"Backend","title":"PolynomialOptimization.Solver.sos_add_equality!","text":"sos_add_equality!(state::AbstractSolver, grouping::IntMonomialVector,\n    constraint::IntPolynomial)\n\nParses a polynomial equality constraint for sums-of-squares and calls the appropriate solver functions to set up the problem structure. grouping contains the basis that will be squared in the process to generate the prefactor.\n\nTo make this function work for a solver, implement the following low-level primitives:\n\nadd_var_free_prepare! (optional)\nadd_var_free! (required)\nadd_var_free_finalize! (optional)\n\nUsually, this function does not have to be called explicitly; use sos_setup! instead.\n\nSee also sos_add_matrix!.\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The following methods (or a subset as previously indicated) must be implemented.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"add_var_nonnegative!(::AbstractSolver{T,V}, ::Indvals{T,V}) where {T,V}\nadd_var_nonnegative!(::AbstractSolver{T,V}, ::IndvalsIterator{T,V}) where {T,V}\nadd_var_rotated_quadratic!\nadd_var_quadratic!\nadd_var_psd!(::AbstractSolver{T,V}, ::Int, ::PSDMatrixCartesian{T,V}) where {T,V}\nadd_var_psd!(::AbstractSolver{T,V}, ::Int, ::IndvalsIterator{T,V}) where {T,V}\nadd_var_psd_complex!\nadd_var_dd!\nadd_var_dd_complex!\nadd_var_l1!\nadd_var_l1_complex!\nadd_var_sdd!\nadd_var_sdd_complex!\nadd_var_free_prepare!\nadd_var_free!\nadd_var_free_finalize!\nfix_constraints!\nadd_constr_slack!","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_nonnegative!-Union{Tuple{V}, Tuple{T}, Tuple{PolynomialOptimization.Solver.AbstractSolver{T, V}, PolynomialOptimization.Solver.Indvals{T, V}}} where {T, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_nonnegative!","text":"add_var_nonnegative!(state::AbstractSolver{T,V}, indvals::Indvals{T,V}) where {T,V}\n\nAdd a nonnegative decision variable to the solver and put its value into the linear constraints (rows in the linear constraint matrix) indexed according to indvals. Falls back to the vector-valued version if not implemented.\n\nSee also Indvals.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_nonnegative!-Union{Tuple{V}, Tuple{T}, Tuple{PolynomialOptimization.Solver.AbstractSolver{T, V}, PolynomialOptimization.Solver.IndvalsIterator{T, V, L, VT, VV} where {L, VT<:AbstractVector{T}, VV<:AbstractVector{V}}}} where {T, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_nonnegative!","text":"add_var_nonnegative!(state::AbstractSolver{T,V}, indvals::IndvalsIterator{T,V}) where {T,V}\n\nAdd multiple nonnegative decision variables to the solver and put their values into the linear constraints (rows in the linear constraint matrix) indexed according to the entries in indvals. Falls back to calling the scalar-valued version multiple times if not implemented.\n\nSee also IndvalsIterator.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_rotated_quadratic!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_rotated_quadratic!","text":"add_var_rotated_quadratic!(state::AbstractSolver{T,V},\n    indvals::IndvalsIterator{T,V}) where {T,V}\n\nAdds decision variables in a rotated quadratic cone to the solver and put their values into the linear constraints (rows in the linear constraint matrix), indexed according to indvals. The N = length(indvals) variables will satisfy x_1 x_2 geq 0, 2x_1 x_2 geq sum_i = 3^N x_i^2.\n\nSee also Indvals, IndvalsIterator.\n\nnote: Number of parameters\nIn the real-valued case, indvals is always of length three, in the complex case, it is of length four. If the scaled diagonally dominant representation is requested, indvals can have any length.\n\nwarning: Warning\nThis function will only be called if supports_quadratic returns true for the given state. If (rotated) quadratic constraints are unsupported, a fallback to a 2x2 PSD variable is used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_quadratic!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_quadratic!","text":"add_var_quadratic!(state::AbstractSolver{T,V}, indvals::IndvalsIterator{T,V}) where {T,V}\n\nAdds decision variables in a quadratic cone to the solver and put their values into the linear constraints (rows in the linear constraint matrix), indexed according to indvals. The N = length(indvals) variables will satisfy x_1 geq 0, x_1^2 geq sum_i = 2^N x_i^2.\n\nSee also Indvals, IndvalsIterator.\n\nnote: Number of parameters\nIn the real-valued case, indvals is always of length three, in the complex case, it is of length four. If the scaled diagonally dominant representation is requested, indvals can have any length.\n\nwarning: Warning\nThis function will only be called if supports_quadratic returns true for the given state. If (rotated) quadratic constraints are unsupported, a fallback to a 2x2 PSD variable is used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_psd!-Union{Tuple{V}, Tuple{T}, Tuple{PolynomialOptimization.Solver.AbstractSolver{T, V}, Int64, PolynomialOptimization.Solver.PSDMatrixCartesian{T, V}}} where {T, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_psd!","text":"add_var_psd!(state::AbstractSolver{T,V}, dim::Int,\n    data::PSDMatrixCartesian{T,V}) where {T,V}\n\nAdd a PSD variable of side dimension dim  3 to the solver. Its requested triangle is indexed according to the return value of psd_indextype); these elements of the matrix are put into the linear constraints (rows in the linear constraint matrix) indicated by the keys when iterating through data, which are of the type T, at positions and with coefficients given by their values. Note that if add_var_quadratic! is not implemented, dim may also be 2. This method is called if psd_indextype returns a PSDIndextypeMatrixCartesian.\n\nhint: Complex-valued PSD variables\nNote that this function will also be called for complex-valued PSD cones if supports_psd_complex returns false. The data will have been rewritten in terms of a real-valued PSD cone, which doubles the dimension. If the solver natively supports complex-valued PSD cones, add_var_psd_complex! must be implemented.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_psd!-Union{Tuple{V}, Tuple{T}, Tuple{PolynomialOptimization.Solver.AbstractSolver{T, V}, Int64, PolynomialOptimization.Solver.IndvalsIterator{T, V, L, VT, VV} where {L, VT<:AbstractVector{T}, VV<:AbstractVector{V}}}} where {T, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_psd!","text":"add_var_psd!(state::AbstractSolver{T,V}, dim::Int, data::IndvalsIterator{T,V}) where {T,V}\n\nConceptually the same as above; but now, data is an iterable through the elements of the PSD variable one-by-one. The individual entries are Indvals. This method is called if psd_indextype returns a PSDIndextypeVector.\n\nhint: Complex-valued PSD variables\nNote that this function will also be called for complex-valued PSD cones if supports_psd_complex returns false. The data will have been rewritten in terms of a real-valued PSD cone, which doubles the dimension. If the solver natively supports complex-valued PSD cones, add_var_psd_complex! must be implemented.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_psd_complex!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_psd_complex!","text":"add_var_psd_complex!(state::AbstractSolver{T,V}, dim::Int, data::PSDMatrixCartesian{T,Complex{V}}) where {T,V}\n\nAdd a Hermitian PSD variable of side dimension dim  3 to the solver. Its requested triangle is indexed according to the return value of psd_indextype); these elements of the matrix are put into the linear constraints (rows in the linear constraint matrix) indicated by the keys when iterating through data, which are of the type T, at positions and with coefficients given by their values. The real part of the coefficient corresponds to the coefficient in front of the real part of the matrix entry, the imaginary part is the coefficient for the imaginary part of the matrix entry. Note that if add_var_quadratic! is not implemented, dim may also be 2. This method is called if psd_indextype returns a PSDIndextypeMatrixCartesian.\n\nwarning: Warning\nThis function will only be called if supports_psd_complex is defined to return true for the given state.\n\n\n\n\n\nadd_var_psd_complex!(state::AbstractSolver{T,V}, dim::Int,\n    data::IndvalsIterator{T,V}) where {T,V}\n\nConceptually the same as above; but now, data is an iterable through the elements of the PSD variable one-by-one. The individual entries are Indvals. This method is called if psd_indextype returns a PSDIndextypeVector. Regardless of the travelling order, for diagonal elements, there will be exactly one entry, which is the real part. For off-diagonal elements, the real part will be followed by the imaginary part. Therefore, the coefficients are real-valued.\n\nwarning: Warning\nThis function will only be called if supports_psd_complex is defined to return true for the given state.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_dd!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_dd!","text":"add_var_dd!(state::AbstractSolver{T,V}, dim::Integer, data::IndvalsIterator{T,V},\n    u) where {T,V}\n\nAdd a constraint for membership in the cone of diagonally dominant matrices to the solver. data is an iterator through the scaled lower triangle of the matrix. A basis change is induced by u, with the meaning that M  DD(u)  M = u Q u with Q  DD.\n\nwarning: Warning\nThis function will only be called if supports_dd returns true for the given state. If diagonally dominant cones are not supported directly, a fallback to a columnwise representation in terms of ell_1 norms will be used (or the fallbacks if this norm is not supported).\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_dd_complex!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_dd_complex!","text":"add_var_dd_complex!(state::AbstractSolver{T,V}, dim::Integer,\n    data::IndvalsIterator{T,V}, u) where {T,V}\n\nAdd a constraint for membership in the cone of complex-valued diagonally dominant matrices to the solver. data is an iterator hrough the scaled lower triangle of the matrix. A basis change is induced by u, with the meaning that M  DD(u)  M = u Q u with Q  DD. For diagonal elements, there will be exactly one entry, which is the real part. For off-diagonal elements, the real part will be followed by the imaginary part. Therefore, the coefficients are real-valued.\n\nwarning: Warning\nThis function will only be called if supports_dd_complex returns true for the given state. If complex-valued diagonally dominant cones are not supported directly, a fallback to quadratic cones on the complex-valued data is tried first (if supported), followed by a columnwise representation in terms of ell_1 norms or their fallback on the realification of the matrix data if not.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_l1!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_l1!","text":"add_var_l1!(state::AbstractSolver{T,V}, indvals::IndvalsIterator{T,V}) where {T,V}\n\nAdds decision variables in an ell_1 norm cone to the solver and put their values into the linear constraints (rows in the linear constraint matrix), indexed according to the indvals. The N = length(indvals) variables will satisfy x_1 geq sum_i = 2^N lvert x_irvert.\n\nSee also Indvals, IndvalsIterator.\n\nwarning: Warning\nThis function will only be called if supports_lnorm returns true for the given state. If ell_infty norm cones are unsupported, a fallback to multiple nonnegative variables will be used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_l1_complex!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_l1_complex!","text":"add_var_l1_complex!(state::AbstractSolver{T,V}, indvals::IndvalsIterator{T,V}) where {T,V}\n\nSame as add_var_l1!, but now two successive items in indvals (starting from the second) are interpreted as determining the real and imaginary part of a component of the ell_1 norm variable.\n\nwarning: Warning\nThis function will only be called if supports_lnorm_complex returns true for the given state. If complex-valued ell_1 norm cones are unsupported, a fallback to multiple nonnegative and quadratic variables will be used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_sdd!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_sdd!","text":"add_var_sdd!(state::AbstractSolver{T,V}, dim::Integer, data::IndvalsIterator{T,V},\n    u) where {T,V}\n\nAdd a constraint for membership in the cone of scaled diagonally dominant matrices to the solver. data is an iterator through the (unscaled) lower triangle of the matrix. A basis change is induced by u, with the meaning that M  SDD(u)  M = u Q u with Q  SDD.\n\nwarning: Warning\nThis function will only be called if supports_sdd returns true for the given state. If scaled diagonally dominant cones are not supported directly, a fallback to (rotated) quadratic cones will be used.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_sdd_complex!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_sdd_complex!","text":"add_var_sdd_complex!(state::AbstractSolver{T,V}, dim::Integer,\n    data::IndvalsIterator{T,V}, u) where {T,V}\n\nAdd a constraint for membership in the cone of complex-valued scaled diagonally dominant matrices to the solver. data is an iterator through the (unscaled) lower triangle of the matrix. A basis change is induced by u, with the meaning that M  SDD(u)  M = u Q u with Q  SDD. For diagonal elements, there will be exactly one entry, which is the real part. For off-diagonal elements, the real part will be followed by the imaginary part. Therefore, the coefficients are real-valued.\n\nwarning: Warning\nThis function will only be called if supports_sdd_complex returns true for the given state. If complex-valued scaled diagonally dominant cones are not supported directly, a fallback to quadratic cones is automatically performed.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_free_prepare!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_free_prepare!","text":"add_var_free_prepare!(state::AbstractSolver, num::Int)\n\nPrepares to add exactly num free variables that may become part of the objective; the actual data is then put into the solver by subsequent calls of add_var_free! and the whole transaction is completed by add_var_free_finalize!. The return value of this function is passed on as eqstate to add_var_free!. The default implementation does nothing.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_free!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_free!","text":"add_var_free!(state::AbstractSolver{T,V}, eqstate, indvals::Indvals{T,V},\n    obj::V) where {T,V}\n\nAdd a free variable to the solver and put its value into the linear constraints (rows in the linear constraint matrix), indexed according to indvals. The variable should also be put into the objective with coefficient obj (which is likely to be zero). The parameter eqstate is, upon first call, the value returned by add_var_free_prepare!; and on all further calls, it will be the return value of the previous call.\n\nSee also Indvals.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_free_finalize!","page":"Backend","title":"PolynomialOptimization.Solver.add_var_free_finalize!","text":"add_var_free_finalize!(state::AbstractSolver, eqstate)\n\nFinishes the addition of free variables to state; the value of eqstate is the return value of the last call to add_var_free!. The default implementation does nothing.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.fix_constraints!","page":"Backend","title":"PolynomialOptimization.Solver.fix_constraints!","text":"fix_constraints!(state::AbstractSolver{T,V}, indvals::Indvals{T,V}) where {T,V}\n\nEnsures that all constraints in the optimization problem are fixed to the values according to indvals. This function will be called exactly once by sos_setup! after all variables and constraints have been set up.\n\nSee also Indvals.\n\n\n\n\n\nfix_constraints(state::AbstractSolver{<:Integer,V}, m::Int,\n    indvals::Indvals{I,V}) where {I,V}\n\nEnsures that all constraints in the optimization problem are fixed to the values according to indvals. This form of the function is called from primal_moment_setup!. m is the number of constraints in the solver.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.add_constr_slack!","page":"Backend","title":"PolynomialOptimization.Solver.add_constr_slack!","text":"add_constr_slack!(state::AbstractSolver{T}, num::Int)\n\nCreates num linear fix-to-zero slack constraints in the problem (i.e., constraints that do not correspond to moments). The result should be an abstract vector (typically a unit range) that contains the indices of type T of all created slack constraints.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#Interface-for-the-moment-optimization-in-primal-form","page":"Backend","title":"Interface for the moment optimization in primal form","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"A very particular case is if the moment optimization should be done using semidefinite variables that the solver allows to define, but only as a single variable (not as a cone in which to put variables); this is the primal form, most suitable for SOS optimizations. However, there can be a good reason to use the primal form for moment optimizations instead: namely, if the solver can exploit a low-rank assumptions on this matrix. In this case, poly_optimize should call primal_moment_setup! instead:","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"primal_moment_setup!","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.primal_moment_setup!","page":"Backend","title":"PolynomialOptimization.Solver.primal_moment_setup!","text":"primal_moment_setup!(state::AbstractSolver, relaxation::AbstractRelaxation,\n    groupings::RelaxationGroupings; verbose=false)\n\nSets up all the necessary moment matrices, constraints, and objective of a polynomial optimization problem in primal form, i.e., for solvers which allow to declare monolithic semidefinite and nonnegative variables and linear constraints. While usually, the SOS form would be more suitable for such solvers, forcing the moment form in primal representation ensures that low-rank assumptions about the primal variable hold true, which can be exploited by some solvers. This function returns a Vector{<:Vector{<:Tuple{Symbol,Any}}} that contains internal information on the problem. This information is required to obtain dual variables and re-optimize the problem and should be stored in the state. It also returns an internal state that is important for the reconstruction of the moment matrices using MomentVector. The internal state has three properties that are part of the public interface:\n\nnum_con::Int is the number of constraints\nnum_nonneg::Int is the number of nonnegative variables\npsd_dim::FastVec{I} holds the side dimensions of the PSD variables (where I is the index type of state)\n\nThe following methods must be implemented by a solver to make this function work:\n\nmindex\nadd_var_nonnegative!, which is called no more than once\nadd_var_psd!\npsd_indextype\nobjective_indextype\nfix_constraints!\n\nwarning: Warning\nDuring the reformulation, the function is able to detect a certain class of unbounded or infeasible problem formulations. If this is the case, it will return missing without invoking the solver.\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The following methods must be implemented:","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"add_var_nonnegative!(::AbstractSolver{<:Integer,V}, ::Int, ::Int, ::SparseMatrixCOO{I,I,V}, ::Tuple{FastVec{I},FastVec{V}}) where {I,V}\nadd_var_psd!(::AbstractSolver{<:Integer,V}, ::Int, ::I, ::SparseMatrixCOO{I,I,V}, ::Union{Nothing,Tuple{FastVec{I},FastVec{V}}}) where {I,V}\nadd_var_psd!(::AbstractSolver{<:Integer,V}, ::Int, ::I, ::SparseMatrixCOO{I,I,V}, ::Union{Nothing,Tuple{Tuple{FastVec{I},FastVec{I}},FastVec{V}}}) where {I,V}\nadd_var_psd!(::AbstractSolver{<:Integer,V}, ::Int, ::I, ::Tuple{FastVec{I},Tuple{FastVec{I},FastVec{I}},FastVec{V}}, ::Union{Nothing,Tuple{FastVec{I},FastVec{V}}}) where {I,V}\nadd_var_psd!(::AbstractSolver{<:Integer,V}, ::Int, ::I, ::Tuple{FastVec{I},Tuple{FastVec{I},FastVec{I}},FastVec{V}}, ::Union{Nothing,Tuple{Tuple{FastVec{I},FastVec{I}},FastVec{V}}}) where {I,V}\nfix_constraints!(::AbstractSolver{<:Integer,V}, ::Int, ::Indvals{<:Integer,V}) where {V}\nobjective_indextype","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_nonnegative!-Union{Tuple{V}, Tuple{I}, Tuple{PolynomialOptimization.Solver.AbstractSolver{<:Integer, V}, Int64, Int64, PolynomialOptimization.Solver.SparseMatrixCOO{I, I, V}, Tuple{PolynomialOptimization.FastVector.FastVec{I}, PolynomialOptimization.FastVector.FastVec{V}}}} where {I, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_nonnegative!","text":"add_var_nonnegative!(state::AbstractSolver{<:Integer,V}, m::Int, n::Int,\n    data::SparseMatrixCOO{I,I,V}, obj::Tuple{FastVec{I},FastVec{V}}) where {I,V}\n\nThis form of the function is called from primal_moment_setup!. m is the number of total constraints, n the number of nonnegative variables; and the linear constraint matrix is given by data. The variables should be put into the objective according to obj. Note that the indices in data will have an offset as defined by the psd_indextype, while the indices in obj take their offset from objective_indextype.\n\nSee also coo_to_csc!\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_psd!-Union{Tuple{V}, Tuple{I}, Tuple{PolynomialOptimization.Solver.AbstractSolver{<:Integer, V}, Int64, I, PolynomialOptimization.Solver.SparseMatrixCOO{I, I, V}, Union{Nothing, Tuple{PolynomialOptimization.FastVector.FastVec{I}, PolynomialOptimization.FastVector.FastVec{V}}}}} where {I, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_psd!","text":"add_var_psd!(state::AbstractSolver{<:Integer,V}, m::Int, dim::I,\n    data::SparseMatrixCOO{I,I,V},\n    obj::Union{Nothing,Tuple{FastVec{I},FastVec{V}}}) where {I,V}\n\nThis form of the function is called from primal_moment_setup! when both the psd_indextype and the objective_indextype are of type PSDIndextypeCOOVectorized. Note that the triangles or scaling factors are allowed to be different. m is the number of constraints in the problem, and the first index in data always corresponds to the constraint index. The matrix also takes part in the objective unless the obj parameter is nothing.\n\nThere are various possible variations in which the data can be passed, as documented for the following functions.\n\nSee also coo_to_csc!\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_psd!-Union{Tuple{V}, Tuple{I}, Tuple{PolynomialOptimization.Solver.AbstractSolver{<:Integer, V}, Int64, I, PolynomialOptimization.Solver.SparseMatrixCOO{I, I, V}, Union{Nothing, Tuple{Tuple{PolynomialOptimization.FastVector.FastVec{I}, PolynomialOptimization.FastVector.FastVec{I}}, PolynomialOptimization.FastVector.FastVec{V}}}}} where {I, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_psd!","text":"add_var_psd!(state::AbstractSolver{<:Integer,V}, m::Int, dim::I,\n    data::SparseMatrixCOO{I,I,V},\n    obj::Union{Nothing,Tuple{Tuple{FastVec{I},FastVec{I}},FastVec{V}}}) where {I,V}\n\nThis form of the function is called from primal_moment_setup! when the psd_indextype is a PSDIndextypeCOOVectorized and the objective_indextype is a PSDIndextypeMatrixCartesian.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_psd!-Union{Tuple{V}, Tuple{I}, Tuple{PolynomialOptimization.Solver.AbstractSolver{<:Integer, V}, Int64, I, Tuple{PolynomialOptimization.FastVector.FastVec{I}, Tuple{PolynomialOptimization.FastVector.FastVec{I}, PolynomialOptimization.FastVector.FastVec{I}}, PolynomialOptimization.FastVector.FastVec{V}}, Union{Nothing, Tuple{PolynomialOptimization.FastVector.FastVec{I}, PolynomialOptimization.FastVector.FastVec{V}}}}} where {I, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_psd!","text":"add_var_psd!(state::AbstractSolver{<:Integer,V}, m::Int, dim::I,\n    data::Tuple{FastVec{I},Tuple{FastVec{I},FastVec{I}},FastVec{V}},\n    obj::Union{Nothing,Tuple{FastVec{I},FastVec{V}}}) where {I,V}\n\nThis form of the function is called from primal_moment_setup! when the psd_indextype is a PSDIndextypeMatrixCartesian and the objective_indextype is a PSDIndextypeCOOVectorized.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.add_var_psd!-Union{Tuple{V}, Tuple{I}, Tuple{PolynomialOptimization.Solver.AbstractSolver{<:Integer, V}, Int64, I, Tuple{PolynomialOptimization.FastVector.FastVec{I}, Tuple{PolynomialOptimization.FastVector.FastVec{I}, PolynomialOptimization.FastVector.FastVec{I}}, PolynomialOptimization.FastVector.FastVec{V}}, Union{Nothing, Tuple{Tuple{PolynomialOptimization.FastVector.FastVec{I}, PolynomialOptimization.FastVector.FastVec{I}}, PolynomialOptimization.FastVector.FastVec{V}}}}} where {I, V}","page":"Backend","title":"PolynomialOptimization.Solver.add_var_psd!","text":"add_var_psd!(state::AbstractSolver{<:Integer,V}, m::Int, dim::I,\n    data::Tuple{FastVec{I},Tuple{FastVec{I},FastVec{I}},FastVec{V}},\n    obj::Union{Nothing,Tuple{Tuple{FastVec{I},FastVec{I}},FastVec{V}}}) where {I,V}\n\nThis form of the function is called from primal_moment_setup! when both the psd_indextype and the objective_indextype are of type PSDIndextypeMatrixCartesian. Note that the triangles are allowed to be different.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.fix_constraints!-Union{Tuple{V}, Tuple{PolynomialOptimization.Solver.AbstractSolver{<:Integer, V}, Int64, PolynomialOptimization.Solver.Indvals{<:Integer, V}}} where V","page":"Backend","title":"PolynomialOptimization.Solver.fix_constraints!","text":"fix_constraints(state::AbstractSolver{<:Integer,V}, m::Int,\n    indvals::Indvals{I,V}) where {I,V}\n\nEnsures that all constraints in the optimization problem are fixed to the values according to indvals. This form of the function is called from primal_moment_setup!. m is the number of constraints in the solver.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.objective_indextype","page":"Backend","title":"PolynomialOptimization.Solver.objective_indextype","text":"objective_indextype(state)\n\nFor a given solver to be called using primal_moment_setup!, define the index type of the objective, which by default is the same as the global one, but can be customized.\n\nSee also psd_indextype.\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Note that these methods work with the COO representation, which can be quickly converted to either CSR or CSC using coo_to_csr! and coo_to_csc!, which respects the offset desired by the solver. Only this interface allows to set psd_indextype to a PSDIndextypeCOOVectorized; but PSDIndextypeVector is now forbidden.","category":"page"},{"location":"backend.html#Helper-functions","page":"Backend","title":"Helper functions","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The solver module exports a number of helper functions which may be of use in implementations:","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"@solver_alias\nmonomial_count\ntrisize\ncount_uniques\ncoo_to_csc!(::Integer, ::SparseMatrixCOO{I,I,V,offset}) where {I,V,offset}\ncoo_to_csr!","category":"page"},{"location":"backend.html#PolynomialOptimization.Solver.@solver_alias","page":"Backend","title":"PolynomialOptimization.Solver.@solver_alias","text":"@solver_alias(alias, original)\n\nDefines the solver identifier alias to map to the same optimization routine as original.\n\n\n\n\n\n","category":"macro"},{"location":"backend.html#PolynomialOptimization.Solver.monomial_count","page":"Backend","title":"PolynomialOptimization.Solver.monomial_count","text":"monomial_count(n, d)\n\nShort helper function that allows to determine the number of monomials in n variables up to degree d, binomn + dn. Uses the underlying cache machinery provided by ExponentsDegree.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.trisize","page":"Backend","title":"PolynomialOptimization.Solver.trisize","text":"trisize(n)\n\nReturns the number of items in the triangle of a matrix of side dimension n, fracn(n +1)2.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.count_uniques","page":"Backend","title":"PolynomialOptimization.Solver.count_uniques","text":"count_uniques(vec::AbstractVector[, callback])\ncount_uniques(vec::AbstractVector, vec::AbstractVector[, callback])\n\nReturn the number of unique elements in the vector(s), which must be sorted but may possibly contain duplicates. The callback is invoked once for every unique entry. Its first parameter is the index of the element in the unique total vector, its second (and third) is the last index/indices correponding to the element in the input vector(s). In the second form which allows to check for two vectors jointly, one of the callback parameters can be missing if the element is present only in one of the two vectors.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Solver.coo_to_csc!-Union{Tuple{offset}, Tuple{V}, Tuple{I}, Tuple{Integer, PolynomialOptimization.Solver.SparseMatrixCOO{I, I, V, offset}}} where {I, V, offset}","page":"Backend","title":"PolynomialOptimization.Solver.coo_to_csc!","text":"coo_to_csc!(nCols, s::SparseMatrixCOO)\n\nConverts a COO matrix into a CSC matrix, where the three vectors (colptr, rowvals, nzvals) are returned. The offset of s is respected. Note that some of the vectors in s are modified by this function.\n\n\n\n\n\n","category":"method"},{"location":"backend.html#PolynomialOptimization.Solver.coo_to_csr!","page":"Backend","title":"PolynomialOptimization.Solver.coo_to_csr!","text":"coo_to_csr!(nRows, s::SparseMatrixCOO)\n\nConverts a COO matrix into a CSR matrix, where the three vectors (rowptr, colvals, nzvals) are returned. The offset of s is respected. Note that some of the vectors in s are modified by this function.\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Additionally, Solver reexports a number of useful types and functions for developing the interface (see src/optimization/solver/Solver.jl); therefore, usually only the Solver submodule itself has to be used and not PolynomialOptimization itself. However, note that it is highly recommended to say using PolynomialOptimization: @assert, @inbounds in the solver implementation; this will replace the Base implementations of the macros by ones that, depending on a debugging constant, enable or disable the desired functionality.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"CurrentModule = PolynomialOptimization.Newton","category":"page"},{"location":"backend.html#[Newton.halfpolytope](@ref)","page":"Backend","title":"Newton.halfpolytope","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Finding the Newton halfpolytope requires a linear solver that supports problem modification for quick reoptimization.  All the functions here are defined in the submodule PolynomialOptimization.Newton and they are not exported.","category":"page"},{"location":"backend.html#solvers_newton","page":"Backend","title":"List of supported solvers","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The following list contains all the solvers and the required packages that provide access to the solver. The name of the solver is identical with the solver method (as a Symbol).","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Solver Package License Speed Accuracy Memory\nCOPT COPT.jl commercial   \nMosek Mosek.jl commercial   ","category":"page"},{"location":"backend.html#Solver-interface-2","page":"Backend","title":"Solver interface","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The following functions need to be implemented so that a solver is available via Newton.halfpolytope. The preprocessing functions can be omitted if no preprocessing capabilities should be provided.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"preproc\nprepare\nalloc_global\nalloc_local\nclonetask\nwork","category":"page"},{"location":"backend.html#PolynomialOptimization.Newton.preproc","page":"Backend","title":"PolynomialOptimization.Newton.preproc","text":"preproc(V, mons::IntMonomialVector, vertexindices, verbose::Bool, singlethread::Bool;\n    parameters...) -> AbstractVector{Bool}\n\nChecks for convex dependencies of exponents from the list of monomials. vertexindices might either be a Vector{Int} that indexes mons or is it Val(:all). In the former case, the convex polytope is spanned by the exponents of all monomials in mons[vertexindices] (and it can be assumed that those are independent, and that vertexindices is fairly short compared to mons); in the latter case, the convex polytope is potentially spanned by the exponents of all monomials in mons. The implementation has to return an AbstractVector{Bool} that for every monomial in mons indicates whether the convex hull spanned by the elements just described (excluding the monomial in question) already contains the monomial. An entry must be false if and only if this is possible, i.e., it is redundant. singlethread will be true if this function will be called in parallel by multiple threads, so that the linear solver itself should be single-threaded.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Newton.prepare","page":"Backend","title":"PolynomialOptimization.Newton.prepare","text":"prepare(V, vertices::IntMonomialVector, num::Int, verbose::Bool; parameters...) ->\n    (nthreads::Int, userdata, userdata_clone)\n\nThis function is responsible for creating an optimization task that can be used to check membership in the Newton halfpolytope. The vertices of the polytope are given by the exponents of mons. The total number of monomials that have to be checked is given by num. The function must return the number of threads that will be used to carry out the optimization (which is not the number of threads that the optimizer uses internally, but determines how PolynomialOptimization will distribute the jobs), an internal optimization task that is passed on to work, and a copy of this task for use in a second thread if the number of threads is greater than one, else nothing. More copies will be created as required by clonetask; however, assuming that setting up the task will potentially require more resources than cloning a task allows the function to estimate the required memory (and therefore a sensible number of threads) better by already performing one clone.\n\nSee also @allocdiff.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Newton.alloc_global","page":"Backend","title":"PolynomialOptimization.Newton.alloc_global","text":"alloc_global(V, nv)\n\nThis function is called once in the main thread before work is executed (which, due to multithreading, might occur more than once). It can create some shared data that is used in a read-only manner by all workers at the same time. The default implementation of this function does nothing.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Newton.alloc_local","page":"Backend","title":"PolynomialOptimization.Newton.alloc_local","text":"alloc_local(V, nv)\n\nThis function is called once in every computation thread before work is executed (which, due to task splitting, might occur more than once). It can create some shared data that is available for reading and writing by every worker. The default implementation of this function does nothing.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Newton.clonetask","page":"Backend","title":"PolynomialOptimization.Newton.clonetask","text":"clonetask(t)\n\nThis function must create a copy of the optimization task t that can run in parallel to t in a different thread.\n\n\n\n\n\n","category":"function"},{"location":"backend.html#PolynomialOptimization.Newton.work","page":"Backend","title":"PolynomialOptimization.Newton.work","text":"work(V, task, data_global, data_local, expiter::IntMonomialVectorIterator{true},\n    progress::Ref{Int}, acceptance::Ref{Int}, add_callback::Function,\n    iteration_callback::Union{Nothing,Function})\n\nIterates through expiter (which gives a tuple consisting of the index and the exponents) and for every exponent checks whether this it can be obtained by a convex combination of the coefficients as they are set up in task. If yes, add_callback must be called with the exponent index as a parameter, and acceptance should be incremented. In any case, progress should be incremented. Additionally, after every check, iteration_callback should be called with no parameters, if it is a function. The data parameters contain the custom data that was previously generated using alloc_global and alloc_local. Only data_local may be mutated.\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Once a solver has been implemented, it should add its solver symbol to the vector Newton.newton_methods, which enables this solver to be chosen automatically.","category":"page"},{"location":"backend.html#Automatic-tightening","page":"Backend","title":"Automatic tightening","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"CurrentModule = PolynomialOptimization","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Automatic tightening of a polynomial optimization problem requires a linear solver that finds a solution to a system of linear equations that minimizes the  norm (better yet, the -norm, if you can implement this). The solver is only called if the number of rows is smaller than the number of columns; else, the solution is calculated using SPQR's direct solver.","category":"page"},{"location":"backend.html#solvers_tighten","page":"Backend","title":"List of supported solvers","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The following list contains all the solvers and the required packages that provide access to the solver. The name of the solver is identical with the solver method (as a Symbol).","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Solver Package License Speed Accuracy Memory\nCOPT COPT.jl commercial   \nMosek Mosek.jl commercial   ","category":"page"},{"location":"backend.html#Solver-interface-3","page":"Backend","title":"Solver interface","text":"","category":"section"},{"location":"backend.html","page":"Backend","title":"Backend","text":"The following function needs to be implemented so that a solver is available via automatic tightening.","category":"page"},{"location":"backend.html","page":"Backend","title":"Backend","text":"tighten_minimize_l1","category":"page"},{"location":"backend.html#PolynomialOptimization.tighten_minimize_l1","page":"Backend","title":"PolynomialOptimization.tighten_minimize_l1","text":"tighten_minimize_l1(V, spmat::SparseMatrixCSC, rhs::Vector)\n\nComputes a solution to the underdetermined linear system spmat * x = rhs with minimal -norm.\n\n\n\n\n\n","category":"function"},{"location":"backend.html","page":"Backend","title":"Backend","text":"Once a solver has been implemented, it should add its solver symbol to the vector PolynomialOptimization.tightening_methods, which enables this solver to be chosen automatically.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"DocTestFilters = [\n  r\"\\d+\\.\\d+ seconds\" => \"seconds\",\n  r\"\\d\\.\\d+e-\\d+\" => s\"~0~\",\n  r\"(\\d*)\\.(\\d{6})\\d+\" => s\"\\1.\\2~\"\n]\nDocTestSetup = :(import Random; Random.seed!(1234))\nCurrentModule = PolynomialOptimization","category":"page"},{"location":"guide.html#Walkthrough","page":"Walkthrough","title":"Walkthrough","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"We start the Julia session by including the required packages.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> using PolynomialOptimization, DynamicPolynomials","category":"page"},{"location":"guide.html#A-simple-unconstrained-problem","page":"Walkthrough","title":"A simple unconstrained problem","text":"","category":"section"},{"location":"guide.html#Constructing-the-problem","page":"Walkthrough","title":"Constructing the problem","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Next, we define some simple optimization problem.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> @polyvar x[1:3];\n\njulia> prob = poly_problem(1 + x[1]^4 + x[2]^4 + x[3]^4 + x[1]^2*x[2]^2 + x[1]^2*x[3]^2 + x[2]^2*x[3]^2 + x[2]*x[3])\nReal-valued polynomial optimization problem in 3 variables\nObjective: 1.0 + xx + x + xx + x + xx + xx + x","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"This is a very simple problem: We have three variables and want to minimize an unconstrained objective function. Currently, prob is just an instance of a Problem: some elementary checks and conversions have been done, but the heavy machinery of polynomial optimization was not applied yet. During the process of constructing the problem, it is possible to automatically perform modifications. These are available via keyword parameters of poly_problem.","category":"page"},{"location":"guide.html#Densely-solving-the-problem","page":"Walkthrough","title":"Densely solving the problem","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Since this problem is so small, we can solve it directly without any sparsity consideration. Note that PolynomialOptimization works with a variety of solvers; however, they are included only as weak dependencies. You have to load the appropriate solver Julia package first to make the solvers available. For a list of supported solvers, see the documentation for poly_optimize or the details section.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> import Clarabel\n\njulia> res = poly_optimize(:Clarabel, prob)\n[ Info: Automatically selecting minimal degree cutoff 2\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): 0.9166666672624658\nTime required for optimization: 0.9356329 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"The solution that we found was indeed optimal and the value is 0.9166.... Note that \"optimal\" here means that the solver converged for the given problem. However, a polynomial optimization problem is difficult to solve in general; therefore, it cannot be optimized directly. Instead, a relaxation of the problem has to be constructed. Normally, this must be done explicitly by instantiating a decendant of AbstractRelaxation; but when poly_optimize is called with a problem instead of a relaxation, it will by default construct a dense relaxation of minimal degree. Therefore, \"optimal\" in fact only means that the relaxation was solved to global optimality, which in general will only yield an underestimator to the original problem. Note that while Clarabel has a very clear return code - SOLVED says that things went well - this is not necessarily the case for other solvers. Use issuccess on the result object to check whether the reported solver status is a good one:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> issuccess(res)\ntrue","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Further note that the optimization time seems to be pretty high for such a small problem. However, this is purely due to the compilation time. Running the optimization again will give a time of the order of a millisecond. Finally, it is not necessary to specify the solver explicitly if only one solver package is loaded; poly_optimize(prob) would work as well. However, if multiple solvers are available, which one is then chosen may depend on the loading order of the packages.","category":"page"},{"location":"guide.html#Checking-optimality","page":"Walkthrough","title":"Checking optimality","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"There are two different ways to check whether the given bound is optimal for the original problem. Obviously, if we find a point x such that the objective evaluated at x gives our bound, then this bound must have been optimal. Of course, the difficulty now lies in finding the point. PolynomialOptimization implements a state of the art solution extraction algorithm, which can relatively quickly (the cost is essentially that of performing an SVD on the moment matrix) obtain solutions. This will only be guaranteed to work if the problem was indeed optimal, there were finitely many solutions in the first place, and a \"good\" moment matrix is obtained (i.e., a dense matrix and no low-rank solver was employed) - but there is an alternative which might work well in case these conditions are not satisfied (apart from optimality, obviously), more on this below. The function poly_solutions gives an iterator that delivers all the (potential) solutions one at a time in an arbitrary order. Alternatively, poly_all_solutions directly calculates all the solutions and grades them according to how much they violate the bound or constraints, if any were given. The solutions are then returned in a best-to-worst order.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> poly_all_solutions(res)\n2-element Vector{Tuple{Vector{Float64}, Float64}}:\n ([-1.1700613807653743e-18, 0.4082426580485429, -0.408242660645461], 5.266275193704928e-10)\n ([-5.352664236308434e-20, -0.4082426580485437, 0.40824266064546183], 5.266276303927953e-10)","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Every element in the vector is a tuple, where the first entry corresponds to the optimal variables, and the second term is the badness of this solution (which can also be calculated manually using poly_solution_badness). Since here, the badness is of the order of 10^-8, i.e., numerically zero, the points are indeed valid global minima.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Note that the solution extraction functions just give a vector of numbers; to assign these numbers to variables, the function variables can be applied to the problem. This is particularly useful if there are multiple variables created at different times, occurring differently in the constraints, such that the order is not clear beforehand.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> variables(prob)\n3-element Vector{Variable{DynamicPolynomials.Commutative{DynamicPolynomials.CreationOrder}, Graded{LexOrder}}}:\n x\n x\n x","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"warning: Variables in the problem\nA polynomial optimization problem is always constructed using poly_problem, and this function supports any input that implements the MultivariatePolynomials interface. In the example here, we used DynamicPolynomials, which is probably the most common choice. However, these inputs are not necessarily well-suited to deliver high performance when constructing the actual optimizations. Therefore, poly_problem will convert all inputs to an internal polynomial format. This internal format does not know about the name of variables - real-valued variables will always be printed as x_i, complex-valued variables as z_j with continuous indices. The variables function now becomes even more important: It contains the original variables of the polynomials that were given to poly_problem.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"A second way to check for optimality is to use the flat extension/truncation criterion originally due to Curto and Fialkow and improved by Nie. This is a sufficient criterion for optimality, and it can be manually checked by calling optimality_certificate on the problem (it will only work with non-sparse problems). The function will return :Optimal if the given minimum value can be certified to be optimal; else it will return :Unknown:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> optimality_certificate(res)\n:Optimal","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Note that this is just a sufficient criterion, and the solution might be optimal even if it is violated. As calculating the certificate will involve calculating the ranks of several matrices (and is more complicated in the complex case), it is not necessarily cheaper than trying to extract solutions; as the latter is more informative, it should usually be the way to go.","category":"page"},{"location":"guide.html#Extracting-a-SOS-certificate","page":"Walkthrough","title":"Extracting a SOS certificate","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Whenever the optimization was successful, a valid sums-of-squares certificate will be available, i.e., a decomposition of the objective (in this simple, unconstrained, case). Here, the minimum value of the objective was found to be 0.9166.... We can therefore obtain a certificate for the positivity of the original objective minus this global minimum:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"# Don't test this, it appears to give very different results even on the same machine depending on whether the test runs\n# directly or via Pkg. The solution is not unique anyway and we have enough tests of the SOS certificate that check what must\n# reliably be the same.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> cert = SOSCertificate(res)\nSum-of-squares certificate for polynomial optimization problem\n1.0 + xx + x + xx + x + xx + xx + x - 0.9166666672624658\n= (-0.24936226129389166 + 0.0 + 0.0 + 0.0 + 0.681040802546548x - 0.13418414698255413xx + 0.6810200107054156x + 0.0 + 0.0 + 0.7150557592772305x)\n+ (-0.00021451050140762083 + 0.0 + 0.0 + 0.0 - 0.6334453950216662x + 0.0016567627201615803xx + 0.6363889265439937x + 0.0 + 0.0 - 0.00254891018011664x)\n+ (-0.05053765946744563 + 0.0 + 0.0 + 0.0 + 0.35808256829743135x + 0.407798788978473xx + 0.3528714670120098x + 0.0 + 0.0 - 0.6182223086852784x)\n+ (0.0 - 0.6308015756295695x - 0.6308015765942243x + 0.0 + 0.0 + 0.0 + 0.0 + 0.0 + 0.0 + 0.0)\n+ (0.0 + 0.0 + 0.0 + 0.0 + 0.0 + 0.0 + 0.0 - 0.571599380223713xx - 0.5716874595884525xx + 0.0)\n+ (-0.1363733706053911 + 0.0 + 0.0 + 0.0 + 0.08189769928366018x - 0.65436868542136xx + 0.0818694633768503x + 0.0 + 0.0 - 0.32632796895148375x)\n+ (0.0 + 0.0 + 0.0 + 0.0 + 0.0 + 0.0 + 0.0 + 0.4384985146463991xx - 0.43843095558075706xx + 0.0)\n+ (0.0 + 0.0 + 0.0 - 0.45290489466060885x + 0.0 + 0.0 + 0.0 + 0.0 + 0.0 + 0.0)","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"This certificate consists of a number of polynomials that, when squared and added, should give rise to the original objective. Note that when printing the certificate, values that are below a certain threshold will be set to zero by default. We can also explicitly iterate through all the polynomials and sum them up, although we have to be careful to map them back to their original representation for this:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> p = zero(polynomial_type(x, Float64));\n\njulia> for p in cert[:objective, 1] # no sparsity, so there is just a single grouping\n           p += PolynomialOptimization.change_backend(p, x)^2\n       end\n\njulia> map_coefficients!(x -> round(x, digits=8), p)\n0.08333334 + xx + x + xx + x + xx + xx + x","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Note how this is precisely the objective minus the global minimum.","category":"page"},{"location":"guide.html#Using-the-Newton-polytope","page":"Walkthrough","title":"Using the Newton polytope","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"The current example is an unconstrained optimization problem; hence, the size of the full basis, which is 10, may be larger than actually necessary. It is not a simple problem to determine the relevant basis elements in general; but unconstrained problems allow for the Newton polytope technique. To use it, we first need to load a supported solver for the Newton polytope, then we simply explicitly construct the Newton relaxation object:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> import Mosek\n\njulia> Relaxation.Newton(prob)\n[ Info: Automatically selecting minimal degree cutoff 2\nRelaxation.Newton of a polynomial optimization problem\nVariable cliques:\n  x[1], x[2], x[3]\nPSD block sizes:\n  [10 => 1]\nRelaxation degree: 2","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"In this case, no basis reduction was possible. However, in other cases, this can work. For example, if you want to inspect the Newton polytope of the polynomials whose squares might make up a certain objective, you can call Newton.halfpolytope directly (the name comes from the fact that you pass the objective to the function Newton.halfpolytope, and by a theorem by Reznick, the Newton polytope of the decomposition functions that have to be squared to give the objective will be contained in half the Newton polytope of the objective itself):","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> @polyvar x y;\n\njulia> Newton.halfpolytope(x^4*y^2 + x^2*y^4 - 3x^2*y^2 +1)\n4-element MonomialVector{DynamicPolynomials.Commutative{DynamicPolynomials.CreationOrder}, Graded{LexOrder}}:\n 1\n xy\n xy\n xy","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"This reveals that, were the Motzkin representable by a sum of squares, the equality","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"x^4 y^2 + x^2 y^4 - 3 x^2 y^2 + 1 = sum_i (alpha_i + beta_i x y + gamma_i x y^2 + delta_i x^2 y)^2","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"would have to hold; but expanding the right-hand side will lead to the coefficient sum_i beta_i^2 in front of the monomial x^2 y^2, which cannot be negative; hence, the Motzkin polynomial is not a sum of squares.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Note that the calculation of the Newton polytope currently requires Mosek or COPT. There are some preprocessing options that may be able to speed up the calculation, although it is already extremely fast by itself and can calculate the correct basis for objectives with hundreds of terms in a decent time (which can be further reduced by multithreading or distributed computing). Check out the documentation for Newton.halfpolytope for more information.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"In case you already happen to know a (better) choice of basis, you may opt for Relaxation.Custom. Note that relaxations are built incrementally, where the only relaxation that can be constructed directly from a problem is the dense one. So what actually happened in calling Relaxation.Newton(prob) is that first a dense relaxation of the problem was constructed, which was then passed on to the Newton relaxation constructor: Relaxation.Newton(Relaxation.Dense(prob)). The info message about the minimal degree cutoff was generated by the dense relaxation. \"Construction\" here does not mean that the full dense basis was actually built in memory; a lazy representation is used for the dense basis. This has the consequence that if you use a custom basis, you can then decide to refine this further by passing the custom relaxation to the Newton relaxation constructor. Similarly, every relaxation can serve as the starting point for another one.","category":"page"},{"location":"guide.html#Applying-inexact-sparsity","page":"Walkthrough","title":"Applying inexact sparsity","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"There are four kinds of inexact sparsity method implemented in PolynomialOptimization:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Relaxation.SparsityCorrelative\nRelaxation.SparsityTermBlock\nRelaxation.SparsityTermChordal\nRelaxation.SparsityCorrelativeTerm","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Applying the analysis as simple as passing the problem to the respective type. For this particular problem, there is no correlative sparsity:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> Relaxation.SparsityCorrelative(prob)\n[ Info: Automatically selecting minimal degree cutoff 2\nRelaxation.SparsityCorrelative of a polynomial optimization problem\nVariable cliques:\n  x[1], x[2], x[3]\nPSD block sizes:\n  [10 => 1]","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"So there is only a single clique, leading to a basis of size 10. However, there is term sparsity:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> tbs = Relaxation.SparsityTermBlock(prob)\n[ Info: Automatically selecting minimal degree cutoff 2\nRelaxation.SparsityTerm of a polynomial optimization problem\nVariable cliques:\n  x[1], x[2], x[3]\nPSD block sizes:\n  [5 => 1, 2 => 1, 1 => 3]","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"We get a basis of size 5, one of size 2, and three bases of size 1 (here, by basis we mean a set of monomials that indexes the moment/SOS matrices). PolynomialOptimization will model these by a 5  5 semidefinite matrix, a rotated second-order cone, as well as three linear constraints. This is much cheaper than a 10  10 semidefinite matrix. Let's optimize the sparse problem:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> poly_optimize(:Clarabel, tbs)\nPolynomial optimization result\nRelaxation method: SparsityTerm\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): 0.9166666718972408\nTime required for optimization: 0.00306 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Again, we get the same optimal value, so introducing the sparsity did not make our relaxation worse (which is per se not guaranteed), and we are still able to get the same optimal solutions:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> poly_all_solutions(ans)\n2-element Vector{Tuple{Vector{Float64}, Float64}}:\n ([0.0, 0.4082661947158492, -0.408266194715714], 4.589421509493263e-9)\n ([0.0, -0.4082661947158492, 0.408266194715714], 4.589421509493263e-9)","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Note that perhaps surprisingly, PolynomialOptimization can still deliver good optimal points despite the fact that term sparsity was in effect. The usual extraction algorithms will fail, as for every moment m, they also require the moment m x to be present for all variables x - term sparsity cannot to provide this (just check that the moment matrix contains lots of NaN values). Hence, poly_all_solutions will automatically switch to a different heuristic solution extraction algorithm that is always successful in the simple case of a rank-1 moment matrix, but can also often also give good results in the more general case such as here. As a rule of thumb, if all the solutions encoded in the moment matrix differ only by the signs or phases of individual components, the heuristic will be successful. Still, the fact that the moments may encode multiple solutions may be an issue that can prevent successfully obtaining a solution vector. We will introduce a way to bypass this problem below.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Assume that our term sparsity gave a worse bound than the dense case (which in general we would not know, since the dense problem is typically far too large to be solved; but we just don't get a proper optimal point, though this could also be due to the Lasserre hierarchy level being insufficient). Then, we could try to iterate the term sparsity hierarchy, keeping the same level in the Lasserre hierarchy.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> Relaxation.iterate!(tbs)\nRelaxation.SparsityTerm of a polynomial optimization problem\nVariable cliques:\n  x[1], x[2], x[3]\nPSD block sizes:\n  [5 => 1, 2 => 2, 1 => 1]\n\njulia> Relaxation.iterate!(tbs)","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"In general, we simply use Relaxation.iterate! to move to the next higher level (note how two of the linear constraints were merged into a quadratic constraint). If iterate! returns the new sparsity object, something changed and we might try to optimize the new sparse problem, getting a potentially better bound. If iterate! returns nothing, the hierachy terminated and nothing more can (for term sparsity: must) be done (as the last level for term sparsity is as good as the dense problem).","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"We can also try what happens if we use term sparsity with chordal cliques instead of connected components:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> tcs = Relaxation.SparsityTermChordal(prob)\n[ Info: Automatically selecting minimal degree cutoff 2\nRelaxation.SparsityTerm of a polynomial optimization problem\nVariable cliques:\n  x[1], x[2], x[3]\nPSD block sizes:\n  [4 => 1, 2 => 2, 1 => 3]\n\njulia> res = poly_optimize(:Clarabel, tcs)\nPolynomial optimization result\nRelaxation method: SparsityTerm\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): 0.9166666672685418\nTime required for optimization: 0.0027146 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"So again, we get the same optimal result. We could even extract a solution point with better accuary, and the problem was smaller, as we now have just a basis of size 4 instead of 5 (at the cost of another quadratic constraint, which is much cheaper than larger semidefinite matrices).","category":"page"},{"location":"guide.html#details","page":"Walkthrough","title":"Details on the optimization process","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"The first parameter for poly_optimize is the solver/method that is used to optimize the problem. For a list of supported methods, see the solver reference.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Note that by passing the keyword argument verbose=true to the optimization function, we get some more insight into what happens behind the hood. Let's redo the last optimization.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> poly_optimize(:Clarabel, tcs, verbose=true)\nBeginning optimization...\nClique merging disabled.\nPSD block sizes:\n  [4 => 1, 2 => 2, 1 => 3]\nStarting solver...\nSetup complete in 0.000266 seconds\n-------------------------------------------------------------\n           Clarabel.jl v0.9.0  -  Clever Acronym\n                   (c) Paul Goulart\n                University of Oxford, 2022\n-------------------------------------------------------------\n\nproblem:\n  variables     = 11\n  constraints   = 20\n  nnz(P)        = 0\n  nnz(A)        = 24\n  cones (total) = 7\n    : Zero        = 1,  numel = 1\n    : Nonnegative = 3,  numel = (1,1,1)\n    : SecondOrder = 2,  numel = (3,3)\n    : PSDTriangle = 1,  numel = 10\n\nsettings:\n  linear algebra: direct / qdldl, precision: Float64\n  max iter = 200, time limit = Inf,  max step = 0.990\n  tol_feas = 1.0e-08, tol_gap_abs = 1.0e-08, tol_gap_rel = 1.0e-08,\n  static reg : on, 1 = 1.0e-08, 2 = 4.9e-32\n  dynamic reg: on,  = 1.0e-13,  = 2.0e-07\n  iter refine: on, reltol = 1.0e-13, abstol = 1.0e-12,\n               max iter = 10, stop ratio = 5.0\n  equilibrate: on, min_scale = 1.0e-04, max_scale = 1.0e+04\n               max iter = 10\n\niter    pcost        dcost       gap       pres      dres      k/t               step\n---------------------------------------------------------------------------------------------\n  0   1.0000e+00   1.0000e+00  0.00e+00  4.99e-01  5.83e-01  1.00e+00  1.91e+00   ------\n  1   1.0240e+00   1.0238e+00  2.08e-04  6.91e-02  8.37e-02  9.27e-02  3.10e-01  9.09e-01\n  2   9.2888e-01   9.2859e-01  2.93e-04  5.10e-03  6.37e-03  6.34e-03  2.54e-02  9.21e-01\n  3   9.1842e-01   9.1835e-01  7.08e-05  1.12e-03  1.42e-03  1.40e-03  5.85e-03  8.71e-01\n  4   9.1762e-01   9.1759e-01  3.15e-05  4.04e-04  5.09e-04  4.96e-04  2.15e-03  7.53e-01\n  5   9.1673e-01   9.1672e-01  3.94e-06  5.76e-05  7.26e-05  7.15e-05  3.06e-04  8.64e-01\n  6   9.1668e-01   9.1668e-01  4.44e-07  7.88e-06  9.94e-06  9.86e-06  4.16e-05  9.30e-01\n  7   9.1667e-01   9.1667e-01  1.05e-07  1.48e-06  1.86e-06  1.83e-06  7.83e-06  8.23e-01\n  8   9.1667e-01   9.1667e-01  3.09e-08  3.49e-07  4.40e-07  4.26e-07  1.85e-06  8.84e-01\n  9   9.1667e-01   9.1667e-01  6.54e-09  6.68e-08  8.41e-08  8.08e-08  3.54e-07  8.23e-01\n 10   9.1667e-01   9.1667e-01  1.35e-09  1.32e-08  1.66e-08  1.59e-08  6.99e-08  9.04e-01\n 11   9.1667e-01   9.1667e-01  2.79e-10  2.51e-09  3.16e-09  3.01e-09  1.34e-08  8.23e-01\n---------------------------------------------------------------------------------------------\nTerminated with status = solved\nsolve time = 5.69ms\nOptimization complete, retrieving moments\nPolynomial optimization result\nRelaxation method: SparsityTerm\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): 0.9166666672685418\nTime required for optimization: 0.1599203 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"So first, PolynomialOptimization will determine the bases for the matrices according to the sparsity pattern. Note that after this step, the if the resulting relaxation is wrapped into a Relaxation.CliqueMerged, an attempt will be made to merge bases if their heuristic cost for treating them separately would be worse than joining them (this concept is nicely explained in the COSMO documenation). In general, doing clique merging will lead to faster optimizations; however, the merging process itself can be quite costly and in fact for large problems might cost much more time than it gains - hence, it must be enabled by explicitly constructing the merged relaxation. After this step is done, the Clarabel data (or any other optimizer structure, which we all address directly without JuMP) is constructed; then the solver runs.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Indeed, due to sparsity, the moment matrix is full of unknowns:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> show(stdout, \"text/plain\", moment_matrix(res))\n1010 LinearAlgebra.Symmetric{Float64, Matrix{Float64}}:\n   1.0         NaN         NaN         NaN             0.166666    -0.166666     0.166666   NaN          NaN            1.60942e-8\n NaN             0.166666   -0.166666  NaN           NaN          NaN          NaN          NaN          NaN          NaN\n NaN            -0.166666    0.166666  NaN           NaN          NaN          NaN          NaN          NaN          NaN\n NaN           NaN         NaN           1.60942e-8  NaN          NaN          NaN          NaN          NaN          NaN\n   0.166666    NaN         NaN         NaN             0.0277777  NaN            0.0277777  NaN          NaN            2.5981e-9\n  -0.166666    NaN         NaN         NaN           NaN            0.0277777  NaN          NaN          NaN          NaN\n   0.166666    NaN         NaN         NaN             0.0277777  NaN            0.0277777  NaN          NaN            2.5981e-9\n NaN           NaN         NaN         NaN           NaN          NaN          NaN            2.5981e-9  NaN          NaN\n NaN           NaN         NaN         NaN           NaN          NaN          NaN          NaN            2.5981e-9  NaN\n   1.60942e-8  NaN         NaN         NaN             2.5981e-9  NaN            2.5981e-9  NaN          NaN           -3.42786e-9","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"The rows and columns of the matrix are indexed by the basis of the relaxation:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> Relaxation.basis(tcs)\n10-element PolynomialOptimization.IntPolynomials.IntMonomialVector{3, 0, UInt64, PolynomialOptimization.IntPolynomials.MultivariateExponents.ExponentsDegree{3, UInt64}, PolynomialOptimization.IntPolynomials.IntMonomial{3, 0, UInt64, PolynomialOptimization.IntPolynomials.MultivariateExponents.ExponentsDegree{3, UInt64}}}:\n 1\n x\n x\n x\n x\n xx\n x\n xx\n xx\n x","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Combining the basis information with the moment matrix, we can see how the package is able to return solutions without having access to the full moment matrix. There are values for the squares of the variables available, so we can deduce two possible candidates for the original variables - at least, if the values assigned to the moments are consistent. Choosing among the signs becomes possible by looking for mixed terms.","category":"page"},{"location":"guide.html#Always-extracting-a-solution","page":"Walkthrough","title":"Always extracting a solution","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"The fact that PolynomialOptimization was unable to extract a (valid) solution can either mean that the relaxation was insufficient and did not converge to the optimum of the actual problem, or that there are multiple solutions which are too difficult for the heuristic to grasp. There is a simple remedy of this problem: By introducing a small, linear perturbation to the objective, the solution will almost surely be unique; so if the extracted solution is bad, this means that the relaxation was insufficient. Of course, now the solution is not a solution to the original problem, but the perturbed one - but assuming a robust problem, the returned optimal point will be close to the actual global optimum. It can therefore then be used as an initial point to another nonlinear solver that will deliver the true global optimum. PolynomialOptimization makes adding a perturbation easy: Just call poly_problem with the keyword parameter perturbation=..., where the magnitude of the perturbation should be specified (typically between 1e-3 and 1e-6 is a good guess). Note that adding a perturbation may degrade sparsity. For this, you may also give a vector of the same length as the number of variables, specifying a different perturbation magnitude for each variable (or just disabling the perturbation by passing 0).","category":"page"},{"location":"guide.html#Changing-the-internal-representation","page":"Walkthrough","title":"Changing the internal representation","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Usually, the moment matrix is modeled as a semidefinite constraint in the solver. However, semidefinite programs scale much less favorably than other types of convex optimization programs such as linear or quadratic ones. It is possible to change between the internal representations that are used for the moment matrix: apart from semidefinite, also the diagonally dominant and scaled diagonally dominant cones are supported. While they scale better, they usually provide worse bounds:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> res_dd = poly_optimize(:Clarabel, prob, representation=RepresentationDD())\n[ Info: Automatically selecting minimal degree cutoff 2\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): 0.5000000105896211\nTime required for optimization: 0.5579216 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Again, the long time is purely due to precompilation of the new methods.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"This time, no semidefinite constraint was employed, only linear ones. As is clearly visible, the bound is quite bad. But this process can be iterated and the diagonally dominant cone can be rotated based on the data from the previous optimization. This is call re-optimization, and while a lot of details can be customized, if none are specified, PolynomialOptimization will take the Cholesky decomposition of the matrix underlying the SOS certificate of the previous iteration as a new rotation basis.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"info: Info\nDifferent representations lead to different problems from the perspective of the solver - in particular, different with respect to which solver functions are used or how the data is aligned in memory. Therefore, changing the type of the representation requires a completely new problem to be set up. However, if only the data within the representatin is changed, the old problem can be re-used, reducing the setup time. This is irrelevant if a solver is used whose interface in PolynomialOptimization does not support this faster reoptimization. Note that by default, the default rotations used for the DD and SDD representation are identities of the type UniformScaling. This is only compatible with other diagonal rotations. Use RepresentationIAs to \"fake\" an upper triangular identity at the beginning.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"# Beware the Documenter bug https://github.com/JuliaDocs/Documenter.jl/issues/2277 (which I consider a bug): the filter must\n# match both the original output as well as the expected output in order to be applied to either - so let's make it so.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> res_dd_rotated = poly_optimize(res_dd)\n# output truncated\nLower bound to optimum (in case of good status): 0.7882579368640298\nTime required for optimization: 1.5101965 seconds\n\njulia> res_dd_rotated = poly_optimize(res_dd_rotated)\n# output truncated\nLower bound to optimum (in case of good status): 0.8744697855281065\nTime required for optimization: 0.0355882 seconds\n\njulia> res_dd_rotated = poly_optimize(res_dd_rotated)\n# output truncated\nLower bound to optimum (in case of good status): 0.9122828985708029\nTime required for optimization: 0.0097173 seconds\n\njulia> res_dd_rotated = poly_optimize(res_dd_rotated)\n# output truncated\nLower bound to optimum (in case of good status): 0.9160797358944991\nTime required for optimization: 0.0102907 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"So indeed, after a couple of iterations, the optimum is approached pretty well. We could have used the scaled diagonally dominant representation instead, which relies on quadratic instead of linear programs, which for this particular example would have been exact without any iteration.","category":"page"},{"location":"guide.html#Constraints","page":"Walkthrough","title":"Constraints","text":"","category":"section"},{"location":"guide.html#Equality-constraints","page":"Walkthrough","title":"Equality constraints","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Equality constraints are accessible by passing the keyword argument zero to poly_problem, which constrains those polynomials to be zero. They are relatively cheap to realize in the solver, as they don't require another semidefinite matrix, just linear constraints or free scalar variables depending on the approach.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> @polyvar x[1:2];\n\njulia> poly_optimize(:Clarabel, poly_problem(-(x[1] -1)^2 - (x[1] - x[2])^2 - (x[2] -3)^2,\n                                             zero=[(x[1] - 1.5)^2 + (x[2] - 2.5)^2 - .5]), 1)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): -3.999999965663831\nTime required for optimization: 0.0014009 seconds\n\njulia> poly_all_solutions(ans)\n1-element Vector{Tuple{Vector{Float64}, Float64}}:\n ([1.0000155981763819, 3.0000155844516696], 2.0076497353471723e-8)\n","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Note that when grading the quality of a solution, the package will determine the violation of the constraints as well as how far the actual value is away from what it should be, and return the worst of all violations. Note that in principle, Grbner basis methods would allow to incorporate equality constraints with a potentially even higher reduction in the number of variables. While an early version of PolynomialOptimization supported this, any Grbner basis method has been removed from the package. Experience showed that the cost of calculating a Grbner basis can easily be many times larger than working with the original problem; furthermore, then taking everything modulo this basis prevents some optimizing assumptions to be made during the problem construction. Lastly, removing a variable or constraint does not help a lot with respect to scaling, as the main issue is the size of the semidefinite cones - which would be given by a basis of standard monomials with respect to the Grbner basis, and the savings there are often minuscule.","category":"page"},{"location":"guide.html#Inequality-constraints","page":"Walkthrough","title":"Inequality constraints","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Inequality constraints are implemented using Putinar's Positivstellensatz or localizing matrices. They can be specified by passing the keyword argument nonneg to poly_problem, which constraints those polynomials to be greater or equal to zero.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> @polyvar x[1:2];\n\njulia> prob = poly_problem(-(x[1]-1)^2 - (x[1]-x[2])^2 - (x[2]-3)^2,\n                           nonneg=[1-(x[1]-1)^2, 1-(x[1]-x[2])^2, 1-(x[2]-3)^2])\nReal-valued polynomial optimization problem in 2 variables\nObjective: -10.0 + 6.0x + 2.0x - 2.0x + 2.0xx - 2.0x\n3 nonnegative constraints\n1: 2.0x - x  0\n2: 1.0 - x + 2.0xx - x  0\n3: -8.0 + 6.0x - x  0\n\njulia> poly_optimize(:Clarabel, prob, 1)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): -2.9999999986040407\nTime required for optimization: 0.002556 seconds\n\njulia> poly_optimize(:Clarabel, prob, 2)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: ALMOST_SOLVED\nLower bound to optimum (in case of good status): -2.000000014367033\nTime required for optimization: 0.0057509 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"This is an example where the first relaxation level is not optimal, but the second is, as inspecting the solutions will show (which also allows us to ignore the somewhat uncertain status of the solver).","category":"page"},{"location":"guide.html#PSD-constraints","page":"Walkthrough","title":"PSD constraints","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"PolynomialOptimization also supports conditions that constrain a matrix that is made up of polynomials to be positive semidefinite. They can be specified by passing the keyword argument psd to poly_problem; note that the matrices must be symmetric/hermitian.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> @polyvar x[1:2];\n\njulia> prob = poly_problem(-x[1]^2 - x[2]^2, zero=[x[1]+x[2]-1],\n                           psd=[[1-4x[1]*x[2]  x[1]; x[1]  4-x[1]^2-x[2]^2]])\nReal-valued polynomial optimization problem in 2 variables\nObjective: -x - x\n1 equality constraint\n1: -1.0 + x + x = 0\n1 semidefinite constraint\n2: [1.0 - 4.0xx  x\n    x             4.0 - x - x]  0\n\njulia> poly_optimize(:Clarabel, prob, 1)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): -3.999999994240309\nTime required for optimization: 0.1618452 seconds\n\njulia> poly_optimize(:Clarabel, prob, 2)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): -3.904891539034092\nTime required for optimization: 0.0041964 seconds\n\njulia> optimality_certificate(ans)\n:Optimal","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"At second level, we get the optimal solution.","category":"page"},{"location":"guide.html#Improving-the-optimization-without-changing-the-level","page":"Walkthrough","title":"Improving the optimization without changing the level","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"The problem can be further tightened by a careful analysis, as Nie noted, by rewriting the Lagrange multipliers as polynomials - which will not modify the problem if the minimum is attained at a critical point (but not that non-critical global minima will be missed). PolynomialOptimization is able to automatically analyze the problem and add the tightening constraints (Mosek or COPT are required at the moment). For this, simply pass tighter=true (or tighter=:Mosek resp. tighter=:COPT) to poly_problem. This will result in a preprocessing that adds constraints, so expect the problem to grow. To see the progress during the preprocessing stage, use verbose=true. It may be the case that the required tightening polynomials cannot be determined since their degree always turns out to be insufficient to satisfy the conditions. Since PolynomialOptimization cannot distinguish this from the case where the degree is just quite high, the procedure may run into an infinite(ly-seeming) loop. Complex-valued problems are not supported at the moment; and PSD constraints will be skipped during the tightening.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> @polyvar x y;\n\njulia> poly_optimize(:Clarabel, poly_problem(x^4*y^2 + x^2*y^4 - 3x^2*y^2 +1), 5)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: ALMOST_SOLVED\nLower bound to optimum (in case of good status): -1.5097034199113162\nTime required for optimization: 0.4214607 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"The given problem is quite hard, as it leads to ill-posed optimization problems with which most solvers expectedly struggle (in fact, while Clarabel gives a solution, the result will differ significantly depending on the operating system). Adding the tightening equalities (here, as there are no additional constraints, this just means to add the condition nablamathrmobjective = 0), the fifth order is already sufficient:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> prob = poly_problem(x^4*y^2 + x^2*y^4 - 3x^2*y^2 +1, tighter=true)\nReal-valued polynomial optimization problem in 2 variables\nObjective: 1.0 - 3.0xx + xx + xx\n2 equality constraints\n1: -6.0xx + 2.0xx + 4.0xx = 0\n2: -6.0xx + 4.0xx + 2.0xx = 0\n\njulia> res = poly_optimize(:Clarabel, prob, 5)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): 8.741532679530553e-8\nTime required for optimization: 0.1160416 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Here, it appears that the default solution extraction mechanism does not work well (in fact, since the algorithm is randomized, you'll get a vastly different result whenever the extraction is performced), so let's try to get the solution via the heuristic method:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> poly_all_solutions(:heuristic, res)\n4-element Vector{Tuple{Vector{Float64}, Float64}}:\n ([1.000000651012571, 1.0000006504036543], 7.809732969654704e-6)\n ([-1.000000651012571, 1.0000006504036543], 7.809732969654704e-6)\n ([1.000000651012571, -1.0000006504036543], 7.809732969654704e-6)\n ([-1.000000651012571, -1.0000006504036543], 7.809732969654704e-6)","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"This was successful in delivering multiple solutions.","category":"page"},{"location":"guide.html#Helping-convergence","page":"Walkthrough","title":"Helping convergence","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Another way to modify the problem is to exploit a prefactor in the objective. Mai et al. showed that by changing the objective from f(x) to theta^k(x) bigl(f(x) + epsilon theta^d(x)bigr), where theta(x) = 1 + Vert xVert^2 there is a bound on k that guarantees membership in the degree-(k + d) sums-of-squares cone. In contrast to other known bounds, this one is very easy to calculate, and it is not exponential (at least for unconstrained problems...). It holds even for noncompact feasible sets, in contrast to Putinar's result. The price to pay is that the objective itself is of course modified and therefore the optimal value of the problem is only in a neighborhood of the original problem.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"By using the noncompact=(, k) when constructing the problem using poly_problem, this is done automatically. Let us apply this to the Motzkin case:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> prob = poly_problem(x^4*y^2 + x^2*y^4 - 3x^2*y^2 +1, noncompact=(1e-5, 1))\nReal-valued polynomial optimization problem in 2 variables\nObjective: 1.00001 + 1.00004x + 1.00004x + 6.000000000000001e-5x - 2.99988xx + 6.000000000000001e-5x + 4.0e-5x - 1.9998799999999999xx - 1.9998799999999999xx + 4.0e-5x + 1.0e-5x + 1.00004xx + 2.00006xx + 1.00004xx + 1.0e-5x\nObjective was scaled by the prefactor 1.0 + x + x\n\njulia> poly_optimize(:Clarabel, prob)\n[ Info: Automatically selecting minimal degree cutoff 4\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): 0.0002699763854160192\nTime required for optimization: 0.0046539 seconds","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Indeed, now a basis of degree 4 was sufficient to find that the minimum value looks pretty nonnegative. However, this is hard to quantify, as for this, we'd have to extract a solution from the perturbed prob. The algorithm to do this is not implemented at the moment, as it would require the successive construction and solution of multiple polynomial optimization problems, which is not very efficient.","category":"page"},{"location":"guide.html#Complex-valued-problems","page":"Walkthrough","title":"Complex-valued problems","text":"","category":"section"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"PolynomialOptimization fully supports the complex-valued Lasserre hierarchy, including its sparse analysis. For this, simply use @complex_polyvar instead of @polyvar to declare your variables as complex. Note that feature of DynamicPolynomials requires at least version 0.6. Use conj at your discretion, but note that real and imag should not be used in the problem description! Instead, use (z + conj(z))/2 for the real and im*(conj(z) - z)/2 for the imaginary part, as well as z*conj(z) for the absolute value square.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"As soon as poly_problem detects complex variables, it switches to the complex-valued hierarchy.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> @complex_polyvar z;\n\njulia> prob = poly_problem(z + conj(z), zero=[z*conj(z)-1])\nComplex-valued polynomial optimization problem in 1 variable\nObjective: z + z\n1 equality constraint\n1: (-1.0 + 0.0im) + zz = 0\n\njulia> poly_optimize(:Clarabel, prob)\n[ Info: Automatically selecting minimal degree cutoff 1\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): -1.9999999928826857\nTime required for optimization: 1.1383336 seconds\n\njulia> poly_all_solutions(ans)\n1-element Vector{Tuple{Vector{ComplexF64}, Float64}}:\n ([-1.0000000000000042 + 0.0im], 7.117322731176046e-9)","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"The dense solution extraction mechanism also works in the complex case.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Let's try a more complicated example from the paper on the complex-valued Lasserre hierarchy (example 4.1):","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> @complex_polyvar z[1:2];\n\njulia> prob = poly_problem(3 - z[1]*conj(z[1]) - .5im*z[1]*conj(z[2])^2 + .5im*z[2]^2*conj(z[1]),\n                           zero=[z[1]*conj(z[1])-.25z[1]^2-.25conj(z[1])^2-1, # abs(z)^2 - z^2/4 - conj(z)^2/4 = 1\n                                 z[1]*conj(z[1])+z[2]*conj(z[2])-3, # abs(z)^2 + abs(z)^2 = 3\n                                 im*z[2]-im*conj(z[2])], # i z - i conj(z) = 0\n                           nonneg=[z[2]+conj(z[2])]); # z + conj(z)  0\n\njulia> poly_optimize(:Clarabel, prob, 3)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): 0.42817470663218404\nTime required for optimization: 1.2524744 seconds\n\njulia> poly_all_solutions(ans)\n1-element Vector{Tuple{Vector{ComplexF64}, Float64}}:\n ([2.2522776434581143e-16 - 0.8164965543178686im, 1.527525200200192 + 3.4269901696444247e-22im], 1.3954041122588023e-7)","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Indeed, this solution gives the same objective value and satisfies the constraints, so we found the optimum! Again, note that the longer optimization times are due to compilation times, as some of the methods for handling complex-valued variables needed to be compiled first.","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"And finally something with matrices:","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"julia> res = poly_optimize(:Clarabel, poly_problem(-z[1]*conj(z[1]) - z[2]*conj(z[2]),\n                                                   psd=[[1-2*(z[1]*z[2]+conj(z[1]*z[2]))  z[1]\n                                                         conj(z[1])  4-z[1]*conj(z[1])-z[2]*conj(z[2])]]), 3)\nPolynomial optimization result\nRelaxation method: Dense\nUsed optimization method: ClarabelMoment\nStatus of the solver: SOLVED\nLower bound to optimum (in case of good status): -3.9999999661436303\nTime required for optimization: 0.3807631 seconds\n\njulia> poly_all_solutions(res)\n5-element Vector{Tuple{Vector{ComplexF64}, Float64}}:\n ([0.26030598038884634 - 0.00026736336860256715im, 0.3407935395847438 + 0.00035003309799230153im], 3.816100332088393)\n ([-0.2603059803888315 + 0.0002673633686024347im, -0.34079353958474984 - 0.00035003309799188205im], 3.8161003320883964)\n ([-0.06036535235750472 + 6.200197140731072e-5im, -0.18480144151354266 - 0.00018981175865375325im], 3.962204377720153)\n ([0.06036535235750221 - 6.200197140723483e-5im, 0.1848014415135279 + 0.00018981175865374902im], 3.962204377720159)\n ([-4.503345903954037e-15 + 5.126149114067468e-17im, 8.070569843533215e-15 - 6.681107940528965e-18im], 3.9999999661436303)\n\njulia> poly_all_solutions(:heuristic, res)\n1-element Vector{Tuple{Vector{ComplexF64}, Float64}}:\n ([8.786670321838077e-19 - 0.0im, -6.275532682330681e-16 - 0.0im], 3.9999999661436303)\n\njulia> optimality_certificate(res)\n:Unknown","category":"page"},{"location":"guide.html","page":"Walkthrough","title":"Walkthrough","text":"Note that the solution extraction algorithm in principle also works in the complex case even though the moment matrix is no longer of Hankel form; the theory is powerful enough to handle this \"minor detail.\" The built-in heuristic will still try to find good solutions and can sometimes do so even in the case of multiple solutions if they only differ in the phase of variables. However, as in the real case, there is no guarantee that the solutions can be decomposed in atomic measures, and therefore the extraction or certification may also fail, which is shown here.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"CurrentModule = PolynomialOptimization","category":"page"},{"location":"includedsolvers.html#solvers_poly_optimize","page":"Supported solvers","title":"Supported solvers","text":"","category":"section"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"The following list contains all the solvers and the required packages that provide access to the solver. A solver of name X will always provide at least one of the two methods :XSOS or :XMoment. The former models the sum-of-squares formulation of the problem - each monomial corresponds to one constraint, and the solution comes from the dual vector. The latter models the Lasserre moment hierarchy of the problem - each monomial corresponds to a primal variable. Which method (or both) is offered by the solver depends on the particular interface that is supported by the solver and which form fits more naturally to this interface. Every solver will also provide an alias :X method that defaults to the recommended method for this solver. The given performance indicators are merely based on experience and may not be accurate for your particular problem. In particular the maximally recommended basis size depends heavily on the structure of the final problem, which can easily put the number up or down by 100 or more. All solvers may expose options that can influence the runtime behavior.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"Solver Package License Methods Speed Accuracy Memory max. recomm. basis size\nClarabel Clarabel.jl Apache moment    ~200\nCOPT COPT.jl commercial moment    ~700\nHypatia[1] Hypatia.jl MIT moment    ~100\nLANCELOT[2] GALAHAD.jl BSD nonlinear n.a. n.a.  n.a.\nLoraine [3] MIT primal moment    moderately large\nLoRADS LoRADS[4] MIT primal moment    very large\nMosek[5] Mosek.jl commercial SOS, moment    ~300 - 500\nProxSDP ProxSDP.jl MIT primal moment    very large\nSCS SCS.jl MIT moment    \nSketchyCGAL  MIT primal moment    \nSpecBM [6] MIT SOS n.a. n.a.  ","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"[1]: Note that by default, a sparse solver is used (unless the problem was constructed with a factor_coercive different from   one). This is typically a good idea for large systems with not too much monomials. However, if you have a very dense   system, the sparse solver will take forever; better pass dense=true to the optimization routine. This will then be much   faster (and always much more accurate).","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"[2]: LANCELOT is a nonlinear solver that directly works on the problem itself. It does not use a relaxation. Therefore, it   cannot provide lower-bound guarantees on the objective value; however, there is no problem with the extraction of a   solution, as the solver directly works on the decision variables. When invoking the LANCELOT solver, a function is   returned which performs the optimization and which requires a vector of initial values as parameter. This function will   then return a 2-tuple with the (locally) optimal objective value and the point of the local optimum.   Currently, the LANCELOT interface does not support complex-valued problems.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"[3]: There is a separate Julia package for Loraine. However, the implementation is so   bad that it is not only unnecessarily inefficient, but would also not allow to solve large-scale systems. Therefore,   PolynomialOptimization provides a rewritten implementation, which is heavily based on the original source code, but   tries to use the available memory more efficiently (though there is still room for improvement). Since only a subset of   the features of the original package has been implemented, this is not yet ready to be contributed to the solver; but it   can be expected that in the future, an external package will be required to use Loraine.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"[4]: There is no Julia package for LoRADS available. You first have to clone the linked Git repositiory and compile the solver   for your system; then, call Solvers.LoRADS.set_solverlib in order to tell PolynomialOptimization where to   look for the binary. After restarting Julia, the method is available.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"[5]: :MosekMoment requires at least version 10, :MosekSOS already works with version 9.   The moment variant is more prone to failure in case of close-to-illposed problems; sometimes, this is an issue of the   presolver, which can be turned off by passing MSK_IPAR_PRESOLVE_USE=\"MSK_PRESOLVE_MODE_OFF\" to poly_optimize.   The performance indicators in the table are valid for :MosekSOS. The new PSD cone interface of Mosek 10 that is used by   the moment-based variant proves to be much slower than the old one; therefore, using :MosekMoment is not recommended.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"[6]: SpecBM is provided by PolynomialOptimization; however, it requires a subsolver for the quadratic master problem.   Currently, Mosek and Hypatia are implemented and must therefore be loaded to make SpecBM work.","category":"page"},{"location":"includedsolvers.html#Packaged-solvers","page":"Supported solvers","title":"Packaged solvers","text":"","category":"section"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"During the development of this package, several interesting solvers were proposed in research. The ones that were implemented are documented on this page. They can be accessed from the PolynomialOptimization.Solvers submodule.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"CurrentModule = PolynomialOptimization.Solvers.Loraine","category":"page"},{"location":"includedsolvers.html#Loraine","page":"Supported solvers","title":"Loraine","text":"","category":"section"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"The Loraine solver is suiteable for large-scale low-rank semidefinite programming. Note that the implementation provided by PolynomialOptimization supports only a subset of Loraine's features (but this much more efficiently): the direct solver is not available, data matrices are assumed not to be rank-one and always sparse.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"Model\nPreconditioner\nSolver\nsolve!","category":"page"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.Loraine.Model","page":"Supported solvers","title":"PolynomialOptimization.Solvers.Loraine.Model","text":"Model(; A, C, b, [c_lin, A_lin,] coneDims, check=true)\n\nConstructs a model for PolynomialOptimization's rewrite of the Loraine solver. This solves the problem\n\n    min vec c_mathrmlin cdot vec x + sum_j langle C_j X_jrangle \n    textsuch that \n    x_i geq 0  forall i \n    X_j succeq 0 forall j \n    A_mathrmlin k cdot vec x + sum_j langle operatornamemat(A_k j) X_krangle = b_k  forall k\n\nwith the following representation in the variables:\n\n1  j  length(coneDims) = length(A) = length(C)\nA is a vector of SparseMatricCSCs, where every matrix corresponds to a semidefinite optimization variable. Each row in the matrix corresponds to one constraint; when the row is reshaped into a matrix in a col-major order (which must then be symmetric), it defines the coefficient matrix operatornamemat(A_k j). The side dimension of the matrix is stored in coneDims.\nIf present, c_lin is a SparseVector and A_lin a SparseMatrixCSC that define the objective and constraints coefficients for the nonnegative variables. They may be omitted only together.\n\nSee also Solver, solve!.\n\ninfo: Info\nThis function checks the validity of the variables; however, it is quite expensive to do the checks for symmetry. They can be disabled by setting check to false.\n\n\n\n\n\n","category":"type"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.Loraine.Preconditioner","page":"Supported solvers","title":"PolynomialOptimization.Solvers.Loraine.Preconditioner","text":"Preconditioner\n\nper CG iteration, PRECONDITIONER_NONE is faster (lower complexity) than PRECONDITIONER_HBETA which is faster than PRECONDITIONER_HALPHA\nas a preconditioner, PRECONDITIONER_HALPHA is better than PRECONDITIONER_HBETA is better than PRECONDITIONER_NONE, in the sense of CG iterations needed to solve the linear system\nsome SDP problems are \"easy\", meaning that CG always converges without preconditioner (i.e., preconditioner = PRECONDITIONER_NONE), so it's always worth trying this option\nPRECONDITIONER_HYBRID starts with (cheaper) H_beta and once it gets into difficulties, switches to H_alpha\n\n\n\n\n\n","category":"type"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.Loraine.Solver","page":"Supported solvers","title":"PolynomialOptimization.Solvers.Loraine.Solver","text":"Solver(model; tol_cg=0.01, tol_cg_up=0.5, tol_cg_min=1e-7, eDIMACS=1e-7,\n    preconditioner=PRECONDITIONER_HALPHA, erank=1, aamat=AMAT_DIAGAA, fig_ev=0,\n    verb=VERBOSITY_SHORT, initpoint=INITPOINT_LORAINE, maxit=100)\n\nDefines a solver for a previously defined model. Only the iterative conjugate gradient method is implemented.\n\nSee also Model, solve!.\n\n\n\n\n\n","category":"type"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.Loraine.solve!","page":"Supported solvers","title":"PolynomialOptimization.Solvers.Loraine.solve!","text":"solve!(solver)\n\nSolves a freshly initialized solver. Note that calling this function twice will produce the same results (unless parameters are changed), as the initial state is always restored at the beginning of the call.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"CurrentModule = PolynomialOptimization.Solvers.LoRADS","category":"page"},{"location":"includedsolvers.html#LoRADS","page":"Supported solvers","title":"LoRADS","text":"","category":"section"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"The experimental LoRADS solver is suitable for large-scale low-rank semidefinite programming. Note that it is currently in a very early stage of development and not yet very customizable. In particular, at the time of writing, it is not possible to configure the tolerance of the termination criterion, which is always set to 10^-5. There is also no interface defined to extract the solution data; PolynomialOptimization relies on the internal representation of the state to access this information. Note that this is likely to change in a new version; the supported solver version is a patched version of 1.0.0. You will also see a lot of solver output, regardless of the verbose flag, as this cannot be turned off. LoRADS has to be compiled from the source into a shared library; its path must then be provided to the package using set_solverlib. This setting will take effect after the Julia session is restarted.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"set_solverlib\nSolver\nConeType\ninit_solver\nset_dual_objective\nconedata_to_userdata\nset_cone\nset_lp_cone\ninit_cone_data\npreprocess\nload_sdpa\nsolve\nget_X\nget_Xlin\nget_S\nget_Slin","category":"page"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.set_solverlib","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.set_solverlib","text":"set_solverlib(path)\n\nChanges the path to the LoRADS library, which takes effect after restarting Julia.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.Solver","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.Solver","text":"Solver()\n\nCreates a new empty LoRADS solver object. This has to be initialized by called init_solver.\n\n\n\n\n\n","category":"type"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.ConeType","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.ConeType","text":"ConeType\n\nThe type of the current cone. Only CONETYPE_DENSE_SDP and CONETYPE_SPARSE_SDP are implemented.\n\n\n\n\n\n","category":"type"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.init_solver","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.init_solver","text":"init_solver(solver, nConstrRows, coneDims::AbstractVector{LoRADSInt}, nLpCols)\n\nInitializes a fresh Solver object with nConstrRows constraints, positive semidefinite variables of side dimension coneDims (a vector of integers), and nLpCols scalar nonnegative variables.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.set_dual_objective","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.set_dual_objective","text":"set_dual_objective(solver, dObj::AbstractVector{Cdouble})\n\nSets the dual objective, i.e., the right-hand side of the constraints, in an initialized Solver object.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.conedata_to_userdata","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.conedata_to_userdata","text":"conedata_to_userdata(cone::ConeType, nConstrRows, dim,\n    coneMatBeg::AbstractVector{LoRADSInt}, coneMatIdx::AbstractVector{LoRADSInt},\n    coneMatElem::AbstractVector{Cdouble})\n\nAllocates a new user data objects and sets its conic data. This consists of a cone type (only CONETYPE_DENSE_SDP and CONETYPE_SPARSE_SDP are supported), the number of rows (which is the same as the number of constraints in the solver) and the side dimension of the semidefinite variable, followed by the constraint matrices in zero-indexed CSR format. Every row corresponds to the vectorized lower triangle of the column of a constraint matrix. The zeroth row is the coefficient matrix for the objective. Therefore, nConstrRows +1 = length(coneMatBeg) -1 should hold (+1 for the objective; -1 for CSR).\n\nThe returned userdata pointer should be assigned to a solver, which will take care of freeing the allocated data. Note that the vectors passed to this function must be preserved until the preprocess function was called, after which they can be freed.\n\nSee also set_cone, init_cone_data.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.set_cone","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.set_cone","text":"set_cone(solver, iCone, userCone)\n\nSets the iConeth cone to the data previously defined using conedata_to_userdata.\n\nSee also init_cone_data.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.set_lp_cone","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.set_lp_cone","text":"set_lp_cone(solver, nConstrRows, nLpCols, lpMatBeg::AbstractVector{LoRADSInt},\n    lpMatIdx::AbstractVector{LoRADSInt}, lpMatElem::AbstractVector{Cdouble})\n\nSet the data of the constraint matrix for the linear variables according to the CSR data specified in the parameters.\n\nwarning: Warning\nThis function is not exported on the original code release and can therefore not be used. However, only the patched version should be used, as it fixes heap corruption errors that can arise during the optimization.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.init_cone_data","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.init_cone_data","text":"init_cone_data(solver, coneMat, coneDims, lpMat)\n\nInitializes the solver for a problem in the form\n\n   min vec a_0 cdot vec x + sum_j langleoperatornamemat(G_j 0) Z_jrangle \n   textsuch that \n   x_i geq 0  forall i \n   Z_j succeq 0  forall j \n   vec a_k cdot vec x - sum_j langleoperatornamemat(G_j k Z_j)rangle = c_k  forall k\n\nwith the following representation in the variables:\n\n1  j  length(coneDims) = length(coneMat)\nconeMat is a vector of matrices, lpMat is a matrix. They should be in CSR storage, where the row index (starting at 0 for the objective, then k for the kth constraint) is the constraint. Since CSR is not natively supported by Julia, the transpose of a SparseMatrixCSC{Cdouble,LoRADSInt} is expected.\nmat makes the unscaled lower triangle into a full matrix\n\nThis is a convenience function that does the job of conedata_to_userdata, set_cone, and preprocess in one step. However, note that it is more efficient to call these functions individually.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.preprocess","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.preprocess","text":"preprocess(solver, coneDims::AbstractVector{LoRADSInt})\n\nInvokes the preprocessor. This should be called after all cones were set up, after which their original data may be reused or destroyed.\n\nSee also conedata_to_userdata, set_cone.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.load_sdpa","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.load_sdpa","text":"load_sdpa(fn)\n\nLoads a problem from a file fn in SDPA format and returns a preprocessed Solver instance, a vector containing the cone dimensions, and the number of nonnegative scalar variables.\n\nwarning: Warning\nThis function will produce memory leaks. The data that is allocated by the LoRADS library cannot be freed by Julia, as no corresponding functions are exported. Only use it for quick tests, not in production.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.solve","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.solve","text":"solve(solver, params, coneDims)\n\nSolves a preprocessed Solver instance.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.get_X","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.get_X","text":"get_X(solver, i)\n\nReturns the ith PSD solution matrix X_i. The result will be a freshly allocated symmetric view of a dense matrix.\n\nwarning: Warning\nThis method may only be called once per i. All further calls with the same i will give wrong output, as the internal solver data is modified.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.get_Xlin","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.get_Xlin","text":"get_Xlin(solver)\n\nReturns the linear solution vector x. The result will be a vector backed by internal solver data and will be invalidated if the solver is destroyed. Copy it if desired.\n\nwarning: Warning\nThis method may only be called once. All further calls will give wrong output, as the internal solver data is modified.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.get_S","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.get_S","text":"get_S(solver, i)\n\nReturns the ith slack variable for the PSD solution matrix S_i. The result will be a freshly allocated symmetric view of a dense matrix.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LoRADS.get_Slin","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LoRADS.get_Slin","text":"get_Slin(solver[, i::AbstractUnitRange])\n\nReturns the slack variables to the nonnegative variables of index i (by default, all). The result will be a freshly allocated vector.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"CurrentModule = PolynomialOptimization.Solvers.SketchyCGAL","category":"page"},{"location":"includedsolvers.html#SketchyCGAL","page":"Supported solvers","title":"SketchyCGAL","text":"","category":"section"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"While the solver was implemented for the purpose of being used within PolynomialOptimization, it also works as a standalone routine (and could in principle be a separate package). SketchyCGAL is a solver that scales very well for large problem sizes by maintaining a sketch of the semidefinite matrices based on the assumption that the optimal solution has low rank; indeed, in polynomial optimizations, if there is an optimal point for the problem that can be encoded in the chosen basis, then this automatically gives rise to a rank-one semidefinite encoding of this point.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"sketchy_cgal\nStatus","category":"page"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.SketchyCGAL.sketchy_cgal","page":"Supported solvers","title":"PolynomialOptimization.Solvers.SketchyCGAL.sketchy_cgal","text":"sketchy_cgal(A, b, C; =(0, 1), rank, =1e-4, max_iter=0, time_limit=0, verbose=false,\n    =1, K=, method=:auto, callback=(_) -> ()[, A_norm][, A_normsquare])\n\nEnhanced implementation of the SketchyCGAL algorithm. This solves the following problem:\n\n    min bigl\n        sum_i operatornametr(C_i X_i) \n            sum_i operatornametr(A_j i X_i) = b_j  forall j\n            alpha_1 leq sum_i operatornametr(X_i) leq alpha_2\n            X_i succeq 0  forall i\n    bigr\n\nThe function returns the optimization status, objective value, and the optimal matrix X (in the form of an Eigen factorization object).\n\nParameters\n\nA is an AbstractMatrix whose entries are of type AbstractMatrix themselves. Alternatively, A can also be an AbstractVector of AbstractMatrices; in this case, A_j i is given by taking the matrix A[i] and reshaping its columns into a square matrix.\nb is an AbstractVector of real numbers\nC is an AbstractVector whose entries are of type AbstractMatrix themselves\n is a 2-tuples of nonnegative numbers, where the numbers defines the bounds on the sum of all traces\nrank controls the rank that is used to approximate the end result. It might either be an integer, which then puts the same rank constraint on all X_i, or a tuple/vector of integers, which allows to specify different rank constraints.\nThe solution accuracy can be controlled by the parameter ; however, no more than max_iter iterations are carried out, and no more iterations will be performed if time_limit was exceeded (in seconds), regardless of . Set any of those three parameters to zero to disable the check.\nThe callback may be called after each iteration and will receive a Status as parameter. If the callback returns false, the iteration will be the last one.\nThe parameters  and K allow to tune the optimization.  is a smoothing, K limits the dual vector to a generalized sphere of radius K around the origin.\nThe method determines the way in which the smallest eigenvalue and its eigenvector are calculated during each iteration. Possible values are (this might also be a vector/tuple, to specify the method for each X_i)\n:lanczos_space (uses the space-efficient implementation described in the SketchyCGAL paper, memory is linear in the problem size)\n:lanczos_time (uses the same principle, but can save about half of the operations by using more memory: quadratic in the problem size)\n:lobpcg_fast (uses the LOBPCG solver from the IterativeSolvers package, bounding the number of iterations with the same heuristic as for the Lanczos methods)\n:lobpcg_accurate (bounds the error instead to /100)\n:auto chooses :lobpcg_accurate for problem sizes smaller than 10, :lanczos_time for problem sizes less than 11500 (where roughly 1 GiB is required for the speedup), and :lanczos_space for all larger problems.\nA_normsquare (or A_norm) is supposed to hold the sum of the squares of the Frobenius-to--norms of all the linear operators contained in the columns of A. If both parameters are omitted, it is calculated automatically; however, this requires memory that scales quartically in the largest side dimension of the A (and may not be supported for all AbstractMatrix types). If both parameters are specified and their values are not consistent, the behavior is undefined.\n\nOptimization status values\n\n:optimal: the desired accuracy  was reached in both the relative suboptimality gap as well as the relative infeasibility\n:max_iter: the maximum number of iterations was reached\n:timeout: the maximum computation time was hit\n:canceled: the callback returned false\n:unknown: an internal error has happened\n\nSee also Status.\n\nsketchy_cgal(primitive1!, primitive2!, primitive3!, n, b, ; rank, primitive3_norm=0,\n    primitive3_normsquare=0, =1e-8, max_iter=10_000, verbose=false, rescale_C=1,\n    rescale_A=[1, ...], rescale_X=1, =1, K=, method=:auto, callback=(_) -> ())\n\nThis is the black-box version that allows for matrix-free operations. n is a tuple or vector that indicates the side dimensions of the semidefinite variables, b is the right-hand side of the constraint vector (of length d); and the primitives effectively calculate\n\nprimitive1!(v, u, i, , ) = (v =  * C[i] * u +  * v), u v in mathbb R^n_i\nprimitive2!(v, u, z, i, , ) = (v =  * adjoint(A[:, i])(z) * u +  * v), u v in mathbb R^n_i, z in mathbb R^d\nprimitive3!(v, u, i, , ) = (v =  * A[:, i](u * u') +  * v), u in mathbb R^n_i, v in mathbb R^d\n\nA[:, i](X) is the linear map [X, A[1, i], ..., X, A[d, i]] and adjoint(A[:, i])(z) = sum(z[j] * A[j, i]). All of them must also return their outputs (which is v).\n\nIf you are able to calculate these oracles faster or more memory-efficiently than the straightforward implementation (which is based on mul!), use the blackbox method. It is recommended to obey the following normalization conditions:\n\n    sum_i lVert C_irVert_mathrm F^2 = 1\n    quad\n    sum_i lVert primitive3_irVert_mathrm F to ell_2^2 = 1\n    quad\n    sum_i lVert A_1 irVert^2 = sum_i lVert A_2 irVert^2 = dotsb\n\nHowever, in any case, you need to specify the norm of primitive3! (i.e., the supremum of norm(primitive3!) applied to matrices with unit Frobenius norm) in the parameter primitive3_norm (or primitive3_normsquare, which is the sum of the individual normsquares). If the norm is unavailable, you need to at least give a lower bound. You may not specify both parameters inconsistently, else the behavior is undefined. If you had to rescale those matrices in order to achieve this normalization condition, you may pass the corresponding rescaling factors in rescale_C (implying that all C have been scaled by this one factor) and rescale_A (implying that a whole row of A has been scaled by one element from this vector). Additionally, the upper bound in  should be one for a better performance, which is achievable through rescale_X (implying that all X have been scaled by this factor). These are posterior factors that indicate the multiplication that has been done before calling the function in order to enforce compliance. They will be taken into account when calculating the termination criteria (such that  then corresponds to the original problem and not the rescaled one), filling the status structure or verbose output, and the final values of primal objective and optimal X.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.SketchyCGAL.Status","page":"Supported solvers","title":"PolynomialOptimization.Solvers.SketchyCGAL.Status","text":"Status{R}\n\nThis struct contains the current information for the Sketchy CGAL solver. Per-iteration callbacks will receive this structure to gather current information.\n\nSee also sketchy_cgal.\n\n\n\n\n\n","category":"type"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"CurrentModule = PolynomialOptimization.Solvers.SpecBM","category":"page"},{"location":"includedsolvers.html#SpecBM","page":"Supported solvers","title":"SpecBM","text":"","category":"section"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"While the solver was implemented for the purpose of being used within PolynomialOptimization, it also works as a standalone routine (and could in principle be a separate package). SpecBM is a spectral bundle algorithm for primal semidefinite programs and is based on the assumption that the optimal dual solution has low rank; indeed, in polynomial optimizations, if there is an optimal point for the problem that can be encoded in the chosen basis, then this automatically gives rise to a rank-one semidefinite moment matrix this point. The implementation also allows for free variables and multiple semidefinite constraints and contains further improvements compared to the reference implementation. It requires either Hypatia or a rather recent version of Mosek (at least 10.1.13) as subsolvers.","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"specbm_primal\nResult","category":"page"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.SpecBM.specbm_primal","page":"Supported solvers","title":"PolynomialOptimization.Solvers.SpecBM.specbm_primal","text":"specbm_primal(A, b, c; [num_frees,] psds, , r_past, r_current, =1e-4, =0.1,\n    maxiter=10000, maxnodescent=15, adaptive=false, =1., adaptive=true, min=1e-5,\n    max=1000., ml=0.001, mu=min(1.5, 1), Nmin=10, verbose=false, step=20, offset=0,\n    At=transpose(A), AAt=A*At, [subsolver,]\n    callback=(data, mastersolver_data)->nothing)\n\nSolves the minimization problem\n\n    min_x  c x  A x = b x = (x_mathrmfree operatornamesvec(X_1) dotsc) X_i  0\n              sum_i operatornametr(X_i)    + mathitoffset\n\nwhere the vector x contains num_frees free variables, followed by the vectorized and scaled lower triangles of PSD matrices X_i that have side dimensions given in psds. Scaled here means that the off-diagonal elements must be multiplied by sqrt2 when going from the matrix to its vectorization, so that scalar products are preserved. This corresponds to the :LS format of an SPMatrix from the StandardPacked package.\n\nArguments\n\nProblem formulation\n\nA::AbstractMatrix{R}: a sparse or dense matrix\nAt::AbstractMatrix{R}: the transpose of A. If omitted, transpose(A) is used instead. However, if the transpose is already known in explicit form (in particular, as another SparseMatrixCSC), some operations can be carried out faster.\nAAt::AbstractMatrix{R}: the product A*At, which is also calculated automatically, but can be given if it is already known.\nb::AbstractVector{R}: a dense or sparse vector\nc::AbstractVector{R}: a dense or sparse vector\noffset::Real: an offset that is added to the objective\nnum_frees: the number of free variables in the problem. The first num_frees entries in x will be free. If this value is omitted, it is automatically calculated based on the dimensions of A and psds.\npsds::AbstractVector{<:Integer}: a vector that, for each semidefinite matrix in the problem, specifies its side dimension. A side dimension of n will affect fracn(n +1)2 variables.\n::Real: an upper bound on the total trace in the problem. Note that by setting adaptive=true, this bound will effectively be removed by dynamically growing as necessary. In this case, the value specified here is the initial value.\nadaptive::Bool: effectively sets rho to infty; note that an initial  still has to be provided.\n\nSpectral bundle parameters\n\nr_past::Integer: the number of past eigenvectors to keep, must be nonnegative\nr_current::Integer: the number of current eigenvectors to keep, must be positive\n::Real: A step is recognized as a descent step if the decrease in the objective value is at least a factor beta in (0 1) smaller than the decrease predicted by the model.\n::Real: the regularization parameter for the augmented Lagrangian; must be positive\nadaptive::Bool=true: enables adaptive updating of  depending on the following five parameters, as described in Liao et al.\nmin::Real: lower bound for the adaptive algorithm that  may not exceed\nmax::Real: upper bound for the adaptive algorithm that  may not exceed\nml::Real:  is doubled if the decrease in the objective value is at least a factor m_mathrm l in (0 beta) larger than predicted by the model, provided no descent step was recognized for at least Nmin iterations.\nmu::Real:  is halved if the decrease in the objective value is at least a factor m_mathrm u  beta smaller than predicted by the model.\nNmin::Integer: minimum number of no-descent-steps before ml becomes relevant\nsubsolver::Symbol: subsolver to solve the quadratic semidefinite subproblem in every iteration of SpecBM. Currently, :Hypatia and :Mosek are supported; however, note that Mosek will require at least version 10.1.11 (better 10.1.13 to avoid some rare crashes).\n\nTermination criteria\n\n::Real: minimum quality of the result in order for the algorithm to terminate successfully (status :Optimal)\nmaxiter::Integer: maximum number of iterations before the algorithm terminates anyway (status :IterationLimit). Must be at least 2.\nmaxnodescent::Integer: maximum number of consecutive iterations that may report no descent step before the algorithm terminates (status :SlowProgress). Must be positive or zero to disable this check.\n\nLogging\n\nverbose::Bool: print the status every step iterations. Note that the first (incomplete) iteration will never be printed.\nstep::Integer: skip a number of iterations and only print every stepth.\n\nAdvanced solver interaction\n\ncallback::Function: a callback that is called with the last problem data (type Data) and the last mastersolver data (type MastersolverData) before the mastersolver is called anew. Changes to the structures may be made.\n\nSee also Result.\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.SpecBM.Result","page":"Supported solvers","title":"PolynomialOptimization.Solvers.SpecBM.Result","text":"Result\n\nContains the result of a SpecBM run\n\nFields\n\nstatus::Symbol: one of :Optimal, :IterationLimit, :SlowProgress\nobjective::R: the objective value\nx::Vector{R}: the optimal vector of primal variables: first, num_frees free variables, then all scaled vectorized lower triangles of the PSD variables\ny::Vector{R}: the optimal vector of dual variables, one for each constraint\niterations::Int: the number of iterations until the given status was reached\nquality::R: the optimality quantifier that is compared against  to determine convergence, which is determined by the maximum of the relative quantities below and the negative primal infeasibility.\nprimal_infeas::R\ndual_infeas::R\ngap::R\nrel_accuracy::R\nrel_primal_infeas::R\nrel_dual_infeas::R\nrel_gap\n\n\n\n\n\n","category":"type"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"CurrentModule = PolynomialOptimization.Solvers.LANCELOT","category":"page"},{"location":"includedsolvers.html#LANCELOT","page":"Supported solvers","title":"LANCELOT","text":"","category":"section"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"PolynomialOptimization provides an interface to LANCELOT. While LANCELOT is part of the GALAHAD library which has a quite recent Julia interface, the LANCELOT part is still pure Fortran without even a C interface. Therefore, here, we exploit that the pre-packaged binaries are compiled with GFortran, version at least 9, so that we know the binary layout of the parameters and can pretend that we like Fortran. Currently, only LANCELOT_simple is supported, which is of course not quite ideal[7]. Since Galahad.jl is a weak dependency, the package has to be loaded first before the Solvers.LANCELOT module becomes available:","category":"page"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"LANCELOT_simple","category":"page"},{"location":"includedsolvers.html#PolynomialOptimization.Solvers.LANCELOT.LANCELOT_simple","page":"Supported solvers","title":"PolynomialOptimization.Solvers.LANCELOT.LANCELOT_simple","text":"LANCELOT_simple(n, X, MY_FUN; MY_GRAD=missing, MY_HESS=missing, BL=nothing, BU=nothing,\n    neq=0, nin=0, CX=nothing, Y=nothing, maxit=1000, gradtol=1e-5, feastol=1e-5,\n    print_level=1)\n\nPurpose\n\nA simple and somewhat NAIVE interface to LANCELOT B for solving the nonlinear optimization problem\n\nmin_x f(x)\n\npossibly subject to constraints of the one or more of the forms\n\nbeginaligned\n   b_mathrm l  leq x leq b_mathrm u \n   c_mathrm e( x )  = 0 \n   c_mathrm i( x )  leq 0\nendaligned\n\nwhere fcolon mathbb R^n to mathbb R, c_mathrm e mathbb R^n to mathbb R^n_mathrmeq and c_mathrm icolon mathbb R^n to mathbb R^n_mathrmin are twice-continuously differentiable functions.\n\nWhy naive?\n\nAt variance with more elaborate interfaces for LANCELOT, the present one completely ignores underlying partial separability or sparsity structure, restricts the possible forms under which the problem may be presented to the solver, and drastically limits the range of available algorithmic options. If simpler to use than its more elaborate counterparts, it therefore provides a possibly substantially inferior numerical performance, especially for difficult/large problems, where structure exploitation and/or careful selection of algorithmic variants matter.\n\nwarning: Warning\nThe best performance obtainable with LANCELOT B is probably not with the present interface.\n\nHow to use it?\n\nUnconstrained problems\n\nThe user should provide, at the very minimum, suitable values for the following input arguments:\n\nn::Integer: the number of variables,\nX::AbstractVector{Float64} (strided vector of size n): the starting point for the minimization\nMY_FUN: a function for computing the objective function value for any X, whose interface has the default form MY_FUN(X::AbstractVector{Float64})::Float64 where X[1:n] contains the values of the variables on input, and which returns a double precision scalar representing the value f(X).\nIf the gradient of f can be computed, then the (optional) keyword argument MY_GRAD must be specified and given a function computing the gradient, whose interface must be of the form MY_GRAD(G::AbstractVector{Float64}, X::AbstractVector{Float64}), where G is a double precision vector of size n in which the function returns the value of the gradient of f at X.\nIf, additionally, the second-derivative matrix of f at X can be computed, the (optional) keyword argument MY_HESS must be specified and given a function computing the Hessian, whose interface must be of the form MY_HESS(H::SPMatrix{Float64}, X::AbstractVector{Float64}), where H is a double precision symmetrix matrix in packed storage format (upper triangular by column, see the StandardPacked.jl package) of the Hessian of f at X.\n\nIn all cases, the best value of x found by LANCELOT B is returned to the user in the vector X and the associated objective function value is the first return value. The second return value reports the number of iterations performed by LANCELOT before exiting. Finally, the last return value contains the exit status of the LANCELOT run, the value 0 indicating a successful run. Other values indicate errors in the input or unsuccessful runs, and are detailed in the specsheet of LANCELOT B (with the exception of the value 19, which reports a negative value for one or both input arguments nin and neq).\n\nExample\n\nLet us consider the optimization problem\n\nmin_x_1 x_2 f(x_1 x_2) = 100 ( x_2 - x_1^2 )^2 + ( 1 - x_1 )^2\n\nwhich is the ever-famous Rosenbrock \"banana\" problem. The most basic way to solve the problem (but NOT the most efficient) is, assuming the starting point X = [-1.2, 1.] known, to perform the call PolynomialOptimization.LANCELOT_simple(2, X, FUN) where the user-provided function FUN is given by\n\nFUN(X) = @inbounds 100 * (X[2] - X[1]^2)^2 + (1 - X[1])^2\n\nThe solution is returned in 60 iterations with exit code 0.\n\nIf we now wish to use first and second derivatives of the objective function, one should use the call\n\nPolynomialOptimization.LANCELOT_simple(2, X, FUN, MY_GRAD=GRAD!, MY_HESS=HESS!)\n\nand provide the additional routines\n\nGRAD!(G, X) = @inbounds begin\n    G[1] = -400 * (X[2] - X[1]^2) * X[1] - 2 * (1 - X[1])\n    G[2] = 200 * (X[2] - X[1]^2)\nend\n\nHESS!(H, X) = @inbounds begin\n    H[1, 1] = -400 * (X[2] - 3 * X[1]^2) + 2\n    H[1, 2] = -400 * X[1]\n    H[2, 2] = 200\nend\n\nConvergence is then obtained in 23 iterations. Note that using exact first-derivatives only is also possible: MY_HESS should then be absent from the calling sequence and providing the subroutine HESS! unnecessary.\n\nBound constrained problems\n\nBound on the problem variables may be imposed by specifying one or both of\n\nBL::AbstractVector{Float64} (double precision vector of size n): the lower bounds on X,\nBU::AbstractVector{Float64} (double precision vector of size n): the upper bounds on X.\n\nNote that infinite bounds (represented by a number larger than 1e20 in absolute value) are acceptable, as well as equal lower and upper bounds, which amounts to fixing the corresponding variables. Except for the specification of BL and/or BU, the interface is identical to that for unconstrained problems.\n\nExample\n\nIf one now wishes to impose zero upper bounds on the variables of our unconstrained problem, one could use the following call\n\nPolynomialOptimization.LANCELOT_simple(2, X, FUN, MY_GRAD=GRAD!, MY_HESS=HESS!,\n    BU=zeros(2))\n\nin which case convergence is obtained in 6 iterations.\n\nEquality constrained problems\n\nIf, additionally, general equality constraints are also present in the problem, this must be declared by specifying the following (optional) input argument:\n\nneq::Integer: the number of equality constraints.\n\nIn this case, the equality constraints are numbered from 1 to neq and the value of the i-th equality constraint must be computed by a user-supplied routine of the form FUN(X, i) (with i = 1, ..., neq) where the function now returns the value of the i-th equality constraint evaluated at X if i is specified. (This extension of the unconstrained case can be implemented by adding an optional argument i to the unconstrained version of FUN or by defining a three-parameter method on its own.) If derivatives are available, then the MY_GRAD and MY_HESS subroutines must be adapted as well: MY_GRAD(G, X, i) and MY_HESS(H, X, i) for computing the gradient and Hessian of the i-th constraint at X. Note that, if the gradient of the objective function is available, so must be the gradients of the equality constraints. The same level of derivative availability is assumed for all problem functions (objective and constraints). The final values of the constraints and the values of their associated Lagrange multipliers is optionally returned to the user in the (optional) double precision keyword arguments CX and Y, respectively (both being of size neq).\n\nInequality constrained problems\n\nIf inequality constraints are present in the problem, their inclusion is similar to that of equality constraints. One then needs to specify the (optional) input argument\n\nnin::Integer: the number of inequality constraints.\n\nThe inequality constraints are then numbered from neq+1 to neq+nin and their values or that of their derivatives is again computed by calling, for i = 1, ..., nin, FUN(X, i), MY_GRAD(G, X, i), MY_HESS(H, X, i). The inequality constraints are internally converted in equality ones by the addition of a slack variables, whose names are set to 'Slack_i', where the character i in this string takes the integers values 1 to nin. The values of the inequality constraints at the final X are finally returned (as for equalities) in the optional double precision keyword argument CX of size nin. The values of the Lagrange multipliers are returned in the optional double precision output argument Y of size nin.\n\nProblems with equality and inequality constraints\n\nIf they are both equalities and inequalities, neq and nin must be specified and the values and derivatives of the constraints are computed by FUN(X, i), GRAD(G, X, i), HESS(H, X, i) (i = 1, ..., neq) for the equality constraints, and FUN(X, i), GRAD(G, X, i), HESS(H, X, i) (i = neq+1, ..., neq+nin) for the inequality constraints. Again, the same level of derivative availability is assumed for all problem functions (objective and constraints). Finally, the optional arguments CX and/or Y, if used, are then of size neq+nin.\n\nExample\n\nIf we now wish the add to the unconstrained version the new constraints\n\nbeginaligned\n    0  leq x_1 \n    x_1 + 3x_2 - 3  = 0 \n    x_1^2 + x_2^2 - 4  leq 0\nendaligned\n\nwe may transform our call to\n\nCX = Vector{Float64}(undef, 2)\nY = Vector{Float64}(undef, 2)\nLANCELOT_simple(2, X, FUN; MY_GRAD=GRAD!, MY_HESS=HESS!, BL=[0., -1e20], neq=1, nin=1,\n    CX, Y)\n\n(assuming we need CX and Y), and add methods for FUN, GRAD! and HESS! as follows\n\nFUN(X, i) = @inbounds begin\n    if i == 1 # the equality constraint\n        return X[1] + 3X[2] - 3\n    elseif i == 2 # the inequality constraint\n        return X[1]^2 + X[2]^2 - 4\n    end\n    return NaN # should never happen\nend\n\nGRAD!(G, X, i) = @inbounds begin\n    if i == 1 # equality constraint's gradient components\n        G[1] = 1\n        G[2] = 3\n    elseif i == 2 # inequality constraint's gradient components\n        G[1] = 2X[1]\n        G[2] = 2X[2]\n    end\n    return\nend\n\nHESS!(H, X, i) = @inbounds begin\n    if i == 1 # equality constraint's Hessian\n        fill!(H, 1.)\n    elseif i == 2 # inequality constraint's Hessian\n        H[1] = 2\n        H[2] = 0\n        H[3] = 2\n    end\n    return\nend\n\nConvergence is then obtained in 8 iterations. Note that, in our example, the objective function or its derivatives is/are computed if the index i is omitted (see above). Of course, the above examples can easily be modified to represent new minimization problems :-).\n\nAvailable algorithmic options\n\nBeyond the choice of derivative level for the problem functions, the following arguments allow a (very limited) control of the algorithmic choices used in LANCELOT.\n\nmaxit::Integer: maximum number of iterations (default: 1000)\ngradtol::Real: the threshold on the infinity norm of the gradient (or of the lagrangian's gradient) for declaring convergence  (default: 1.0e-5)\nfeastol::Real: the threshold on the infinity norm of the constraint violation for declaring convergence (for constrained problems) (default: 1.0e-5)\nprint_level::Integer: a positive number proportional to the amount of output by the package: 0 corresponds to the silent mode, 1 to a single line of information per iteration (default), while higher values progressively produce more output.\n\nOther sources\n\nThe user is encouraged to consult the specsheet of the (non-naive) interface to LANCELOT within the GALAHAD software library for a better view of all possibilities offered by an intelligent use of the package. The library is described in the paper\n\nN. I. M. Gould, D. Orban, Ph. L. Toint,\nGALAHAD, a library of thread-sage Fortran 90 packages for large-scale\nnonlinear optimization,\nTransactions of the AMS on Mathematical Software, vol 29(4),\npp. 353-372, 2003\n\nThe book\n\nA. R. Conn, N. I. M. Gould, Ph. L. Toint,\nLANCELOT, A Fortan Package for Large-Scale Nonlinear Optimization\n(Release A),\nSpringer Verlag, Heidelberg, 1992\n\nis also a good source of additional information.\n\nMain author: Ph. Toint, November 2007. Copyright reserved, Gould/Orban/Toint, for GALAHAD productions\n\n\n\n\n\n","category":"function"},{"location":"includedsolvers.html","page":"Supported solvers","title":"Supported solvers","text":"[7]: The branch lancelot goes further and defines an interface for the full version of LANCELOT, which is a lot more   sophisticated. Unfortunately, it also seems to be broken at the moment and bugfixing will require some debugging of the   disassembly. This is not a priority at the moment (which is whenever you read this statement).","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"CurrentModule = PolynomialOptimization.IntPolynomials.MultivariateExponents","category":"page"},{"location":"intpolynomials.html#IntPolynomials","page":"IntPolynomials","title":"IntPolynomials","text":"","category":"section"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"PolynomialOptimization allows data in the form of any implementation that supports the MultivariatePolynomials interface. However, it does not keep the data in this way, which would not be particularly efficient. Instead, it is converted into an internal format, the IntPolynomial. It offers very compact storage (in some way even more compact than SIMDPolynomials) and is particularly focused on being as allocation-free as possible - which means that once the original polynomials were created, at no further stage in processing the polynomials or monomial bases will any allocations be done.","category":"page"},{"location":"intpolynomials.html#AbstractExponents","page":"IntPolynomials","title":"AbstractExponents","text":"","category":"section"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"For this, we first define a basic set of exponents which can occur in our monomial; this is a subtype of AbstractExponents. Such a set of exponents is always parameterized by the number of variables and by the integer data type that contains the index of the monomials in the set. By default, this is UInt, but any Integer descendant can be chosen. Note that with n variables (complex and conjugate variables counted separately) of maximum degree d, the datatype must be able to hold the value binomn + dn. In the typical scenario with not too high degrees, machine data types should be sufficient (say, d = 4, then even UInt32 can hold up to 564 variables, and with UInt64, more than 10^5 variables are possible). A monomial vector is then either a complete cover of a degree-bound exponent set or a finite subset of any exponent set. It needs no extra space apart from the description of the exponent set itself if it is a complete cover, and only the space required to describe the subindices (they need not necessarily be a vector, they can also be a range) if it is a subset.","category":"page"},{"location":"intpolynomials.html#Exponent-set-types","page":"IntPolynomials","title":"Exponent set types","text":"","category":"section"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"AbstractExponents\nAbstractExponentsUnbounded\nAbstractExponentsDegreeBounded\nExponentsAll\nExponentsDegree\nExponentsMultideg\nindextype","category":"page"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents","text":"AbstractExponents{N,I}\n\nAbstract supertype for all collections of multivariate exponents in N variables (N > 0). Every collection is iterable (both using a default lazy iteration and a mutable iteration into a vector by means of iterate! or veciter) and indexable (return a lazy collection of exponents) with index type I. The collection has a length if it is finite; it is never empty.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponentsUnbounded","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponentsUnbounded","text":"AbstractExponentsUnbounded{N,I} <: AbstractExponents{N,I}\n\nAbstract supertype for unbounded collections of multivariate exponents. These collections do not have a length; they are infinite. Their cache is always initialized incrementally as required.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponentsDegreeBounded","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponentsDegreeBounded","text":"AbstractExponentsDegreeBounded{N,I} <: AbstractExponents{N,I}\n\nAbstract supertype for finite collections of multivariate exponents, bounded by their degrees. These collections have a length; they also provide at least the fields mindeg and maxdeg that describe (a superset of) the covered degree range. Their cache is always initialized completely when required.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.ExponentsAll","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.ExponentsAll","text":"ExponentsAll{N,I}()\n\nRepresents an index range for a fixed number of variables N without any degree bound.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.ExponentsDegree","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.ExponentsDegree","text":"ExponentsDegree{N,I}(mindeg::Integer, maxdeg::Integer)\n\nRepresents an exponent range that is restricted by a bound on the total degree.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.ExponentsMultideg","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.ExponentsMultideg","text":"ExponentsMultideg{N,I}(mindeg::Integer, maxdeg::Integer,\n    minmultideg::AbstractVector, maxmultideg::AbstractVector)\n\nRepresents an exponent range that is restricted both by a global bound on the degree and by individual bounds on the variable degrees. Note that the vectors must not be used afterwards, and the constructor may clip maxmultideg to be no larger than maxdeg in each entry.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.indextype","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.indextype","text":"indextype(e)\n\nReturns the index type of an instance or subtype of AbstractExponents. This function is not exported.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#Working-with-exponents","page":"IntPolynomials","title":"Working with exponents","text":"","category":"section"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Exponent sets can be indexed and iterated. If consecutive elements are required, iteration is slightly faster, as indexing requires to determine the degree for every operation. However, in this case it is fastest to preallocate a vector that can hold the exponents and iterate! with mutation of this vector, or use the veciter wrapper for this task:","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"iterate!(::AbstractVector{Int}, ::AbstractExponents)\nveciter(::AbstractExponents, ::AbstractVector{Int})","category":"page"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.iterate!-Tuple{AbstractVector{Int64}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.iterate!","text":"iterate!(v::AbstractVector{Int}, e::AbstractExponents)\n\nIterates through a set of exponents by maintaining an explicit representation of all exponents. This is slightly more efficient that the lazy iteration version if every exponent has to be accessed explicitly. Note that v must be initialized with a valid exponent combination in e (this may be done via copyto!(v, first(e))). The function returns true if successful and false if the end was reached.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.veciter-Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents, AbstractVector{Int64}}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.veciter","text":"veciter(e::AbstractExponents[, v::AbstractVector{Int}])\n\nCreates an iterator over exponents that stores its result in a vector. This results in zero-allocations per iteration (as the iteration over e also does), but is more efficient if every element in e must be accessed. If the vector v is given as an argument, the data will be stored in this vector. If the vector is omitted, it will be created once at the beginning of the iteration process. The vector must never be altered, as it also serves as the state for the iterator; therefore, the same iterator may also not be nested.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"When working with individual indices or exponents, conversion functions are provided.","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"exponents_to_index\nexponents_from_index(::AbstractExponents{<:Any,I}, ::I) where {I<:Integer}\nexponents_sum\nexponents_product","category":"page"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_to_index","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_to_index","text":"exponents_to_index(::AbstractExponents{N,I}, exponents,\n    degree::Int=sum(exponents, init=0)[, report_lastexp::Int])\n\nCalculates the index of a monomial in N variables in an exponent set with exponents given by the iterable exponents (whose length should match N, else the behavior is undefined). The data type of the output is I. If exponents is not present in the exponent set, the result is zero(I). degree must always match the sum of all elements in the exponent set, but if it is already known, it can be passed to the function. No validity check is performed on degree.\n\ninfo: Truncated lengths\nIf the last argument report_lastexp is set to a value between 1 and N, the function will only consider the first report_lastexp exponents and return the largest index whose left exponents are compatible with those in exponents (whose length should still match N, unless degree is correctly specified manually). Again, if no such match can be found, the index is zero. If report_lastexp is set, the result will be a 2-tuple whose first entry is the index and whose second entry is the value of the last considered exponent, i.e. exponents[report_lastexp] if exponents is indexable.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_from_index-Union{Tuple{I}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{<:Any, I}, I}} where I<:Integer","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_from_index","text":"exponents_from_index(::AbstractExponents{N,I}, index::I[, degree::Int])\n\nCalculates the exponents that are associated with the monomial index index in N variables within a given exponent set. The return value will be a lazy implementation of AbstractVector{Int} (though iterating is more efficient than indexing). degree must match the degree of the index, if it is specified.\n\nSee also degree_from_index.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_sum","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_sum","text":"exponents_sum(e::AbstractExponents{N,I}, exponents...) -> Tuple{I,Int}\n\nCalculates the index of the sum of all exponents within e. If the result cannot be found in e, the function will return zero. Returns the total degree as second entry in the tuple.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_product","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_product","text":"exponents_product(e::AbstractExponents{N,I}, exponents, p::Integer) -> Tuple{I,Int}\n\nCalculates the index of the product of the exponents with the number p. If the result cannot be found in e, the function will return zero. Returns the total degree as second entry in the tuple.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Note that exponents_from_index returns a lazy implementation of an AbstractVector{Int}; if the same exponents must be accessed multple times, it might be beneficial to collect the result or copy it to a pre-allocated vector.","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Further information can be obtained about one or two indices or exponent sets:","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"degree_from_index(::AbstractExponents{N,I}, ::I) where {N,I<:Integer}\nconvert_index(::AbstractExponents{N,I}, ::AbstractExponents{N,IS}, ::IS, ::Int) where {N,I<:Integer,IS<:Integer}\ncompare_indices(::AbstractExponents{N,I1}, ::I1, ::_CompareOp, ::AbstractExponents{N,I2}, ::I2, ::Int) where {N,I1<:Integer,I2<:Integer}\nBase.:(==)(::AbstractExponents{N,I1}, ::AbstractExponents{N,I2}) where {N,I1<:Integer,I2<:Integer}\nisequal(::AbstractExponents{N}, ::AbstractExponents{N}) where {N}\nissubset(::AbstractExponents{N}, ::AbstractExponents{N}) where {N}","category":"page"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.degree_from_index-Union{Tuple{I}, Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I}, I}} where {N, I<:Integer}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.degree_from_index","text":"degree_from_index(::AbstractExponents{N,I}, index::I)\n\nReturns the degree that is associated with a given monomial index index in N variables. If the index was larger than the maximally allowed index in the exponent set, a degree larger than the maximal degree allowed in the iterator will be returned (not necessarily lastindex +1).\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.convert_index-Union{Tuple{IS}, Tuple{I}, Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, IS}, IS, Int64}} where {N, I<:Integer, IS<:Integer}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.convert_index","text":"convert_index(target::AbstractExponents{N}, source::AbstractExponents{N,I},\n    index::I[, degree::Int])\n\nConverts an index from an exponent set source to an exponent set target. index is always assumed to be a valid index for source, else the behavior is undefined.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.compare_indices-Union{Tuple{I2}, Tuple{I1}, Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I1}, I1, Union{typeof(!=), typeof(<), typeof(<=), typeof(==), typeof(>), typeof(>=)}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I2}, I2, Int64}} where {N, I1<:Integer, I2<:Integer}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.compare_indices","text":"compare_indices(e::AbstractExponents, index, op, e::AbstractExponents,\n    index[, degree::Int])\n\nCompares two indices from two possibly different exponent sets. degree, if given, must be the common degree of both exponents); when degree is omitted, also different degrees are possible. Both indices are assumed to be valid for their respective exponent sets, else the behavior is undefined. However, it is not necessary that an index is also valid in the other's exponent set. If e and e denote the conversion of the indices to a common exponent set (say, ExponentsAll), then the result is op(e, e). The only allowed values for op are ==, !=, <, <=, >, and >=.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#Base.:==-Union{Tuple{I2}, Tuple{I1}, Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I1}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I2}}} where {N, I1<:Integer, I2<:Integer}","page":"IntPolynomials","title":"Base.:==","text":"a == b\n\nCompares whether two exponents with the same variable number are equal. Exponents with different index types are not considered equal.\n\nSee isequal.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#Base.isequal-Union{Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N}}} where N","page":"IntPolynomials","title":"Base.isequal","text":"isequal(a, b)\n\nCompares whether two exponents with the same variable number are equal. Different index types are disregarded.\n\nSee ==.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#Base.issubset-Union{Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N}}} where N","page":"IntPolynomials","title":"Base.issubset","text":"issubset(a, b)\n\nChecks whether the exponent set a is fully contained in b. Different index types are disregarded.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Degree-bound exponent sets have a length:","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Base.length(::AbstractExponentsDegreeBounded)","category":"page"},{"location":"intpolynomials.html#Base.length-Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponentsDegreeBounded}","page":"IntPolynomials","title":"Base.length","text":"length(e::AbstractExponentsDegreeBounded)\n\nReturns the total number of exponents present in e.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#Internals","page":"IntPolynomials","title":"Internals","text":"","category":"section"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"The internal cache of exponents is extremely important for fast access. Unless the unsafe versions of the functions are used, it is always ensured that the cache is large enough to do the required operations; this is a quick check, but it can be elided using the unsafe versions. They, as well as the functions that allow direct access to the cache, must only be used if you know exactly what you are doing.","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"index_counts\nexponents_from_index(::Unsafe, ::AbstractExponents{<:Any,I}, ::I, ::Int) where {I<:Integer}\ndegree_from_index(::Unsafe, ::AbstractExponents{N,I}, ::I) where {N,I<:Integer}\nconvert_index(::Unsafe, ::AbstractExponents{N,I}, ::AbstractExponents{N,IS}, ::IS, ::Int) where {N,I<:Integer,IS<:Integer}\ncompare_indices(::Unsafe, ::AbstractExponents{N,I1}, ::I1, ::_CompareOp, ::AbstractExponents{N,I2}, ::I2, ::Int) where {N,I1<:Integer,I2<:Integer}\nBase.length(::Unsafe, ::AbstractExponentsDegreeBounded)","category":"page"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.index_counts","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.index_counts","text":"index_counts(unsafe, ::AbstractExponents{N,I})\n\nReturns the current Matrix{I} that holds the exponents counts for up to N variables in its N+1 columns (in descending order, ending with zero variables), and for the maximal degrees in the rows (in ascending order, starting with zero). The result is neither guaranteed to be defined at all nor have the required form if the cache was not calculated before.\n\n\n\n\n\nindex_counts(::AbstractExponents{N,I}, degree::Integer) -> Tuple{Matrix{I},Bool}\n\nSafe version of the above. If the boolean in the result is true, the matrix will have at least degree+1 rows, i.e., all entries up to degree are present. If it is false, the requested degree is not present in the exponents and the matrix will have fewer rows. Note that true does not mean that the degree is actually present in the exponents, only that its information has been calculated.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_from_index-Union{Tuple{I}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.Unsafe, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{<:Any, I}, I, Int64}} where I<:Integer","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.exponents_from_index","text":"exponents_from_index(unsafe, ::AbstractExponents{N,I}, index::I[, degree::Int])\n\nUnsafe variant of exponents_from_index: assumes that the cache for degree degree has already been populated, and that the exponent set contains index. If degree is omitted, it is calculated using the unsafe variant of degree_from_index.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.degree_from_index-Union{Tuple{I}, Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.Unsafe, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I}, I}} where {N, I<:Integer}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.degree_from_index","text":"degree_from_index(unsafe, ::AbstractExponents{N,I}, index::I)\n\nUnsafe variant of degree_from_index: assumes that the required cache for the degree that is associated with index has already been initialized, else the behavior is undefined (and in fact, non-deterministic, as it depends on the current size of the cache).\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.convert_index-Union{Tuple{IS}, Tuple{I}, Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.Unsafe, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, IS}, IS, Int64}} where {N, I<:Integer, IS<:Integer}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.convert_index","text":"convert_index(unsafe, target::AbstractExponents{N}, source::AbstractExponents{N,I},\n    index::I[, degree::Int])\n\nUnsafe variant of convert_index: assumes that caches for both the source and the target are set up as required for degree. If degree is omitted, it is calculated using the unsafe variant of degree_from_index.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.compare_indices-Union{Tuple{I2}, Tuple{I1}, Tuple{N}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.Unsafe, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I1}, I1, Union{typeof(!=), typeof(<), typeof(<=), typeof(==), typeof(>), typeof(>=)}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I2}, I2, Int64}} where {N, I1<:Integer, I2<:Integer}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.compare_indices","text":"compare_indices(unsafe, e::AbstractExponents, index, op, e::AbstractExponents,\n    index[, degree::Int])\n\nUnsafe variant of compare_indices: assumes that both caches are set up as required for degree (which, if given, must be the common degree of both exponents). If degree is omitted, it is calculated using the unsafe variant of degree_from_index.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#Base.length-Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.Unsafe, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponentsDegreeBounded}","page":"IntPolynomials","title":"Base.length","text":"length(unsafe, e::AbstractExponentsDegreeBounded)\n\nUnsafe variant of length: requires the cache to be set up correctly, else the behavior is undefined.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Note that the unsafe singleton is not exported on purpose.","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"CurrentModule = PolynomialOptimization","category":"page"},{"location":"intpolynomials.html#Limitations","page":"IntPolynomials","title":"Limitations","text":"","category":"section"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Monomials do not support a lot of operations once they are constructed (hence the \"simple\"); however, they of course allow to iterate through their exponents, convert the index between different types of exponent sets, can be conjugated (sometimes with zero cost) and can be multiplied with each other. Polynomials can be evaluated at a fully specified point.","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"This makes IntPolynomials very specialized for the particular needs of PolynomialOptimization; however, all the functionality is wrapped in its own subpackage and can be loaded independently of the main package. Don't do the conversion manually and then pass the converted polynomials to poly_problem - when the polynomial problem is initialized, depending on the keyword arguments, some operations still need to be carried out using the full interface of MultivariatePolynomials, not just the restricted subset that IntPolynomials provides.","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Handling the exponents without the MultivariatePolynomials machinery is also deferred to subpackage of IntPolynomials, MultivariateExponents. Note that these exponents and their indices always refer to the graded lexicographic order, which is the default in DynamicPolynomials.","category":"page"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"CurrentModule = PolynomialOptimization.IntPolynomials","category":"page"},{"location":"intpolynomials.html#The-MultivariatePolynomials-interface","page":"IntPolynomials","title":"The MultivariatePolynomials interface","text":"","category":"section"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"IntVariable\nIntRealVariable\nIntComplexVariable\nIntMonomial\nIntMonomial{Nr,Nc}(::AbstractExponents, ::AbstractVector{<:Integer}...) where {Nr,Nc}\nIntConjMonomial\nIntMonomial(::IntConjMonomial{Nr,Nc,<:Integer,<:AbstractExponents}) where {Nr,Nc}\nIntMonomialVector\nIntPolynomial\nchange_backend","category":"page"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntVariable","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntVariable","text":"IntVariable{Nr,Nc,I<:Unsigned} <: AbstractVariable\n\nIntVariable is the basic type for any simple variable in a polynomial right with Nr real and Nc complex-valued variables. The variable is identified by its index of type I alone. A variable can be explicitly cast to I in order to obtain its index.\n\nTo construct a real-valued or complex-valued variable, see IntRealVariable and IntComplexVariable.\n\nwarning: Warning\nNote that I has nothing to do with the index type used to identify monomials or exponents; in fact, I is automatically calculated as the smallest Unsigned descendant that can still hold the value Nr+2Nc.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntRealVariable","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntRealVariable","text":"IntRealVariable{Nr,Nc}(index::Integer)\n\nCreates a new real-valued simple variable whose identity is determined by index. Real-valued variables with the same index are considered identical; however, they are different from complex-valued variables constructed with the same index. A real variable will print as x, where the subscript is given by index. The variable is part of the polynomial ring with Nr real and Nc complex variables.\n\nwarning: Warning\nThis method is for construction of the variable only. Do not use it in type comparisons; variables constructed with this method will not be of type IntRealVariable (in fact, don't think of it as a type), but rather of type IntVariable!\n\nSee also IntVariable, IntComplexVariable.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntComplexVariable","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntComplexVariable","text":"IntComplexVariable{Nr,Nc}(index::Integer, isconj::Bool=false)\n\nCreates a new complex-valued simple variable whose identity is determined by index, and which is a conjugate variable if isconj is set appropriately. Complex-valued variables with the same index and isconj state are considered identical; however, they are different from real-valued variables constructed with the same index, even if they are not conjugate. A complex variable will print as z (if isconj=false) or z (if isconj=true), where the subscript is given by index. The variable is part of the polynomial ring with Nr real and Nc complex variables.\n\nwarning: Warning\nThis method is for construction of the variable only. Do not use it in type comparisons; variables constructed with this method will not be of type IntComplexVariable (in fact, don't think of it as a type), but rather of type IntVariable!\n\nSee also IntVariable, IntRealVariable.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntMonomial","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntMonomial","text":"IntMonomial{Nr,Nc,I<:Integer,E<:AbstractExponents} <: AbstractMonomial\n\nIntMonomial represents a monomial. In order to be used together in operations, the number of real-valued variables Nr and the number of complex-valued variables Nc are fixed in the type. The monomial is identified according to its index (of type I) in an exponent set of type E. This should be an unsigned type, but to allow for BigInt, no such restriction is imposed.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntMonomial-Union{Tuple{Nc}, Tuple{Nr}, Tuple{PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents, Vararg{AbstractVector{<:Integer}}}} where {Nr, Nc}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntMonomial","text":"IntMonomial{Nr,0[,I]}([e::AbstractExponents,]\n    exponents_real::AbstractVector{<:Integer})\nIntMonomial{0,Nc[,I]}([e::AbstractExponents,]\n    exponents_complex::AbstractVector{<:Integer},\n    exponents_conj::AbstractVector{<:Integer})\nIntMonomial{Nr,Nc[,I]}([e::AbstractExponents,]\n    exponents_real::AbstractVector{<:Integer},\n    exponents_complex::AbstractVector{<:Integer},\n    exponents_conj::AbstractVector{<:Integer})\n\nCreates a monomial within an exponent set e. If e is omitted, ExponentsAll{Nr+2Nc,UInt} is chosen by default. Alternatively, all three methods may also be called with the index type I as a third type parameter, omitting e, which then chooses ExponentsAll{Nr+2Nc,I} by default.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntConjMonomial","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntConjMonomial","text":"IntConjMonomial(m::IntMonomial) <: AbstractMonomial\n\nThis is a wrapper type for the conjugate of a simple monomial. A lot of operations allow to pass either IntConjMonomial or IntMonomial. Constructing the conjugate using this type works in zero time.\n\nSee also conj.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntMonomial-Union{Tuple{PolynomialOptimization.IntPolynomials.IntConjMonomial{Nr, Nc}}, Tuple{Nc}, Tuple{Nr}} where {Nr, Nc}","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntMonomial","text":"IntMonomial(c::IntConjMonomial)\n\nConverts a IntConjMonomial into a IntMonomial. This performs the calculation of the conjugate index.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntMonomialVector","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntMonomialVector","text":"IntMonomialVector{Nr,0[,I]}([e::AbstractExponents,]\n    exponents_real::AbstractMatrix{<:Integer}, along...)\nIntMonomialVector{0,Nc[,I]}([e::AbstractExponents,]\n    exponents_complex::AbstractMatrix{<:Integer},\n    exponents_conj::AbstractMatrix{<:Integer}, along...)\nIntMonomialVector{Nr,Nc[,I]}([e::AbstractExponents,]\n    exponents_real::AbstractMatrix{<:Integer},\n    exponents_complex::AbstractMatrix{<:Integer},\n    exponents_conj::AbstractMatrix{<:Integer}, along...)\n\nCreates a monomial vector, where each column corresponds to one monomial and each row contains its exponents. The internal representation will be made with respect to the exponent set e. If e is omitted, ExponentsAll{Nr+2Nc,UInt} is chosen by default. Alternatively, all three methods may also be called with the index type I as a third type parameter, omitting e, which then chooses ExponentsAll{Nr+2Nc,I} by default. All matrices must have the same number of columns; complex and conjugate matrices must have the same number of rows. The input will be sorted; if along are present, those vectors will be put in the same order as the inputs. The input must not contain duplicates.\n\n\n\n\n\nIntMonomialVector[{I}](mv::AbstractVector{<:AbstractMonomialLike}, along...;\n    vars=variables(mv))\n\nCreates a IntMonomialVector from a generic monomial vector that supports MultivariatePolynomials's interface. The monomials will internally be represented by the type I (UInt by default). The keyword argument vars must contain all real-valued and original complex-valued (so not the conjugates) variables that occur in the monomial vector. However, the order of this iterable (which must have a length) controls how the MP variables are mapped to IntVariables. The variables must be commutative; there is currently no way to check for this, so in the conversion process, commutativity is simply assumed. The input must not contain duplicates. It will be sorted; if along are present, those vectors will be put in the same order as the inputs.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.IntPolynomial","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.IntPolynomial","text":"IntPolynomial[{I}](p::AbstractPolynomialLike{C}, coefficient_type=C; kwargs...) where {C}\n\nCreates a new IntPolynomial based on any polynomial-like object that implements MultivariatePolynomials's AbstractPolynomialLike interface. The coefficients will be of type coefficient_type, the internal index type for the monomials will be I (UInt if omitted). Keyword arguments are passed on to IntMonomialVector, which allows to influence the variable mapping.\n\n\n\n\n\n","category":"type"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.change_backend","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.change_backend","text":"change_backend(mv::IntMonomialVector, variable::AbstractVector{<:AbstractVariable})\n\nChanges a IntMonomialVector into a different implementation of MultivariatePolynomials, where the variables are taken from the given vector in the order as they appear (but keeping real and complex variables distinct).\n\nThis conversion is not particularly efficient, as it works with generic implementations.\n\n\n\n\n\nchange_backend(p::IntPolynomial, variables::AbstractVector{<:AbstractVariable})\n\nChanges a IntPolynomial into a different implementation of MultivariatePolynomials, where the variables are taken from the given vector in the order as they appear (but keeping real and complex variables distinct). Note that the coefficients are returned without making a copy, which depending on the backend can imply back-effects on p itself should the output be changed. This conversion is not particularly efficient, as it works with generic implementations.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#Implementation-peculiarities","page":"IntPolynomials","title":"Implementation peculiarities","text":"","category":"section"},{"location":"intpolynomials.html","page":"IntPolynomials","title":"IntPolynomials","text":"Base.conj(::IntMonomialOrConj)\nvariable_index\nmonomial_product\nmonomial_index\neffective_nvariables\nMultivariatePolynomials.monomials(::Val{Nr}, ::Val{Nc}, ::AbstractUnitRange{<:Integer}) where {Nr,Nc}\nBase.intersect(::IntMonomialVector{Nr,Nc}, ::IntMonomialVector{Nr,Nc}) where {Nr,Nc}\nMultivariatePolynomials.merge_monomial_vectors(::Val{Nr}, ::Val{Nc}, ::AbstractExponents{N,I}, ::AbstractVector) where {Nr,Nc,N,I<:Integer}\nMultivariatePolynomials.merge_monomial_vectors(::AbstractVector{<:IntMonomialVector})\nveciter(::IntMonomialVector, ::AbstractVector{Int}, ::Val{indexed}) where {indexed}\nkeepat!!","category":"page"},{"location":"intpolynomials.html#Base.conj-Tuple{Union{PolynomialOptimization.IntPolynomials.IntConjMonomial{Nr, Nc, I, E}, PolynomialOptimization.IntPolynomials.IntMonomial{Nr, Nc, I, E}} where {Nr, Nc, I<:Integer, E<:PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents}}","page":"IntPolynomials","title":"Base.conj","text":"conj(m::Union{<:IntMonomial,<:IntConjMonomial})\n\nCreates the conjugate of a IntMonomial. The result type of this operation will always be IntMonomial. If the conjugate can be used to work with lazily, consider wrapping the monomial in a IntConjMonomial instead.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.variable_index","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.variable_index","text":"variable_index(v::IntVariable{Nr,Nc})\n\nReturns the index of the variable v, where real-valued variables have indices between 1 and Nr, and complex-valued variables have indices between Nr+1 and Nr+Nc. Conjugates have the same indices as their ordinary variables.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.monomial_product","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.monomial_product","text":"monomial_product(e::AbstractExponents, m...)\n\nCalculates the product of all monomials (or conjugates, or variables) m. The result must be part of the exponent set e. If the default multiplication * is used instead, e will always be ExponentsAll with the jointly promoted index type.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.monomial_index","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.monomial_index","text":"monomial_index([e::AbstractExponents,] m...)\n\nCalculates the index of the given monomial (or the product of all given monomials, or conjugates, or variables) m. The result must be part of the exponent set e. If e is omitted, it will be be ExponentsAll with the jointly promoted index type.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.effective_nvariables","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.effective_nvariables","text":"effective_nvariables(x::Union{<:IntMonomialVector{Nr,Nc},\n                              <:AbstractArray{<:IntMonomialVector{Nr,Nc}}}...)\n\nCalculates the number of effective variable of its arguments: there are at most Nr + 2Nc variables that may occur in any of the monomial vectors or arrays of monomial vectors in the arguments. This function calculates efficiently the number of variables that actually occur at least once anywhere in any argument.\n\n\n\n\n\n","category":"function"},{"location":"intpolynomials.html#MultivariatePolynomials.monomials-Union{Tuple{Nc}, Tuple{Nr}, Tuple{Val{Nr}, Val{Nc}, AbstractUnitRange{<:Integer}}} where {Nr, Nc}","page":"IntPolynomials","title":"MultivariatePolynomials.monomials","text":"monomials(Nr, Nc, degree::AbstractUnitRange{<:Integer};\n    minmultideg=nothing, maxmultideg=nothing, filter_exps=nothing,\n    filter_mons=nothing, I=UInt)\n\nReturns a IntMonomialVector with Nr real and Nc complex variables, total degrees contained in degree, ordered according to Graded{LexOrder} and individual variable degrees varying between minmultideg and maxmultideg (where real variables come first, then complex variables, then their conjugates).\n\nThe monomial vector will take possession of the min/maxmultidegs, do not modify them afterwards.\n\nAn additional filter may be employed to drop monomials during the construction. Note that the presence of a filter function will change to a less efficient internal representation. The filter function can get a vector with the exponents as its argument (filter_exps) or a filter function that gets the corresponding IntMonomial. The former is more efficient if every exponent has to be retrieved (but do not alter the argument).\n\nThe internal representation will be of the type I.\n\nThis function can be made type-stable by passing Nr and Nc as Vals.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#Base.intersect-Union{Tuple{Nc}, Tuple{Nr}, Tuple{PolynomialOptimization.IntPolynomials.IntMonomialVector{Nr, Nc, I, E, T} where {I<:Integer, E, T<:(PolynomialOptimization.IntPolynomials.IntMonomial{Nr, Nc, I})}, PolynomialOptimization.IntPolynomials.IntMonomialVector{Nr, Nc, I, E, T} where {I<:Integer, E, T<:(PolynomialOptimization.IntPolynomials.IntMonomial{Nr, Nc, I})}}} where {Nr, Nc}","page":"IntPolynomials","title":"Base.intersect","text":"intersect(a::IntMonomialVector{Nr,Nc}, b::IntMonomialVector{Nr,Nc})\n\nCalculates efficiently the intersection of two monomial vectors.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#MultivariatePolynomials.merge_monomial_vectors-Union{Tuple{I}, Tuple{N}, Tuple{Nc}, Tuple{Nr}, Tuple{Val{Nr}, Val{Nc}, PolynomialOptimization.IntPolynomials.MultivariateExponents.AbstractExponents{N, I}, AbstractVector}} where {Nr, Nc, N, I<:Integer}","page":"IntPolynomials","title":"MultivariatePolynomials.merge_monomial_vectors","text":"merge_monomial_vectors(::Val{Nr}, ::Val{Nc}, e::AbstractExponents, X::AbstractVector)\n\nReturns the vector of monomials contained X in increasing order and without any duplicates. The individual elements in X must be sorted iterables with a length and return IntMonomials compatible with the number of real Nr and complex variables Nc. The output will internally use the exponents e. The result type will be a IntMonomialVector with Nr and Nc as given, I and the exponents determined by e, and indexed internally with a Vector{I}.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#MultivariatePolynomials.merge_monomial_vectors-Tuple{AbstractVector{<:PolynomialOptimization.IntPolynomials.IntMonomialVector}}","page":"IntPolynomials","title":"MultivariatePolynomials.merge_monomial_vectors","text":"merge_monomial_vectors(X::AbstractVector)\n\nPotentially type-unstable variant that automatically determines the output format. If X has a defined eltype with known eltype <:IntMonomial{Nr,Nc,I,E}, Nr, Nc, and I are determined automatically in a type-stable manner. If not, they are taken from the eltype of the first iterable in X (which is not type stable). If this is not possible, either, they are taken from the first element in the first nonempty iterable in X.\n\nRegarding the automatic determination of E, the following rule is applied: it is assumed that if E is known in the element type, then every monomial will have the same instance of e as the exponents per iterable (this is always satisfied if the iterables are IntMonomialVectors). If there is one exponent that covers all others, this instance will be used. If not, the largest necessary exponents will be constructed; the result will be indexed unless all can be merged contiguously). Note that inhomogeneous iterables must implement last if the elements are based on ExponentsAll.\n\ninfo: Info\nThis method has a very general type signature and may therefore also be called for other implementations of MultivariatePolynomials. However, this case will be caught and then forwarded to the generic MP implementation.\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.veciter-Union{Tuple{indexed}, Tuple{PolynomialOptimization.IntPolynomials.IntMonomialVector, AbstractVector{Int64}, Val{indexed}}} where indexed","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.veciter","text":"veciter(mv::IntMonomialVector[, v::AbstractVector{Int}], indexed::Bool=false)\n\nCreates an iterator over all exponents present in mv (see veciter for AbstractExponents). By setting indexed to true, this iterator will instead give a tuple similar to enumerate, where the first index corresponds to the index of the monomial in the exponent set (so it does not necessarily start at 1 or have unit step). For type stability, indexed may instead be Val(false) or Val(true).\n\n\n\n\n\n","category":"method"},{"location":"intpolynomials.html#PolynomialOptimization.IntPolynomials.keepat!!","page":"IntPolynomials","title":"PolynomialOptimization.IntPolynomials.keepat!!","text":"keepat!!(x::IntMonomialVector, i::AbstractVector{Bool})\n\nKeeps the j monomial in x only if i[j] is true. This will mutate x if possible (i.e., if it was already indexed by a vector before), but it might also create a new vector if required (i.e., if a whole range of exponents was covered). Always use the return value, never rely on x. This function is not exported.\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"CurrentModule = PolynomialOptimization","category":"page"},{"location":"auxreference.html#Reference-of-auxilliaries","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"","category":"section"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"This reference page lists some additional functions and type that are required for the package to work properly. They are not part of the public API and are not exported; however, they may be useful even when separated from the purpose of polynomial optimization.","category":"page"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"@allocdiff\nkeepcol!\nissubset_sorted","category":"page"},{"location":"auxreference.html#PolynomialOptimization.@allocdiff","page":"Reference of auxilliaries","title":"PolynomialOptimization.@allocdiff","text":"@allocdiff\n\nA macro to evaluate an expression, discarding the resulting value, instead returning the difference in the number of bytes allocated after vs. before evaluation of the expression (which is not guaranteed to be the peak, but allows to capture allocations done in third-party libraries that don't use Julia's GC, contrary to @allocated). In order to provide consistent results, Julia's GC is disabled while the expression is evaluated. Note that on *nix systems, the value is only accurate up to the kibibyte.\n\n\n\n\n\n","category":"macro"},{"location":"auxreference.html#PolynomialOptimization.keepcol!","page":"Reference of auxilliaries","title":"PolynomialOptimization.keepcol!","text":"keepcol!(A::Union{<:Matrix,<:SparseMatrixCSC}, m::AbstractVector{Bool})\n\nOnly keep the columns in the matrix A that are true in the vector m (which must have the same length as size(A, 2)). The output is logically the same as A[:, m]; however, note that the data underlying A is mutated: this function does not create a copy. The function returns the new matrix; the old one should no longer be used (it might become invalid, as the sparse matrix struct is immutable).\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#PolynomialOptimization.issubset_sorted","page":"Reference of auxilliaries","title":"PolynomialOptimization.issubset_sorted","text":"issubset_sorted(a, b)\n\nEquivalent to issubset(a, b), but assumes that both a and b are sorted vectors.\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#Chordal-graphs","page":"Reference of auxilliaries","title":"Chordal graphs","text":"","category":"section"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"The functionality for chordal graphs is taken from the ChordalGraph.jl package (MIT license). Some features that we don't need were removed, inefficient implementations were improved.","category":"page"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"Relaxation.chordal_completion!\nRelaxation.chordal_cliques!","category":"page"},{"location":"auxreference.html#PolynomialOptimization.Relaxation.chordal_completion!","page":"Reference of auxilliaries","title":"PolynomialOptimization.Relaxation.chordal_completion!","text":"chordal_completion!(G::Graphs.SimpleGraph)\n\nAugment G by a chordal completion using a greedy minimal fill-in, and also return a perfect elimination ordering. This is a more efficient implementation of ChordalGraph.jl/GreedyOrder (for the \"MF\" case).\n\nSee also ChordalGraph.jl\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#PolynomialOptimization.Relaxation.chordal_cliques!","page":"Reference of auxilliaries","title":"PolynomialOptimization.Relaxation.chordal_cliques!","text":"chordal_cliques!(G::Graphs.SimpleGraph)\n\nMake the given graph chordal, and then calculate its maximal cliques. This is almost the same implementation as in ChordalGraph.jl/chordal_cliques!.\n\nSee also chordal_completion!, ChordalGraph.jl\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#Sorting-of-multiple-vectors","page":"Reference of auxilliaries","title":"Sorting of multiple vectors","text":"","category":"section"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"To simplify the task of sorting one vector and at the same time sorting multiple other vectors according to the same order - a common task that usually requires first computing a sortperm and then indexing all vectors with this permutation - PolynomialOptimization provides a helper function.","category":"page"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"sort_along!","category":"page"},{"location":"auxreference.html#PolynomialOptimization.sort_along!","page":"Reference of auxilliaries","title":"PolynomialOptimization.sort_along!","text":"sort_along!(v::AbstractVector, along::AbstractVector...; lo=1, hi=length(v),\n    o=Base.Order.Forward, relevant=1)\n\nThis helper function sorts the vector v as sort! would do, but at the same time puts all vectors in along in the same order as v. Therefore, sort_along!(v, a, b, c) is equivalent to\n\np = sortperm(v)\npermute!.((v, a, b, c), (p,))\n\nbut does not require any new intermediate allocations. Note that by increasing relevant (to at most 1 + length(along)), additional vectors in along can be taken into account when breaking ties in v.\n\ninfo: Implementation\nNote that the sorting functions are exactly identical to the ones used in Julia 1.8 (there was a big overhaul in 1.9 which made everything extremely complicated; we don't reproduce the new behavior). This means that either InsertionSort or QuickSort are used.\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#FastVector","page":"Reference of auxilliaries","title":"FastVector","text":"","category":"section"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"To improve the speed in some implementation details, PolynomialOptimization provides a \"fast\" vector type. This is basically just a wrapper around the stdlib Vector; however, it actually takes proper advantage of sizehints. The fact that Julia does this badly has been known for quite some time (#24909), but the default implementation has not changed (this will be different starting from Julia 1.11). Our own FastVec is a bit more specific than the PushVector, but also allows for more aggressive optimization. FastVec can directly be used in ccall with a pointer element type.","category":"page"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"CurrentModule = PolynomialOptimization.FastVector","category":"page"},{"location":"auxreference.html","page":"Reference of auxilliaries","title":"Reference of auxilliaries","text":"FastVec\nBase.sizehint!(::FastVec, ::Integer)\nBase.resize!(::FastVec, ::Integer)\nBase.empty!(::FastVec)\nprepare_push!\nBase.push!(::FastVec{V}, ::Any) where {V}\nunsafe_push!\nBase.insert!(::FastVec{V}, ::Integer, ::Any) where {V}\nunsafe_insert!\nBase.append!(::FastVec, ::Any)\nunsafe_append!\nBase.prepend!(::FastVec, ::Any)\nunsafe_prepend!\nBase.splice!(::FastVec, ::Integer, ::Any)\nBase.similar(::FastVec{V}) where {V}\nBase.copyto!(::FastVec, ::Integer, ::FastVec, ::Integer, ::Integer)\nBase.deleteat!(::FastVec, ::Integer)\nfinish!","category":"page"},{"location":"auxreference.html#PolynomialOptimization.FastVector.FastVec","page":"Reference of auxilliaries","title":"PolynomialOptimization.FastVector.FastVec","text":"FastVec{V}(undef, n::Integer; buffer::Integer=n)\n\nCreates a new FastVec, which is a vector of elements of type V. The elements are initially undefined; there are n items. The vector has a capacity of size buffer; while it can hold arbitrarily many, pushing to the vector is fast as long as the capacity is not exceeded.\n\n\n\n\n\n","category":"type"},{"location":"auxreference.html#Base.sizehint!-Tuple{PolynomialOptimization.FastVector.FastVec, Integer}","page":"Reference of auxilliaries","title":"Base.sizehint!","text":"sizehint!(v::FastVec, len::Integer)\n\nChanges the size of the internal buffer that is kept available to quickly manage pushing into the vector. If len is smaller than the actual length of this vector, this is a no-op.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#Base.resize!-Tuple{PolynomialOptimization.FastVector.FastVec, Integer}","page":"Reference of auxilliaries","title":"Base.resize!","text":"resize!(v::FastVec, n::Integer)\n\nEnsures that the internal buffer can hold at least n items (meaning that larger buffers will not be shrunk, but smaller ones will be increased to exactly n) and sets the length of the vector to n.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#Base.empty!-Tuple{PolynomialOptimization.FastVector.FastVec}","page":"Reference of auxilliaries","title":"Base.empty!","text":"empty!(v::FastVec)\n\nClears the vector without freeing the internal buffer.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#PolynomialOptimization.FastVector.prepare_push!","page":"Reference of auxilliaries","title":"PolynomialOptimization.FastVector.prepare_push!","text":"prepare_push!(v::FastVec, new_items::Integer)\n\nPrepares pushing (or appending) at last new_items in the future, in one or multiple calls. This ensures that the internal buffer is large enough to hold all the new items that will be pushed without allocations in between.\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#Base.push!-Union{Tuple{V}, Tuple{PolynomialOptimization.FastVector.FastVec{V}, Any}} where V","page":"Reference of auxilliaries","title":"Base.push!","text":"push!(v::FastVec, el)\n\nAdds el to the end of v, increasing the length of v by one. If there is not enough space, grows v. Note that if you made sure beforehand that the capacity of v is sufficient for the addition of this element, consider calling unsafe_push! instead, which avoids the length check.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#PolynomialOptimization.FastVector.unsafe_push!","page":"Reference of auxilliaries","title":"PolynomialOptimization.FastVector.unsafe_push!","text":"unsafe_push!(v::FastVec, el)\n\nAdds el to the end of v, increasing the length of v by one. This function assumes that the internal buffer of v holds enough space to add at least one element; if this is not the case, it will lead to memory corruption. Call push! instead if you cannot guarantee the necessary buffer size.\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#Base.insert!-Union{Tuple{V}, Tuple{PolynomialOptimization.FastVector.FastVec{V}, Integer, Any}} where V","page":"Reference of auxilliaries","title":"Base.insert!","text":"insert!(v::FastVec, index::Integer, el)\n\nInsert an el into v at the given index. index is the index of item in the resulting v. Note that if you made sure beforehand that the capacity of v is sufficient for the addition of this element, consider calling unsafe_insert! instead, which avoids the length check.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#PolynomialOptimization.FastVector.unsafe_insert!","page":"Reference of auxilliaries","title":"PolynomialOptimization.FastVector.unsafe_insert!","text":"unsafe_insert!(v::FastVec, index::Integer, el)\n\nInsert an el into v at the given index. index is the index of item in the resulting v. This function assumes that the internal buffer of v holds enough space to add at least one element; if this is not the case, it will lead to memory corruption. Call insert! instead if you cannot guarantee the necessary buffer size.\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#Base.append!-Tuple{PolynomialOptimization.FastVector.FastVec, Any}","page":"Reference of auxilliaries","title":"Base.append!","text":"append!(v::FastVec, els)\n\nAppends all items in els to the end of v, increasing the length of v by length(els). If there is not enough space, grows v. Note that if you made sure beforehand that the capacity of v is sufficient for the addition of these elements, consider calling unsafe_append! instead, which avoids the length check.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#PolynomialOptimization.FastVector.unsafe_append!","page":"Reference of auxilliaries","title":"PolynomialOptimization.FastVector.unsafe_append!","text":"unsafe_append!(v::FastVec, els)\n\nAppends all items in els to the end of v, increasing the length of v by length(els). This function assumes that the internal buffer of v holds enough space to add at least all elements in els; if this is not the case, it will lead to memory corruption. Call append! instead if you cannot guarantee the necessary buffer size.\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#Base.prepend!-Tuple{PolynomialOptimization.FastVector.FastVec, Any}","page":"Reference of auxilliaries","title":"Base.prepend!","text":"prepend!(v::FastVec, els)\n\nPrepends all items in els to the beginning of v, increasing the length v by length(els). If there is not enough space, grows v. Note that if you made sure beforehand that the capacity of v is sufficient for the addition of these elements, consider calling unsafe_prepend! instead, which avoids the length check.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#PolynomialOptimization.FastVector.unsafe_prepend!","page":"Reference of auxilliaries","title":"PolynomialOptimization.FastVector.unsafe_prepend!","text":"unsafe_prepend!(v::FastVec, els::AbstractVector)\n\nPrepends all items in els to the beginning of v, increasing the length v by length(els). This function assumes that the internal buffer of v holds enough space to add at least all elements in els; if this is not the case, it will lead to memory corruption. Call prepend! instead if you cannot guarantee the necessary buffer size.\n\n\n\n\n\n","category":"function"},{"location":"auxreference.html#Base.splice!-Tuple{PolynomialOptimization.FastVector.FastVec, Integer, Any}","page":"Reference of auxilliaries","title":"Base.splice!","text":"splice!(v::FastVec, index::Integer, [replacement]) -> item\n\nRemove the item at the given index, and return the removed item. Subsequent items are shifted left to fill the resulting gap. If specified, replacement values from an ordered collection will be spliced in place of the removed item. No unsafe version of this function exists. To insert replacement before an index n without removing any items, use splice!(collection, n:n-1, replacement).\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#Base.similar-Union{Tuple{PolynomialOptimization.FastVector.FastVec{V}}, Tuple{V}} where V","page":"Reference of auxilliaries","title":"Base.similar","text":"similar(v::FastVec)\n\nCreates a FastVec of the same type and length as v. All the common variants to supply different element types or lengths are also available; when changing the length, you might additionally specify the keyword argument buffer that also allows to change the internal buffer size.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#Base.copyto!-Tuple{PolynomialOptimization.FastVector.FastVec, Integer, PolynomialOptimization.FastVector.FastVec, Integer, Integer}","page":"Reference of auxilliaries","title":"Base.copyto!","text":"copyto!(dest::FastVec, doffs::Integer, src::FastVec, soffs::Integer, n::Integer)\ncopyto!(dest::Array, doffs::Integer, src::FastVec, soffs::Integer, n::Integer)\ncopyto!(dest::FastVec, doffs::Integer, src::Array, soffs::Integer, n::Integer)\n\nImplements the standard copyto! operation between FastVecs and also mixed with source or destination as an array.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#Base.deleteat!-Tuple{PolynomialOptimization.FastVector.FastVec, Integer}","page":"Reference of auxilliaries","title":"Base.deleteat!","text":"deleteat!(a::FastVec, i)\n\nRemove the item at the given i and return the modified a. Subsequent items are shifted to fill the resulting gap. The internal buffer size is not modified. The index must be either an integer or a unit range.\n\n\n\n\n\n","category":"method"},{"location":"auxreference.html#PolynomialOptimization.FastVector.finish!","page":"Reference of auxilliaries","title":"PolynomialOptimization.FastVector.finish!","text":"finish!(v::FastVec)\n\nReturns the Vector representation that internally underlies the FastVec instance. This function should only be called when no further operations on the FastVec itself are carried out.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"CurrentModule = PolynomialOptimization","category":"page"},{"location":"reference.html#Optimization-reference","page":"Reference","title":"Optimization reference","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"This reference page lists all functions that are relevant for polynomial optimization.","category":"page"},{"location":"reference.html#Problem-definition","page":"Reference","title":"Problem definition","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"Problem\npoly_problem(::P) where {P<:AbstractPolynomialLike}\nvariables\nnvariables\nisreal","category":"page"},{"location":"reference.html#PolynomialOptimization.Problem","page":"Reference","title":"PolynomialOptimization.Problem","text":"Problem\n\nThe basic structure that describes a polynomial optimization problem. In order to perform optimizations on this problem, construct AbstractRelaxations from it. Note that the variables in a Problem are rewritten to internal data types, i.e., they will probably not display in the same way as the original variables (they are simply numbered consecutively).\n\nThis type is not exported.\n\nSee also poly_problem, poly_optimize, AbstractRelaxation.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.poly_problem-Tuple{P} where P<:AbstractPolynomialLike","page":"Reference","title":"PolynomialOptimization.poly_problem","text":"poly_problem(objective; zero=[], nonneg=[], psd=[], perturbation=0.,\n    factor_coercive=1, perturbation_coefficient=0., perturbation_form=0,\n    noncompact=(0., 0), tighter=false, soscert=false, verbose=false,\n    monomial_index_type=UInt)\n\nAnalyze a polynomial optimization problem and return a Problem that can be used for sparse analysis and optimization.\n\nArguments\n\nProblem formulation\n\nobjective::AbstractPolynomial: the objective that is to be minimized\nzero::AbstractVector{<:AbstractPolynomialLike}: a vector with all polynomials that should be constrained to be zero.\nnonneg::AbstractVector{<:AbstractPolynomialLike}: a vector with all polynomials that should be constrained to be nonnegative. The values of the polynomials must always be effectively real-valued (in the sense that their imaginary parts evaluate to zero even if no values are plugged in).\npsd::AbstractVector{<:AbstractMatrix{<:AbstractPolynomialLike}}: a vector of matrices that should be constrainted to be positive semidefinite. The matrices must be symmetric/hermitian.\n\nProblem representation\n\nmonomial_index_type::Type{<:Integer}: internally, whatever interface of MultivariatePolynomials is used, the data is converted to the efficient IntPolynomial representation. Every monomial is represented by a single number of the type given for this keyword argument. The default is usually a good choice, allowing quite large problems. For very small problems, the index type might be reduced (however, note that the index must be large enough to capture the monomial for every desired relaxation, and there will be no warning on overflow!); if the problem is extremely large, it might also be enlarged to UInt128 or BigInt, the latter in particular with potentially severe performance and memory consumption issues.\n\nProblem modification\n\nFor unique solution extraction\n\nperturbation::Union{Float64, <:AbstractVector{Float64}}: adds a random linear perturbation with an absolute value not greater than this value to the objective for every variable (or, in the vector case, with different magnitudes for each variable). This will ensure that the result is unique and hence solution extraction will always work, at the cost of potentially reducing sparsity and slightly changing the actual result.\n\nFor noncompact sets\n\nThe following four parameters allow to automatically modify the problem according to a strategy by Mai, Lasserre, and Magron that was mainly developed for noncompact semialgebraic sets. It will modify the objective to\n\nmathrmfactor_coercive bigl(\n    mathrmobjective + mathrmperturbation_coefficient cdot mathrmperturbation_form\n  bigr)text\n\nUsually, perturbation_form and factor_coercive are both given by 1 + lVertmathrmvariablesrVert^2. If perturbation_coefficient is strictly positive, then for almost all degrees, the optimal value of the modified problem is then in biglf_mathrmopt f_mathrmopt + mathrmperturbation_coefficient cdot mathrmperturbation_form^d_mathrm o(x_mathrmopt)bigr. Often, this even works for a strictly zero coefficient (relatively generic conditions were found by Huang, Nie, and Yuan). Note that when modifying a problem in such a way, all sparsity methods provided by this package will be useless.\n\nfactor_coercive::AbstractPolynomial: Let k be divisible by 2r, then this must be the dehomogenization of a coercive positive form in n+1 variables of degree 2r to the power k(2r). Be aware that using this parameter will multiply the objective by another polynomial and therefore require a higher total relaxation degree to model the problem! Set this factor to zero (or use the parameter soscert) in order to change the polynomial optimization problem into one of certifying membership in the cone of SOS polynomials.\nperturbation_coefficient::Float64: a nonnegative prefactor that determines the strength of the perturbation and whose inverse dictates the scaling of a sufficient lower degree bound that guarantees optimality.\nperturbation_form::AbstractPolynomial: must be the dehomogenization of a positive form in n+1 variables of degree 2(1+degree(objective)2).\nnoncompact::Tuple{Real,Int}, now called (epsilon k): this is a shorthand that will set the previous three parameters to their standard values, factor_coercive=(1 + sum(variables.^2))^k, perturbation_coefficient to the value passed to this parameter, and perturbation_form=(1 + sum(variables.^2))^maxhalfdegree(objective). Be aware that using this parameter will multiply the objective by another polynomial and therefore require a higher total relaxation degree to model the problem!\n\nFor convergence at earlier levels\n\nNie provides a way to add additional constraints based on optimality conditions to the problems. This can speed up or make possible convergence at all. However, not every problem can be tightened in such a way, and sometimes, tightening might also increase the minimal degree required to optimize the problem. Note that the problem will end up with more equality and inequality constraints than originally entered. The augmentation does not change the solution of the original problem in case the minimum is attained at a critical point; if it is not, tightening will lead to missing this minimum.\n\ntighter::Union{Bool,Symbol}: if set to a valid solver or true (= choose default), tries to automatically construct constraints using Nie's method. Note that the algorithm internally needs to create lots of dense polynomials of appropriate degrees before solving for the coefficients. It is therefore possible that for larger problems, this can take a very long time. For a list of supported solvers, see the solver reference. This parameter can also be called tighten; if any of those two is true, it is assumed that this was the intended value.\n\nSOS membership\n\nUsually, a problem constructed with poly_problem will minimize the given objective under the constraints. Instead, membership of the objective in the quadratic module generated by the constraints can also be checked.\n\nsoscert::Bool: if set to true, disables the lower bound optimization. This is simply a shorthand for setting factor_coercive to zero.\n\nProgress monitoring\n\nverbose::Bool: if set to true, information about the current state of the method is printed; this may be useful for large and complicated problems whose construction can take some time.\n\nSee also Problem, poly_optimize, Relaxation.AbstractRelaxation.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#MultivariatePolynomials.variables","page":"Reference","title":"MultivariatePolynomials.variables","text":"variables(problem::Union{Problem,<:AbstractRelaxation})\n\nReturns the original variables (not their internal rewrites) associated to a given polynomial optimization problem. This defines the order in which solutions are returned. In the complex case, they do not contain conjugates.\n\nSee also poly_optimize, poly_solutions, poly_all_solutions.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#MultivariatePolynomials.nvariables","page":"Reference","title":"MultivariatePolynomials.nvariables","text":"nvariables(problem::Union{Problem,<:AbstractRelaxation})\n\nReturns the number of variables associated to a given polynomial optimization problem. In the complex case, conjugates are not counted.\n\nSee also variables.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#Base.isreal","page":"Reference","title":"Base.isreal","text":"isreal(problem::Union{Problem,<:AbstractRelaxation})\n\nReturns whether a given polynomial optimization problem contains only real-valued variables or also complex ones.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#Relaxations","page":"Reference","title":"Relaxations","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"Types and functions related to relaxations of polynomial optimization problems are found in the submodule Relaxation. The types in this module are mostly not exported, so that a qualified name is required.","category":"page"},{"location":"reference.html","page":"Reference","title":"Reference","text":"CurrentModule = PolynomialOptimization.Relaxation","category":"page"},{"location":"reference.html","page":"Reference","title":"Reference","text":"AbstractRelaxation\npoly_problem(::AbstractRelaxation)\nbasis\nMultivariatePolynomials.degree(::AbstractRelaxation)\ngroupings\niterate!(::AbstractRelaxation)\nCore.Type(::Problem, ::Tuple{Vararg{Any}})\nRelaxationGroupings","category":"page"},{"location":"reference.html#PolynomialOptimization.Relaxation.AbstractRelaxation","page":"Reference","title":"PolynomialOptimization.Relaxation.AbstractRelaxation","text":"AbstractRelaxation\n\nThis is the general abstract type for any kind of relaxation of a polynomial optimization problem. Its concrete types can be used for analyzing and optimizing the problem.\n\nSee also poly_problem, Problem, poly_optimize.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.poly_problem-Tuple{PolynomialOptimization.Relaxation.AbstractRelaxation}","page":"Reference","title":"PolynomialOptimization.poly_problem","text":"poly_problem(relaxation::AbstractRelaxation)\n\nReturns the original problem associated with a relaxation.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#PolynomialOptimization.Relaxation.basis","page":"Reference","title":"PolynomialOptimization.Relaxation.basis","text":"basis(relaxation::AbstractRelaxation[, clique::Int]) -> IntMonomialVector\n\nConstructs the basis that is associated with a given polynomial relaxation. If clique is given, only the monomials that are relevant for the given clique must be returned.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#MultivariatePolynomials.degree-Tuple{PolynomialOptimization.Relaxation.AbstractRelaxation}","page":"Reference","title":"MultivariatePolynomials.degree","text":"degree(problem::AbstractRelaxation)\n\nReturns the degree associated with the relaxation of a polynomial optimization problem.\n\nSee also poly_problem.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#PolynomialOptimization.Relaxation.groupings","page":"Reference","title":"PolynomialOptimization.Relaxation.groupings","text":"groupings(relaxation::AbstractRelaxation) -> RelaxationGroupings\n\nAnalyze the current state and return the bases and cliques as indicated by its relaxation in a RelaxationGroupings struct.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.iterate!-Tuple{PolynomialOptimization.Relaxation.AbstractRelaxation}","page":"Reference","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.iterate!","text":"iterate!(relaxation::AbstractRelaxation)\n\nSome sparse polynomial optimization relaxations allow to iterate their sparsity, which will lead to a more dense representation and might give better bounds at the expense of a more costly optimization. Return nothing if the iterations converged (state did not change any more), else return the new state. Note that state will be modified.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#Core.Type-Tuple{PolynomialOptimization.Problem, Tuple}","page":"Reference","title":"Core.Type","text":"Relaxation.XXX(problem::Problem[, degree]; kwargs...)\n\nThis is a convenience wrapper for Relaxation.XXX(Relaxation.Dense(problem, degree)) that works for any AbstractRelaxation XXX. degree is the degree of the Lasserre relaxation, which must be larger or equal to the halfdegree of all polynomials that are involved. If degree is omitted, the minimum required degree will be used. Specifying a degree larger than the minimal only makes sense if there are inequality or PSD constraints present, else it needlessly complicates calculations without any benefit.\n\nThe keyword arguments will be passed on to the constructor of XXX.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#PolynomialOptimization.Relaxation.RelaxationGroupings","page":"Reference","title":"PolynomialOptimization.Relaxation.RelaxationGroupings","text":"RelaxationGroupings\n\nContains information about how the elements in a certain (sparse) polynomial optimization problem combine. Groupings are contained in the fields obj, zero, nonneg, and psd:\n\nsum_i mathitobj_i^top sigma_i overlinemathitobj_i is the SOS representation of the objective with sigma_i succeq 0\nsum_i mathitzero_k i^top f_k is the prefactor for the k equality constraint with f_k a free vector\nsum_i mathitnonneg_k i^top sigma_k i overlinemathitnonneg_k i is the SOS representation of the prefactor of the k nonnegative constraint with sigma_k i succeq 0\nsum_i (mathitpsd_k i^top otimes mathbb1) Z_k i (overlinemathitpsd_k i otimes mathbb1) is the SOS matrix representation of the prefactor of the k PSD constraint with Z_k i succeq 0\n\nThe field var_cliques contains a list of sets of variables, each corresponding to a variable clique in the total problem. In the complex case, only the declared variables are returned, not their conjugates.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#Relaxations-based-on-a-global-basis","page":"Reference","title":"Relaxations based on a global basis","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"AbstractRelaxationBasis\nDense\nNewton\nCustom","category":"page"},{"location":"reference.html#PolynomialOptimization.Relaxation.AbstractRelaxationBasis","page":"Reference","title":"PolynomialOptimization.Relaxation.AbstractRelaxationBasis","text":"AbstractRelaxationBasis{Prob} <: AbstractRelaxation{Prob}\n\nAn AbstractRelaxationBasis is a relaxation of a polynomial optimization problem that is built using a single basis for everything (objective and constraints). The groupings for the individual elements will come from a degree truncation of the same shared basis for all constituents of the problem (intersected with a parent grouping).\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Relaxation.Dense","page":"Reference","title":"PolynomialOptimization.Relaxation.Dense","text":"Dense(problem::Problem[, degree])\n\nConstructs a full dense relaxation out of a polynomial optimization problem. This is the largest possible representation for a given degree bound, giving the best bounds. It is wasteful at the same time, as a Newton relaxation gives equally good bounds; but contrary to the Newton one, solution reconstruction works much better with a dense basis. degree is the degree of the Lasserre relaxation, which must be larger or equal to the halfdegree of all polynomials that are involved. If degree is omitted, the minimum required degree will be used. Specifying a degree larger than the minimal only makes sense if there are inequality or PSD constraints present, else it needlessly complicates calculations without any benefit.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Relaxation.Newton","page":"Reference","title":"PolynomialOptimization.Relaxation.Newton","text":"Newton(relaxation::AbstractRelaxation; [method,] parameters...)\n\nConstructs a relaxation based on the Newton halfpolytope applied to another relaxation of a polynomial optimization problem. This will be a superset of the largest possible representation for a given degree bound, with no negative consequences for finding the optimum. It can be much smaller than a dense basis, but solution reconstruction may be harder.\n\nWhen constraints are present, the polytope is determined based on the Putinar reformulation, where all constraints and the objective are moved to one side (comprising a new virtual objective). The prefactors for the constraints are determined by the previous relaxation method.\n\nNote that for the complex-valued hierarchy, strictly speaking there is no \"Newton polytope\"; as the representation of complex-valued polynomials is unique, the process is much simpler there; still, the size reduction is accomplished by using Newton.\n\nThe method determines which solver to use for determining the Newton polytope. If omitted, this will be the default solver (in the complex case, it must be :complex). The parameters are passed on to Newton.halfpolytope.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Relaxation.Custom","page":"Reference","title":"PolynomialOptimization.Relaxation.Custom","text":"Custom(problem::Problem, basis)\n\nConstructs a relaxation out of a polynomial optimization problem for the case in which a suitable basis is already known.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#Relaxations-based-on-individual-sparsity","page":"Reference","title":"Relaxations based on individual sparsity","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"AbstractRelaxationSparse\nSparsityCorrelative\nSparsityTerm\nSparsityTermBlock\nSparsityTermChordal\nSparsityCorrelativeTerm\nTermMode\nCliqueMerged\niterate!(::SparsityTerm)","category":"page"},{"location":"reference.html#PolynomialOptimization.Relaxation.AbstractRelaxationSparse","page":"Reference","title":"PolynomialOptimization.Relaxation.AbstractRelaxationSparse","text":"AbstractRelaxationSparse{Prob} <: AbstractRelaxation{Prob}\n\nAn AbstractRelaxationSparse is a relaxation of a polynomial optimization problem that applies sparsity methods to reduce the size of the associated problem, possibly at the expense of lowering the objective bound.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Relaxation.SparsityCorrelative","page":"Reference","title":"PolynomialOptimization.Relaxation.SparsityCorrelative","text":"SparsityCorrelative(relaxation::AbstractRelaxation; [high_order_zero,]\n    [high_order_nonneg,] [high_order_psd,] [low_order_zero,] [low_order_nonneg,]\n    [low_order_psd,] chordal_completion=true, verbose::Bool=false)\n\nAnalyze the correlative sparsity of a problem. Correlative sparsity is a variable-based sparsity analysis. It was first defined by Waki et al. in 2006 and extended by Josz and Molzahn in 2018. Variables are grouped into cliques based on the terms in the objective in which they appear together. Additional grouping is induced by variables that occur anywhere in a constraint of high order; or by variables that occur in a term in a constraint of low order. The parameters high_order_... allow to specify which constraints - identified by their indices - are of high order. If the parameter is omitted, all such constraints are of high order. Conversely, low_order_... can be used to specify that all but the listed constraints are of high order. Both parameters cannot be used simultaneously for the same set of constraints. Note that the order of the constraints is also influenced by the parent relaxation. If a correlative sparsity relaxation is applied to another relaxation that already limited the prefactor of a constraint to be of degree zero, it must necessarily be of low order.\n\nBy default, the correlative sparsity graph is completed to a chordal graph before the cliques are determined, which guarantees that the maximal cliques can be determined quickly; however, this may degrade the sparsity and it may be favorable not to carry out the completion.\n\nIf correlative and term sparsity are to be used together, use SparsityCorrelativeTerm instead of nesting the sparsity objects.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Relaxation.SparsityTerm","page":"Reference","title":"PolynomialOptimization.Relaxation.SparsityTerm","text":"SparsityTerm\n\nCommon base class that term sparsity methods use or wrap. The SparsityTermBlock and SparsityTermChordal constructors are shorthands that create SparsityTerm objects with the method parameter appropriately set. SparsityCorrelativeTerm is a very thin wrapper around SparsityTerm.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Relaxation.SparsityTermBlock","page":"Reference","title":"PolynomialOptimization.Relaxation.SparsityTermBlock","text":"SparsityTermBlock(relaxation::AbstractProblem; verbose::Bool=false)\n\nAnalyze the term sparsity of the problem. Term sparsity is a recent iterative sparsity analysis that groups terms with shared supports. Its last iteration will give the same optimal value as the original problem, although it may still be of a smaller size. Often, even the uniterated analysis already gives the same bound as the dense problem. The terms are grouped based on connected components of a graph; this can be improved by using the smallest chordal extension (see SparsityTermChordal), which will lead to even smaller problem sizes, but typically also worse bounds.\n\nIf correlative and term sparsity are to be used together, use SparsityCorrelativeTerm or nest the sparsity objects.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.Relaxation.SparsityTermChordal","page":"Reference","title":"PolynomialOptimization.Relaxation.SparsityTermChordal","text":"SparsityTermChordal(relaxation::AbstractProblem; chordal_completion=true, verbose=false)\n\nAnalyze the term sparsity of the problem using chordal cliques. Chordal term sparsity is a recent iterative sparsity analysis that groups terms with shared supports. Even in its last iteration, it may give strictly smaller values than the dense problem. The basis elements are grouped in terms of chordal cliques of the term sparsity graph. This uses maximal cliques; as obtaining maximal cliques of an arbitrary graph is not efficient, the graph is extended to a chordal graph if chordal_completion is true using a heuristic. Disabling the chordal completion can lead to smaller problem sizes.\n\nIf correlative and term sparsity are to be used together, use SparsityCorrelativeTerm or nest the sparsity objects.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.Relaxation.SparsityCorrelativeTerm","page":"Reference","title":"PolynomialOptimization.Relaxation.SparsityCorrelativeTerm","text":"SparsityCorrelativeTerm(relaxation::AbstractRelaxation; method=TERM_MODE_BLOCK, kwargs...)\n\nAnalyze both the correlative as well as the term sparsity of the problem. This is the most versatile kind of sparsity analysis, combining the effects of correlative sparsity with term analysis per clique. However, it is nothing more than first performing correlative sparsity analysis, followed by term sparsity analysis. This constructor will take all keyword arguments and distribute them appropriately to the SparsityCorrelative and SparsityTerm constructors. The returned object will be a very thin wrapper around SparsityTerm, with the only difference in printing; SparsityCorrelativeTerm objects by default print the clique grouping. Note that the same can be achieved for any relaxation (or groupings of a relaxation) if the IO parameter bycliques is set to true.\n\nSee also SparsityCorrelative, SparsityTermBlock, SparsityTermChordal, TermMode.\n\n\n\n\n\nSparsityCorrelativeTerm(relaxation::SparsityCorrelative; method=TERM_MODE_BLOCK, kwargs...)\n\nThis form allows to wrap an already created correlative sparsity pattern into a term sparsity pattern.\n\nSee also SparsityCorrelative, TermMode.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Relaxation.TermMode","page":"Reference","title":"PolynomialOptimization.Relaxation.TermMode","text":"@enum TermMode TERM_MODE_DENSE TERM_MODE_BLOCK TERM_MODE_CLIQUES\n    TERM_MODE_CHORDAL_CLIQUES TERM_MODE_NONE\n\nSpecifies which kind of completion procedure is used for the iteration of term sparsity pattern. Valid values are TERM_MODE_DENSE (Dense), TERM_MODE_BLOCK (SparsityTermBlock), TERM_MODE_CHORDAL_CLIQUES (SparsityTermChordal), and TERM_MODE_CLIQUES (SparsityTermChordal with chordal_completion = false). TERM_MODE_NONE can be used during iteration to disable the iteration of individual constraints.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Relaxation.CliqueMerged","page":"Reference","title":"PolynomialOptimization.Relaxation.CliqueMerged","text":"CliqueMerged(relaxation::AbstractRelaxation)\n\nPerforms clique merging on the parent relaxation. Clique merging may allow to reduce the solver time by merging together smaller blocks of variables with huge overlap into a single larger one; however, it comes at a significant cost itself. Basically, clique merging undoes some of the sparsity analysis performed before when it might be too excessive. This is is an equivalent reformulation that is as exact as relaxation itself.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.IntPolynomials.MultivariateExponents.iterate!-Tuple{PolynomialOptimization.Relaxation.SparsityTerm}","page":"Reference","title":"PolynomialOptimization.IntPolynomials.MultivariateExponents.iterate!","text":"iterate!(relaxation::Union{SparsityTerm,SparsityCorrelativeTerm}; [method,]\n    objective=true, zero=true, nonneg=true, psd=true, varclique_methods=missing)\n\nSparsityTerm implementations allow to customize the iteration procedure by the keyword arguments. The arguments objective, zero, nonneg, and psd can be boolean values (false means that these elements will not contribute to the iteration, true that they will). zero, nonneg, and psd can also be AbstractSets of integers, indicating that only the constraints with the indices specified in the set will contribute to the iteration. This all implies that the method used for their iteration will be given by method. Custom methods can be assigned if the parameters are set to a TermMode or a vector of TermModes.\n\nThe parameter method therefore determines the default that is assigned to the elements, and if not specified, it will be determined by the default method with which relaxation was constructed. Instead of a single TermMode, method may also be a vector successively assigning modes to the objective, the first zero constraints, ..., the nonnegative constraints, the psd constraints (eliminating the need for the other keywords).\n\nThe parameter varclique_methods instead allows to assign custom methods to individual variable cliques. Note that a variable clique can cover the objective and all constraints; in the case of conflicting assignments, the clique assignment takes precedence (but a clique mode may also be missing individually, in which case the default is taken).\n\n\n\n\n\n","category":"method"},{"location":"reference.html#Optimization-and-problem-solutions","page":"Reference","title":"Optimization and problem solutions","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"CurrentModule = PolynomialOptimization","category":"page"},{"location":"reference.html","page":"Reference","title":"Reference","text":"poly_optimize(::Val, ::AbstractRelaxation)\npoly_optimize(::Val, ::Problem, ::Vararg{Any})\npoly_optimize(::Result)\nSolver.RepresentationMethod\nRepresentationPSD\nRepresentationSDD\nRepresentationDD\nRepresentationIAs\nResult\nissuccess(::Result)\npoly_problem(::Result)\noptimality_certificate\npoly_all_solutions\npoly_solutions\npoly_solution_badness\nmoment_matrix\nMomentVector\nMomentAssociation\nSOSCertificate\nsos_matrix\nIterateRepresentation","category":"page"},{"location":"reference.html#PolynomialOptimization.Solver.poly_optimize-Tuple{Val, PolynomialOptimization.Relaxation.AbstractRelaxation}","page":"Reference","title":"PolynomialOptimization.Solver.poly_optimize","text":"poly_optimize([method, ]relaxation::AbstractRelaxation; verbose=false,\n    representation=RepresentationPSD(), [precision::Real], kwargs...)\n\nOptimize a relaxed polynomial optimization problem that was construced via poly_problem and then wrapped into an AbstractRelaxation. Returns a Result object.\n\nInstead of modeling the moment/SOS matrices as positive semidefinite, other representations such as the (scaled) diagonally dominant description are also possible. The representation parameter can be used to define a representation that is employed for the individual groupings. This may either be an instance of a RepresentationMethod - which requires the method to be independent of the dimension of the grouping - or a callable. In the latter case, it will be passed as a first parameter an identifier[1] of the current conic variable, and as a second parameter the side dimension of its matrix. The method must then return a RepresentationMethod instance.\n\nverbose=true will enable logging; this will print basic information about the relaxation itself as well as instruct the solver to output a detailed log. The PSD block sizes reported accurately represent the side dimensions of semidefinite variables and how many of these variables appear. The free block sizes are only very loose upper bounds on the maximal number of equality constraints that will be constructed by multiplying two elements from a block, as duplicates will be ignored. Any additional keyword argument is passed on to the solver.\n\nFor a list of supported methods, see the solver reference. If method is omitted, the default solver is used. Note that this depends on the loaded solver packages, and possibly also their loading order if no preferred solver has been loaded.\n\nThe keyword arguments are different for every solver; in general, if the solver permits access to its configuration options via a name, each option can be passed as a keyword argument. If the solver requires the use of integer constants (that are defined in some solver package), the options must be passed in the parameters keyword argument, which must be an iterable of pair-like elements. As the parameters for each solver are quite different, the optional precision argument will set some characteristic options related to the precision of the result (feasibility measures, duality gap) all to this value unless they are overwritten explicitly. However, be aware that each solver defines its own set of termination criteria, so the meaning of precision is not very strict.\n\nSee also RepresentationIAs.\n\n[1]: This identifier will be a tuple, where the first element is a symbol - either :objective, :nonneg, or :psd - to   indicate the general reason why the variable is there. The second element is an Int denoting the index of the   constraint (and will be undefined for the objective, but still present to avoid extra compilation). The last element   is an Int denoting the index of the grouping within the constraint/objective.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#PolynomialOptimization.Solver.poly_optimize-Tuple{Val, PolynomialOptimization.Problem, Vararg{Any}}","page":"Reference","title":"PolynomialOptimization.Solver.poly_optimize","text":"poly_optimize([method, ]problem::Problem[, degree::Int]; kwargs...)\n\nConstruct a Relaxation.Dense by default.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#PolynomialOptimization.Solver.poly_optimize-Tuple{PolynomialOptimization.Result}","page":"Reference","title":"PolynomialOptimization.Solver.poly_optimize","text":"poly_optimize(result::Result; [representation=IterateRepresentation(), ]kwargs...)\n\nRe-optimizes a previously optimized polynomial optimization problem. This is usually pointless, as the employed optimizers will find globally optimal solutions. However, this method allows to change the representation used for the constraints (or objective). If representation is a callable, it will now receive as a third parameter the type of the RepresentationMethod used before for this constraint[2], and as a fourth parameter the associated SOS matrix from the previous optimization. For efficiency reasons, this should only be used for changes that preserve the structure of the representation (i.e., whether it was PSD/DD/SDD and if its rotation was diagonal, triangular, or dense). If a structure non-preserving change is made, the problem needs to be constructed from scratch. For non-diagonal rotations, consider using RepresentationIAs in the first optimization.\n\nwarning: Warning\nThe internal state of the previous solver run will be re-used whenever possible. Therefore, no further data may be queried from the previous result afterwards, unless a re-optimization from scratch was necessary. While result will still be able to offer information about the relaxation, method, time, status, and objective value, moment matrices can only be accessed if they were already cached (i.e., accessed) before. Existing SOS certificates of the previous result will still be available, but new ones may not be constructed.\n\nSee also IterateRepresentation.\n\n[2]: Roughly, as the exact type is not known. For sure, it will be possible to distinguish between   RepresentationPSD, RepresentationDD, and RepresentationSDD. The matrix type will not be   concrete, but either Union{<:UniformScaling,<:Diagonal} if a diagonal representation was used before,   UpperOrUnitUpperTriangular, LowerOrUnitLowerTriangular, or Matrix else. The complex identification will be true   if a complex-valued cone was used and false else (where during specification, it could also have been true for   real-valued data, which would simply be ignored). In any case, the third parameter can be used as a constructor accepting   (unless it is for RepresentationPSD) the new rotation matrix as parameter. This is recommended, as in this way   the complex value cannot change back to true for real-valued data, which would be interpreted as a change in   structure, even if it is not.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#PolynomialOptimization.Solver.RepresentationMethod","page":"Reference","title":"PolynomialOptimization.Solver.RepresentationMethod","text":"RepresentationMethod{M,Complex}\n\nUnion type that defines how the optimizer constraint   0 is interpreted. Usually, \"\" means positive semidefinite; however, there are various other possibilities giving rise to weaker results, but scale more favorably. The following methods are supported:\n\nRepresentationPSD\nRepresentationSDD\nRepresentationDD\n\nSee also RepresentationIAs.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Solver.RepresentationPSD","page":"Reference","title":"PolynomialOptimization.Solver.RepresentationPSD","text":"RepresentationPSD <: RepresentationMethod\n\nModel the constraint \"  0\" as a positive semidefinite cone membership,   PSD. This is the strongest possible model, but the most resource-intensive.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Solver.RepresentationSDD","page":"Reference","title":"PolynomialOptimization.Solver.RepresentationSDD","text":"RepresentationSDD([u]; complex=true) <: RepresentationMethod\n\nModel the constraint \"  0\" as a membership in the scaled diagonally dominant cone, sigma = u^dagger Q u for some Q  SDD. The matrix u is by default an identity of any dimension; however, usually, care must be taken to have a matrix of suitable dimension. The membership Q  SDD is achieved using the scaled diagonally dominant (dual) cone directly or (rotated) quadratic cones.\n\nIf  is a Hermitian matrix, a complex-valued scaled diagonally dominant (dual) cone will be used, if supported. If not, fallbacks to the (rotated) quadratic cones are used; however, if complex=false and the ordinary scaled diagonally dominant (dual) cone is supported, rewrite the matrix as a real one and then use the real-valued cone. This is usually never advisable, as the rotated quadratic cone always works on the complex data. Note that if rewritten, u must be real-valued and have twice the side dimension of the complex-valued matrix.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Solver.RepresentationDD","page":"Reference","title":"PolynomialOptimization.Solver.RepresentationDD","text":"RepresentationDD([u]; complex=true) <: RepresentationMethod\n\nModel the constraint \"  0\" as a membership in the diagonally dominant cone, sigma = u^dagger Q u for some Q  DD. The matrix u is by default an identity of any dimension; however, usually, care must be taken to have a matrix of suitable dimension. The membership Q  DD is achieved using the diagonally dominant (dual) cone directly, ell_1- or ell_infty-norm cones or linear inequalities; slack variables will be added as necessary.\n\nIf  is a Hermitian matrix, a complex-valued diagonally dominant (dual) cone will be used, if supported. If not, fallbacks will first try quadratic cones on the complex-valued data, and if this is also not supported, rewrite the matrix as a real one and then apply the real-valued DD constraint. By setting the complex parameter to false, the rewriting to a real matrix will always be used, regardless of complex-valued solver support. Note that if rewritten, u must be real-valued and have twice the side dimension of the complex-valued matrix.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Solver.RepresentationIAs","page":"Reference","title":"PolynomialOptimization.Solver.RepresentationIAs","text":"RepresentationIAs(r::Type{RepresentationDD,RepresentationSDD},\n    m::Type{<:AbstractMatrix}=UpperTriangular; complex=true)\n\nDefault callable that will instantiate a correctly-sized representation of type r with an identity rotation that is, however, not recognized as a diagonal rotation but as type m instead. Use this type in the first call to poly_optimize if you want to re-optimize the problem afterwards with rotations of type m. The default for m, UpperTriangular, is suitable for the automatic Cholesky-based reoptimization.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.Result","page":"Reference","title":"PolynomialOptimization.Result","text":"Result\n\nResult of a polynomial optimization, returned by calling poly_optimize on an AbstractRelaxation. A Result struct r contains information about\n\nthe relaxation employed for the optimization (r.relaxation)\nthe optimized problem (available via poly_problem)\nthe used method (r.method)\nthe time required for the optimization in seconds (r.time)\nthe status of the solver (r.status), which also depends on the solver type. Use issuccess to check whether this is a successful status.\nthe returned primal value of the solver (r.objective), which, if the status was successful, is a lower bound to the true minimum\nthe moment information in vector form (r.moments), which allows to construct a moment matrix, extract solutions (poly_all_solutions or poly_solutions), and an optimality certificate.\n\nThis type is not exported.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#LinearAlgebra.issuccess-Tuple{PolynomialOptimization.Result}","page":"Reference","title":"LinearAlgebra.issuccess","text":"issuccess(r::Result)\n\nReturns true if the solver successfully solved the relaxation and provided a solution, and false otherwise.\n\ninfo: Info\nSolvers often do not have just a single \"good\" status code, but also \"near successes\". Whether they will return true or false is dependent on the implementation. The status field of the result is always available to get the original return value.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#PolynomialOptimization.poly_problem-Tuple{PolynomialOptimization.Result}","page":"Reference","title":"PolynomialOptimization.poly_problem","text":"poly_problem(r::Result)\n\nReturns the problem that was associated with the optimization result.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#PolynomialOptimization.optimality_certificate","page":"Reference","title":"PolynomialOptimization.optimality_certificate","text":"optimality_certificate(result::Result, =1e-6)\n\nThis function applies the flat extension/truncation criterion to determine whether the optimality of the given problem can be certified, in which case it returns :Optimal. If no such certificate is found, the function returns :Unknown. The criterion is meaningless for sparse problems or if a full basis is not available. The parameter  controls the bound below which singular values are considered to be zero, and its negative below which eigenvalues are considered to be negative.\n\nSee also poly_optimize.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.poly_all_solutions","page":"Reference","title":"PolynomialOptimization.poly_all_solutions","text":"poly_all_solutions([method, ]result::Result, args...; verbose=false, rel_threshold=100,\n    abs_threshold=Inf, kwargs...)\n\nObtains a vector of all the solutions to a previously optimized problem; then iterates over all of them and grades and sorts them by their badness. Every solution of the returned vector is a tuple that first contains the optimal point and second the badness at this point. Solutions that are rel_threshold times worse than the best solution or worse than abs_threshold will be dropped from the result.\n\nSee also poly_optimize, poly_solutions.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.poly_solutions","page":"Reference","title":"PolynomialOptimization.poly_solutions","text":"poly_solutions([method, ]result::Result, args...; verbose, kwargs...)\n\nExtracts solutions from a polynomial optimization result using the method method. Depending on the chosen method, the result may be an iterator or a vector. Consult the documentation of the methods for further information. If method is omitted, a default method will be chosen according to the relaxation that was used for the optimization.\n\nSee also poly_optimize, poly_all_solutions.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.poly_solution_badness","page":"Reference","title":"PolynomialOptimization.poly_solution_badness","text":"poly_solution_badness(result::Result, solution)\n\nDetermines the badness of a solution by comparing the value of the objective with the value according to the optimization given in result, and also by checking the violation of the constraints. The closer the return value is to zero, the better. If the return value is too large, solution probably has nothing to do with the actual solution.\n\nSee also poly_optimize, poly_solutions, poly_all_solutions.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.moment_matrix","page":"Reference","title":"PolynomialOptimization.moment_matrix","text":"moment_matrix(problem::Result; max_deg=Inf, prefix=1)\n\nAfter a problem has been optimized, this function assembles the associated moment matrix (possibly by imposing a degree bound max_deg, and possibly multiplying each monomial by the monomial or variable prefix, which does not add to max_deg). Note that prefix has to be a valid IntMonomial or IntVariable of appropriate type.\n\nSee also poly_optimize, poly_optimize.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.MomentVector","page":"Reference","title":"PolynomialOptimization.MomentVector","text":"MomentVector(relaxation::AbstractRelaxation, values::AbstractVector{R} where {R<:Real})\n\nMomentVector is a representation of the result of a polynomial optimization. It contains all the values of the moments that were present in the optimization problem. This vector can be indexed in two ways:\n\nlinearly, which will just transparently yield what linearly indexing values would yield\nwith a monomial (or multiple monomials, which means that the product of all the monomials is to be considered), which will yield the value that is associated with this monomial; if the problem was complex-valued, this will be a Complex{R}.\n\nIn order to get an association-like iterator, use MomentAssociation.\n\nThis type is not exported.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.MomentAssociation","page":"Reference","title":"PolynomialOptimization.MomentAssociation","text":"MomentAssociation(m::MomentVector)\n\nCreates a associative iterator over the moment vector m that, upon iteration, returns Pairs assigning values to monomials.\n\nThis type is not exported.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.SOSCertificate","page":"Reference","title":"PolynomialOptimization.SOSCertificate","text":"SOSCertificate(result::Result)\n\nConstruct a SOS certificate from a given optimization result. The returned object will pretty-print to show the decomposition of the optimization problem in terms of a positivstellensatz. To obtain the polynomials for the individual terms, the object can be indexed. The first index is one of :objective, :zero, :nonneg, :psd; the second index is the number of the desired element (omitted for :objective); the last index is the index of the desired grouping due to sparsity. If the last index is omitted, a vector over all groupings is returned.\n\nThe returned vectors of polynomials are, for :objective and :nonneg, to be summed over their squares; for :psd, the returned matrix m of polynomials is to be left-multiplied with its adjoint: m' * m. For :zero, a single polynomial is returned. If all these operations are carried out while multiplying with the corresponding prefactors (i.e., the constraints themselves), the resulting polynomial should be equal to the original objective. Note that this need not be the case; only if the relaxation level was sufficient will a SOS certificate in fact be valid.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#PolynomialOptimization.sos_matrix","page":"Reference","title":"PolynomialOptimization.sos_matrix","text":"sos_matrix(relaxation::AbstractRelaxation, state[, constraint])\n\nExtracts the SOS matrices associated with a solved relaxation. A constraint is identified in the same way as the first argument that is passed to a RepresentationMethod (see [1]). Short forms are allowed when there is just a single grouping. Usually, a SOSCertificate is more desirable than the construction of individual SOS matrices.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.IterateRepresentation","page":"Reference","title":"PolynomialOptimization.IterateRepresentation","text":"IterateRepresentation(; keep_structure=false)\n\nDefault iteration method for DD and SDD representations. This is will perform a Cholesky decomposition of the old SOS matrix and use it as the new rotation, ensuring that results never get worse (at least in theory; since a positive definite SOS matrix is only guaranteed up to a certain tolerance, bad things could still happen).\n\nNote that the resulting rotation matrix will be upper triangular, which may break a previous structure. By setting keep_structure to true, the structure will be preserved (if it was diagonal, this would mean keeping only the diagonal of the Cholesky factor, with no theoretical guarantees, not even about convergence; if it was lower triangular the adjoint will be taken, which will not give any convergence guarantees, as the rotated DD/SDD cone is implemented with respect to the upper triangular factorization).\n\nSee also poly_optimize\n\n\n\n\n\n","category":"type"},{"location":"reference.html#Newton-polytope-construction-(manually)","page":"Reference","title":"Newton polytope construction (manually)","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"Note that using these functions is usually not necessary; construct a Newton relaxation instead.","category":"page"},{"location":"reference.html","page":"Reference","title":"Reference","text":"Newton.halfpolytope\nNewton.halfpolytope_from_file","category":"page"},{"location":"reference.html#PolynomialOptimization.Newton.halfpolytope","page":"Reference","title":"PolynomialOptimization.Newton.halfpolytope","text":"halfpolytope(method, poly; verbose=false, preprocess_quick=true,\n    preprocess_randomized=false, preprocess_fine=false, preprocess=nothing,\n    filepath=nothing, parameters...)\n\nCalculates the Newton polytope for the sum of squares optimization of a given objective, which is half the Newton polytope of the objective itself. This requires the availability of a linear solver. For a list of supported solvers, see the solver reference.\n\nThere are three preprocessing methods which can be turned on individually or collectively using preprocess; depending on the problem, they may reduce the amount of time that is required to construct the convex hull of the full Newton polytope:\n\npreprocess_quick is the Akl-Toussaint heuristic. Every monomial will be checked against a linear program that scales as the number of variables in the objective. This is enabled by default.\npreprocess_randomized performs a reduction of the possible number of monomials that comprise the convex hull by picking smaller random subsets of them and eliminating entries in the subset that can be expressed by other entries. This is a good idea if the number of candidate monomials for the vertices of the convex hull is huge (so that preprocess_fine will take too long) but also very redundant. The final polish can be done by enabling both this and the following preprocessing option. Randomized reduction will use multithreading if possible.\npreprocess_fine performs an extensive reduction of the possible number of monomials that comprise the convex hull. Every monomial will be checked against a linear program that scales as the number of monomials in the objective (though it might become more efficient when monomials are ruled out).\n\nAfter preprocessing is done, the monomials in the half Newton polytope are constructed efficiently subject to a simple min/max-degree constraint using ExponentsMultideg and taken over into the basis if they are contained in the convex polytope whose vertices were determined based on the objective and preprocessing; this is done by performing a linear program for each candidate monomial.\n\nThe parameters will be passed on to the linear solver in every case (preprocessing and construction).\n\ninfo: Multithreading\nFor large initial sets of monomials ( 10), the final construction will use multithreading if possible. Make sure to start Julia with an appropriate number of threads configured.\n\ntip: Distributed computing\nThis function is capable of using MPI for multi-node distributed computing. For this, make sure to start Julia using mpiexec, appropriately configured; then load the MPI package in addition to PolynomialOptimization (this is required for distributed computing to work). If MPI.Init was not called before, PolynomialOptimization will do it for you. This function is compatible with the MPI thread level MPI.THREAD_FUNNELED if multithreading is used in combination with MPI. Currently, only the main function will use MPI, not the preprocessing.Note that the function will assume that each MPI worker has the same number of threads available. Further note that Julia's GC works in a multithreaded context using the SIGSEG signal. This is known to cause problems among all MPI backends, which can usually be fixed by using the most recent version of MPI and setting some environment variables. Not all of these settings are incorporated into the MPI package yet. For OpenMPI and Intel MPI, set ENV[\"IPATH_NO_BACKTRACE\"] = \"1\".\n\nwarning: Verbose output\nThe verbose option generates very helpful output to observe the current progress. It also works in a multithreaded and distributed context. However, consider the fact that providing these messages requires additional computational and communication effort and should not be enabled when speed matters.\n\ntip: Interrupting the computation/Large outputs\nIf you expect the final Newton basis to be very large, so that keeping everything in memory (potentially in parallel) might be troublesome, the option filepath allows to instead write the output to a file. This is also useful if the process of determining the polytope is aborted, as it can be resumed from its current state (also in a multithreaded or multiprocessing context) if the same file name is passed to filepath, provided the Julia configuration (number of threads, number of processes) was the same at any time. Make sure to always delete the output files if you compute with a different configuration or the results will probably be corrupt!Using this option will create one (or multiple, if multithreading/multiprocessing is used) file that has the file name filepath with the extension .out, and for every .out file also a corresponding .prog file that captures the current status. The .out file(s) will hold the resulting basis in a binary format, the .prog file is a small indicator required for resuming the operation after an abort. This function will return true when it is finished and the data was stored to a file; it will not load the actual data. To do so, use halfpolytope_from_file in a separate step, which can also tell you exactly how much memory will be required for this operation.\n\nSee also halfpolytope_from_file.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#PolynomialOptimization.Newton.halfpolytope_from_file","page":"Reference","title":"PolynomialOptimization.Newton.halfpolytope_from_file","text":"halfpolytope_from_file(filepath, objective; estimate=false, verbose=false)\n\nConstructs the Newton polytope for the sum of squares optimization of a given objective, which is half the Newton polytope of the objective itself. This function does not do any calculation, but instead loads the data that has been generated using halfpolytope with the given filepath on the given objective.\n\ninfo: Info\nThis function will not take into account the current Julia configuration, but instead lists all files that are compatible with the given filepath. This allows you to, e.g., create the data in a multi-node context with moderate memory requirements per CPU, but load it later in a single process with lots of memory available (though note that the integer size must match; data that was created on a 64-bit system can be reconstructed only on a 64-bit system). However, this requires you not to have multiple files from different configurations running.The function ignores the .prog files and just assembles the output of the .out files, so it does not check whether the calculation actually finished.\n\ntip: Memory requirements\nIf the parameter estimate is set to true, the function will only analyze the size of the files and from this return an estimation of how many monomials the output will contain. This is an overestimation, as it might happen that the files contain a small number of duplicates if the calculation was interrupted and subsequently resumed (although this is not very likely and the result should be pretty accurate).\n\nSee also halfpolytope.\n\n\n\n\n\n","category":"function"},{"location":"index.html#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"PolynomialOptimization is a Julia package that allows to easily optimize large-scale polynomial optimization problems (currently commutative only). It builds on MultivariatePolynomials to provide a concise interface for the specification of the problem and allows to directly control the problem's sparsity using correlative sparsity, (chordal) term sparsity, and a combination of both. It also supports complex-valued problems and positive semidefinite constraints, and allows to extract solutions even for sparse problems. It provides a solver interface specifically designed for the optimization problems arising in polynomial optimization. This interface makes it easy to implement new solvers. On purpose, MathOptInterface/JuMP is not employed; despite JuMP being very performant for a modelling framework, it introduces a significant overhead that is omitted in this way. The following solvers are supported:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Clarabel\nCOPT\nHypatia\nLoraine (own implementation)\nLoRADS\nLANCELOT (not based on relaxations)\nMosek\nProxSDP\nSCS\nSketchy CGAL (own implementation)\nSpecBM Primal (own implementation)","category":"page"},{"location":"index.html#Overview","page":"Introduction","title":"Overview","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Depth=3","category":"page"}]
}
